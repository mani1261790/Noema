{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# ニューラルネットワーク\n\nニューラルネットワークの理解で重要なのは、式を暗記することより「何を入力し、どこで誤差が生まれ、どう更新するか」を追えることです。\nこのノートでは、単一ニューロン（ロジスティック回帰）から始めて、XORでの失敗、2層MLPによる改善、勾配チェック、ミニバッチ学習、PyTorch実装へ進みます。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    TORCH_AVAILABLE = True\nexcept ModuleNotFoundError:\n    torch = None\n    nn = None\n    optim = None\n    TORCH_AVAILABLE = False\n\nnp.random.seed(42)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "まずは単一ニューロンで OR 問題を学習します。\nここで使う損失（BCE）は、正解に高い確率を出すほど小さくなります。\n\n- 正解が 1 のとき: 予測確率 `p` が 1 に近いほど損失は小さい\n- 正解が 0 のとき: `p` が 0 に近いほど損失は小さい\n\n`grad_w`, `grad_b` は「どちらへ重みを動かすと損失が下がるか」を表す量です。\n\n`logits` は sigmoid を通す前の生のスコアで、まだ確率ではありません。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X_or = np.array([\n    [0.0, 0.0],\n    [0.0, 1.0],\n    [1.0, 0.0],\n    [1.0, 1.0],\n])\n\ny_or = np.array([[0.0], [1.0], [1.0], [1.0]])\n\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\n\ndef bce_from_logits(y_true, logits):\n    # log(1 + exp(logits)) - y*logits\n    return float(np.mean(np.logaddexp(0.0, logits) - y_true * logits))\n\n\ndef train_logistic(X, y, lr=0.5, epochs=800):\n    w = np.zeros((X.shape[1], 1))\n    b = 0.0\n    loss_history = []\n\n    for _ in range(epochs):\n        logits = X @ w + b\n        prob = sigmoid(logits)\n        loss = bce_from_logits(y, logits)\n\n        grad_w = (X.T @ (prob - y)) / len(X)\n        grad_b = float(np.mean(prob - y))\n\n        w -= lr * grad_w\n        b -= lr * grad_b\n\n        loss_history.append(loss)\n\n    return w, b, loss_history\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "w_or, b_or, loss_or = train_logistic(X_or, y_or, lr=0.5, epochs=1000)\nprob_or = sigmoid(X_or @ w_or + b_or)\npred_or = (prob_or >= 0.5).astype(int)\n\nprint('w =', np.round(w_or.ravel(), 4), 'b =', round(float(b_or), 4))\nprint('prob =', np.round(prob_or.ravel(), 4))\nprint('pred =', pred_or.ravel())\nprint('true =', y_or.ravel().astype(int))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, ax = plt.subplots(figsize=(6.2, 3.6))\nax.plot(loss_or, color='#2b6cb0')\nax.set_title('OR: Logistic Regression Loss')\nax.set_xlabel('epoch')\nax.set_ylabel('BCE loss')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "次に XOR 問題で同じモデルを試します。\nXOR では正例 `(0,1), (1,0)` と負例 `(0,0), (1,1)` が対角にあり、1本の直線では分けられません。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X_xor = np.array([\n    [0.0, 0.0],\n    [0.0, 1.0],\n    [1.0, 0.0],\n    [1.0, 1.0],\n])\n\ny_xor = np.array([[0.0], [1.0], [1.0], [0.0]])\n\nw_xor, b_xor, loss_xor_logistic = train_logistic(X_xor, y_xor, lr=0.5, epochs=5000)\nprob_xor_logistic = sigmoid(X_xor @ w_xor + b_xor)\npred_xor_logistic = (prob_xor_logistic >= 0.5).astype(int)\n\nprint('logistic prob =', np.round(prob_xor_logistic.ravel(), 4))\nprint('logistic pred =', pred_xor_logistic.ravel())\nprint('true          =', y_xor.ravel().astype(int))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, ax = plt.subplots(figsize=(6.2, 3.6))\nax.plot(loss_xor_logistic, color='#c05621')\nax.set_title('XOR: Logistic Regression Loss')\nax.set_xlabel('epoch')\nax.set_ylabel('BCE loss')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "ここで 2層MLP（入力→隠れ層→出力）を使います。\n隠れ層の非線形変換が入ることで、XORのような非線形境界を表現できます。\n\nこの後のコードで使う変数名:\n- `z1`: 隠れ層への入力\n- `h`: 隠れ層の出力\n- `z2`: 出力層への入力\n- `p`: 予測確率\n- `d*`: 損失の勾配（どちらへ動かせば損失が減るか）\n\n特に `tanh` の微分は `1 - tanh(z)^2` を使います。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def train_mlp_xor(X, y, hidden_dim=4, lr=0.2, epochs=5000, batch_size=None, seed=0):\n    rng = np.random.default_rng(seed)\n\n    W1 = rng.normal(0.0, 0.6, size=(X.shape[1], hidden_dim))\n    b1 = np.zeros((1, hidden_dim))\n    W2 = rng.normal(0.0, 0.6, size=(hidden_dim, 1))\n    b2 = np.zeros((1, 1))\n\n    loss_history = []\n\n    if batch_size is None:\n        batch_size = len(X)\n\n    for _ in range(epochs):\n        indices = rng.permutation(len(X))\n        X_shuf = X[indices]\n        y_shuf = y[indices]\n\n        for start in range(0, len(X), batch_size):\n            end = start + batch_size\n            xb = X_shuf[start:end]\n            yb = y_shuf[start:end]\n\n            z1 = xb @ W1 + b1\n            h = np.tanh(z1)\n            z2 = h @ W2 + b2\n            p = sigmoid(z2)\n\n            dz2 = (p - yb) / len(xb)\n            dW2 = h.T @ dz2\n            db2 = np.sum(dz2, axis=0, keepdims=True)\n\n            dh = dz2 @ W2.T\n            dz1 = dh * (1 - np.tanh(z1) ** 2)\n            dW1 = xb.T @ dz1\n            db1 = np.sum(dz1, axis=0, keepdims=True)\n\n            W2 -= lr * dW2\n            b2 -= lr * db2\n            W1 -= lr * dW1\n            b1 -= lr * db1\n\n        full_logits = np.tanh(X @ W1 + b1) @ W2 + b2\n        loss_history.append(bce_from_logits(y, full_logits))\n\n    params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n    return params, loss_history\n\n\ndef mlp_predict_prob(X, params):\n    z1 = X @ params[\"W1\"] + params[\"b1\"]\n    h = np.tanh(z1)\n    z2 = h @ params[\"W2\"] + params[\"b2\"]\n    return sigmoid(z2)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "params_full, loss_xor_mlp = train_mlp_xor(X_xor, y_xor, hidden_dim=4, lr=0.2, epochs=5000, batch_size=4, seed=1)\nprob_xor_mlp = mlp_predict_prob(X_xor, params_full)\npred_xor_mlp = (prob_xor_mlp >= 0.5).astype(int)\n\nprint('mlp prob =', np.round(prob_xor_mlp.ravel(), 4))\nprint('mlp pred =', pred_xor_mlp.ravel())\nprint('true     =', y_xor.ravel().astype(int))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, ax = plt.subplots(figsize=(6.4, 3.7))\nax.plot(loss_xor_logistic, label='logistic (single neuron)', color='#c05621')\nax.plot(loss_xor_mlp, label='2-layer MLP', color='#2b6cb0')\nax.set_title('XOR: Loss Comparison')\nax.set_xlabel('epoch')\nax.set_ylabel('BCE loss')\nax.legend()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "逆伝播の実装が合っているかを確認する定番手法が勾配チェックです。\n\n手順は3つです。\n1. ある重みを `+eps` / `-eps` だけ動かす\n2. そのときの損失差から傾きを近似する（数値微分）\n3. 逆伝播で計算した勾配と近ければ実装は妥当と判断する\n\n目安として、`rel_err` が `1e-4` 以下なら逆伝播実装は概ね妥当と判断できます。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def mlp_forward(X, params):\n    z1 = X @ params[\"W1\"] + params[\"b1\"]\n    h = np.tanh(z1)\n    z2 = h @ params[\"W2\"] + params[\"b2\"]\n    p = sigmoid(z2)\n    return z1, h, z2, p\n\n\ndef mlp_backward(X, y, params):\n    z1, h, z2, p = mlp_forward(X, params)\n    dz2 = (p - y) / len(X)\n    dW2 = h.T @ dz2\n    db2 = np.sum(dz2, axis=0, keepdims=True)\n\n    dh = dz2 @ params[\"W2\"].T\n    dz1 = dh * (1 - np.tanh(z1) ** 2)\n    dW1 = X.T @ dz1\n    db1 = np.sum(dz1, axis=0, keepdims=True)\n\n    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n    return grads\n\n\ndef mlp_loss(X, y, params):\n    _, _, z2, _ = mlp_forward(X, params)\n    return bce_from_logits(y, z2)\n\n\ng = mlp_backward(X_xor, y_xor, params_full)\nchecks = [\n    (\"W1\", (0, 0)),\n    (\"W1\", (1, 2)),\n    (\"W2\", (0, 0)),\n    (\"W2\", (3, 0)),\n]\n\neps = 1e-5\ngrad_map = {\"W1\": g[\"dW1\"], \"W2\": g[\"dW2\"]}\n\nfor name, idx in checks:\n    params_plus = {k: v.copy() for k, v in params_full.items()}\n    params_minus = {k: v.copy() for k, v in params_full.items()}\n\n    params_plus[name][idx] += eps\n    params_minus[name][idx] -= eps\n\n    num = (mlp_loss(X_xor, y_xor, params_plus) - mlp_loss(X_xor, y_xor, params_minus)) / (2 * eps)\n    ana = float(grad_map[name][idx])\n    rel = abs(num - ana) / max(1e-12, abs(num) + abs(ana))\n\n    print(f\"{name}{idx} -> analytic={ana:.8f}, numeric={num:.8f}, rel_err={rel:.3e}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "同じMLPでも、バッチサイズを変えると更新の揺れ方が変わります。\nこのデータは4件なので、full batch は毎回4件で更新、mini-batch(2) は2件ずつ更新します。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "params_batch2, loss_xor_batch2 = train_mlp_xor(X_xor, y_xor, hidden_dim=4, lr=0.2, epochs=5000, batch_size=2, seed=1)\n\nfig, ax = plt.subplots(figsize=(6.4, 3.7))\nax.plot(loss_xor_mlp, label='full batch (4)', color='#2b6cb0')\nax.plot(loss_xor_batch2, label='mini-batch (2)', color='#2f855a')\nax.set_title('Batch Size Effect on XOR Training')\nax.set_xlabel('epoch')\nax.set_ylabel('BCE loss')\nax.legend()\nplt.tight_layout()\nplt.show()\n\nprob_batch2 = mlp_predict_prob(X_xor, params_batch2)\npred_batch2 = (prob_batch2 >= 0.5).astype(int)\nprint('mini-batch pred =', pred_batch2.ravel())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "最後に同じXORを PyTorch でも学習します。\nPyTorch が未導入の環境ではこの節をスキップするので、ノート全体はそのまま読み進められます。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if TORCH_AVAILABLE:\n    torch.manual_seed(42)\n\n    X_t = torch.tensor(X_xor, dtype=torch.float32)\n    y_t = torch.tensor(y_xor, dtype=torch.float32)\n\n    model = nn.Sequential(\n        nn.Linear(2, 4),\n        nn.Tanh(),\n        nn.Linear(4, 1),\n    )\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.2)\n\n    torch_loss = []\n    for _ in range(3000):\n        logits = model(X_t)\n        loss = criterion(logits, y_t)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        torch_loss.append(float(loss.detach()))\n\n    with torch.no_grad():\n        logits_t = model(X_t)\n        prob_t = torch.sigmoid(logits_t).numpy()\n        pred_t = (prob_t >= 0.5).astype(int)\n\n    print('torch prob =', np.round(prob_t.ravel(), 4))\n    print('torch pred =', pred_t.ravel())\n\n    fig, ax = plt.subplots(figsize=(6.2, 3.6))\n    ax.plot(torch_loss, color='#6b46c1')\n    ax.set_title('PyTorch MLP Loss (XOR)')\n    ax.set_xlabel('epoch')\n    ax.set_ylabel('BCE loss')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('PyTorchが未導入のため、この節はスキップしました。')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "ここまでの流れを対応づけると、\n\n- 単一ニューロン: 線形境界の分類\n- 2層MLP: 非線形境界の分類\n- 逆伝播: 誤差を各層へ配る更新規則\n- 勾配チェック: 実装検証\n- ミニバッチ: 計算効率と更新ノイズの調整\n- PyTorch: 実装を安全に高速化する実務ツール\n\nさらに対応を明示すると、\n`BCEWithLogitsLoss` は NumPy 側の logits ベース損失、\n`loss.backward()` は `mlp_backward`、\n`optimizer.step()` は重み更新ステップに対応します。\n\n次のノート（損失関数と勾配降下法）では、更新規則をさらに体系化していきます。"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
