{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import operator as op\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool UseとRAG\n",
    "\n",
    "RAG（Retrieval Augmented Generation）は、外部知識を検索してから回答を生成する方式です。\n",
    "Tool Useは、計算・検索・Web操作などをツール呼び出しとして明示的に実行する方式です。\n",
    "このノートでは、RAGとTool Useを同じ推論ループで扱う最小実装を作ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずRAGの最小パイプラインを作ります。\n",
    "\n",
    "実行前提: Python 3.10+ と `numpy` が必要です。未導入なら `pip install numpy` を実行してください。\n",
    "\n",
    "1. 文書をチャンク化\n",
    "2. 検索で上位チャンクを取得\n",
    "3. 必要なら再ランキング\n",
    "4. 取得文脈を使って回答生成（根拠付き）\n",
    "\n",
    "このノートでは固定長チャンクと文単位チャンクを同じインデックスに入れて比較します。\n",
    "後段で重複除外を行い、どちらの分割が効いたかを観察できるようにしています。\n",
    "\n",
    "用語メモ\n",
    "- `hit@k`: 上位k件のどこかに正解文書が入る割合\n",
    "- `MRR`: 正解順位の逆数平均（1位=1.0, 2位=0.5）\n",
    "- `routing accuracy`: 質問に対して適切なツールを選べた割合\n",
    "- `lexical overlap`: 回答語と根拠文脈語の重なり率（厳密な事実性指標ではない）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_docs = [\n",
    "    {\n",
    "        'id': 'doc-rl-1',\n",
    "        'title': '強化学習の基礎',\n",
    "        'text': 'ベルマン最適方程式は最適価値関数を再帰的に定義する。価値反復法はこの更新を繰り返す。',\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc-llm-1',\n",
    "        'title': 'LLMのファインチューニング',\n",
    "        'text': 'SFTは指示と回答ペアを用いて応答スタイルを調整する。LoRAは低ランク行列のみを更新する。',\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc-rag-1',\n",
    "        'title': 'RAGの実装ポイント',\n",
    "        'text': '固定長チャンクと意味チャンクで検索精度が変わる。再ランキングで関連度上位を並び替えると精度が改善する。',\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc-safe-1',\n",
    "        'title': 'ガードレール',\n",
    "        'text': 'Input Railsは危険入力を検知して遮断する。Output Railsは生成結果を検査して安全性を保つ。',\n",
    "    },\n",
    "]\n",
    "\n",
    "for d in knowledge_docs:\n",
    "    print(d['id'], d['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_length_chunk(text, chunk_size=26, overlap=6):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "        if i + chunk_size >= len(text):\n",
    "            break\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def sentence_chunk(text):\n",
    "    parts = re.split(r'[。!?！？]', text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "chunk_db = []\n",
    "for doc in knowledge_docs:\n",
    "    # 教材用に2方式を併走して比較し、後段で重複除外する\n",
    "    f_chunks = fixed_length_chunk(doc['text'])\n",
    "    s_chunks = sentence_chunk(doc['text'])\n",
    "\n",
    "    for idx, c in enumerate(f_chunks):\n",
    "        chunk_db.append({\n",
    "            'chunk_id': f\"{doc['id']}-f{idx}\",\n",
    "            'doc_id': doc['id'],\n",
    "            'title': doc['title'],\n",
    "            'text': c,\n",
    "            'mode': 'fixed',\n",
    "        })\n",
    "\n",
    "    for idx, c in enumerate(s_chunks):\n",
    "        chunk_db.append({\n",
    "            'chunk_id': f\"{doc['id']}-s{idx}\",\n",
    "            'doc_id': doc['id'],\n",
    "            'title': doc['title'],\n",
    "            'text': c,\n",
    "            'mode': 'sentence',\n",
    "        })\n",
    "\n",
    "print('chunk count:', len(chunk_db))\n",
    "print('fixed sample   :', [c['text'] for c in chunk_db if c['mode'] == 'fixed'][:2])\n",
    "print('sentence sample:', [c['text'] for c in chunk_db if c['mode'] == 'sentence'][:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ja_like(s):\n",
    "    s = re.sub(r'\\s+', '', s)\n",
    "    # 教育用: 文字2-gramでトークン化\n",
    "    if len(s) < 2:\n",
    "        return [s] if s else []\n",
    "    return [s[i:i+2] for i in range(len(s)-1)]\n",
    "\n",
    "\n",
    "def build_tfidf_index(chunks):\n",
    "    tokenized = [tokenize_ja_like(c['text']) for c in chunks]\n",
    "    vocab = sorted(set(t for toks in tokenized for t in toks))\n",
    "    stoi = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "    # TF: チャンク内の語頻度\n",
    "    tf = np.zeros((len(chunks), len(vocab)), dtype=np.float64)\n",
    "    for i, toks in enumerate(tokenized):\n",
    "        for t in toks:\n",
    "            tf[i, stoi[t]] += 1.0\n",
    "\n",
    "    # DF: その語を含むチャンク数, IDF: 珍しい語を重くする係数\n",
    "    df = np.count_nonzero(tf > 0, axis=0)\n",
    "    idf = np.log((1 + len(chunks)) / (1 + df)) + 1.0\n",
    "\n",
    "    tfidf = tf * idf[None, :]\n",
    "    norm = np.linalg.norm(tfidf, axis=1, keepdims=True) + 1e-12\n",
    "    tfidf = tfidf / norm\n",
    "\n",
    "    return {\n",
    "        'vocab': vocab,\n",
    "        'stoi': stoi,\n",
    "        'idf': idf,\n",
    "        'matrix': tfidf,\n",
    "        'chunks': chunks,\n",
    "    }\n",
    "\n",
    "\n",
    "def query_vector(query, index):\n",
    "    v = np.zeros(len(index['vocab']), dtype=np.float64)\n",
    "    for t in tokenize_ja_like(query):\n",
    "        j = index['stoi'].get(t)\n",
    "        if j is not None:\n",
    "            v[j] += 1.0\n",
    "    v = v * index['idf']\n",
    "    v /= np.linalg.norm(v) + 1e-12\n",
    "    return v\n",
    "\n",
    "\n",
    "def retrieve(query, index, top_k=5):\n",
    "    q = query_vector(query, index)\n",
    "    scores = index['matrix'] @ q\n",
    "    order = np.argsort(scores)[::-1][:top_k]\n",
    "    out = []\n",
    "    for i in order:\n",
    "        ch = index['chunks'][i]\n",
    "        out.append({\n",
    "            'chunk_id': ch['chunk_id'],\n",
    "            'doc_id': ch['doc_id'],\n",
    "            'title': ch['title'],\n",
    "            'text': ch['text'],\n",
    "            'score': float(scores[i]),\n",
    "            'mode': ch['mode'],\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "index = build_tfidf_index(chunk_db)\n",
    "res = retrieve('ベルマン最適方程式を説明して', index, top_k=6)\n",
    "for r in res:\n",
    "    print(r['chunk_id'], round(r['score'], 4), r['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, retrieved):\n",
    "    q_terms = set(tokenize_ja_like(query))\n",
    "    reranked = []\n",
    "    for r in retrieved:\n",
    "        c_terms = set(tokenize_ja_like(r['text']))\n",
    "        title_terms = set(tokenize_ja_like(r['title']))\n",
    "\n",
    "        overlap = len(q_terms & c_terms) / max(len(q_terms), 1)\n",
    "        title_overlap = len(q_terms & title_terms) / max(len(q_terms), 1)\n",
    "\n",
    "        # これは確率ではなく線形スコア。重みは検証データで調整する。\n",
    "        score = 0.65 * r['score'] + 0.25 * overlap + 0.10 * title_overlap\n",
    "        rr = dict(r)\n",
    "        rr['rerank_score'] = float(score)\n",
    "        reranked.append(rr)\n",
    "    reranked.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "query = 'ベルマン最適方程式を1文で説明して'\n",
    "retrieved = retrieve(query, index, top_k=6)\n",
    "reranked = rerank(query, retrieved)\n",
    "\n",
    "print('top reranked chunks:')\n",
    "for r in reranked[:3]:\n",
    "    print(r['chunk_id'], round(r['rerank_score'], 4), '|', r['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_citations(query, ranked_chunks, max_chunks=3):\n",
    "    ctx = ranked_chunks[:max_chunks]\n",
    "\n",
    "    # 取得文脈から重なり最大の文を抽出（extractive generation）\n",
    "    q_terms = set(tokenize_ja_like(query))\n",
    "    best = None\n",
    "    for c in ctx:\n",
    "        score = len(q_terms & set(tokenize_ja_like(c['text'])))\n",
    "        if best is None or score > best['score']:\n",
    "            best = {'score': score, 'text': c['text']}\n",
    "\n",
    "    if best is None or best['score'] == 0:\n",
    "        return {\n",
    "            'answer_text': '根拠文脈で十分な裏付けが見つからなかったため、追加情報が必要です。',\n",
    "            'refs': [],\n",
    "            'used_chunks': ctx,\n",
    "        }\n",
    "\n",
    "    answer_text = best['text']\n",
    "    refs = [f\"[{c['doc_id']}:{c['chunk_id']}]\" for c in ctx if c['text'] == best['text']]\n",
    "    if not refs and ctx:\n",
    "        refs = [f\"[{ctx[0]['doc_id']}:{ctx[0]['chunk_id']}]\"]\n",
    "\n",
    "    return {\n",
    "        'answer_text': answer_text,\n",
    "        'refs': refs,\n",
    "        'used_chunks': ctx,\n",
    "    }\n",
    "\n",
    "\n",
    "def lexical_overlap_ratio(answer_text, chunks):\n",
    "    # 回答語と根拠文脈語の重なり率（粗い指標）\n",
    "    a = set(tokenize_ja_like(answer_text))\n",
    "    c = set()\n",
    "    for ch in chunks:\n",
    "        c |= set(tokenize_ja_like(ch['text']))\n",
    "    return len(a & c) / max(len(a), 1)\n",
    "\n",
    "\n",
    "query = 'ベルマン最適方程式を1文で説明して'\n",
    "# これは比較用に手で置いた no-RAG の失敗例（モデル実行結果ではない）\n",
    "baseline_no_rag_manual = 'ベルマン最適方程式は量子状態を直接最適化する式です。'\n",
    "rag_out = generate_with_citations(query, reranked)\n",
    "rag_answer = rag_out['answer_text'] + ' ' + ' '.join(rag_out['refs'])\n",
    "\n",
    "print('manual baseline (no-RAG example):', baseline_no_rag_manual)\n",
    "print('RAG answer                      :', rag_answer)\n",
    "print('lexical overlap baseline =', round(lexical_overlap_ratio(baseline_no_rag_manual, rag_out['used_chunks']), 4))\n",
    "print('lexical overlap RAG      =', round(lexical_overlap_ratio(rag_out['answer_text'], rag_out['used_chunks']), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからTool Useです。\n",
    "LLMにすべてを内部推論させるより、外部ツール（検索・計算・Web操作）を明示的に呼び出す設計は、\n",
    "失敗箇所の切り分けと監査ログの取得に向いています。\n",
    "ただしルーティング誤りやツール側失敗があるので、評価と監視が必須です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_retrieve(query, top_k=3):\n",
    "    # 一旦深めに取得してから上位k件へ\n",
    "    fetch_k = max(top_k * 3, top_k)\n",
    "    r = rerank(query, retrieve(query, index, top_k=fetch_k))\n",
    "\n",
    "    # 同一テキストの重複を除外\n",
    "    dedup = []\n",
    "    seen = set()\n",
    "    for x in r:\n",
    "        key = (x['doc_id'], x['text'])\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(x)\n",
    "        if len(dedup) >= top_k:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'type': 'retrieval_result',\n",
    "        'items': [{\n",
    "            'doc_id': x['doc_id'],\n",
    "            'chunk_id': x['chunk_id'],\n",
    "            'text': x['text'],\n",
    "            'score': x['rerank_score'],\n",
    "        } for x in dedup]\n",
    "    }\n",
    "\n",
    "\n",
    "_ALLOWED_BIN_OPS = {\n",
    "    ast.Add: op.add,\n",
    "    ast.Sub: op.sub,\n",
    "    ast.Mult: op.mul,\n",
    "    ast.Div: op.truediv,\n",
    "    ast.Pow: op.pow,\n",
    "}\n",
    "_ALLOWED_UNARY_OPS = {ast.UAdd: op.pos, ast.USub: op.neg}\n",
    "\n",
    "\n",
    "def _safe_eval(node):\n",
    "    if isinstance(node, ast.Expression):\n",
    "        return _safe_eval(node.body)\n",
    "\n",
    "    if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):\n",
    "        return float(node.value)\n",
    "\n",
    "    if isinstance(node, ast.UnaryOp) and type(node.op) in _ALLOWED_UNARY_OPS:\n",
    "        return _ALLOWED_UNARY_OPS[type(node.op)](_safe_eval(node.operand))\n",
    "\n",
    "    if isinstance(node, ast.BinOp) and type(node.op) in _ALLOWED_BIN_OPS:\n",
    "        left = _safe_eval(node.left)\n",
    "        right = _safe_eval(node.right)\n",
    "\n",
    "        # 過剰計算を防ぐ簡易ガード\n",
    "        if isinstance(node.op, ast.Pow) and abs(right) > 10:\n",
    "            raise ValueError('exponent too large')\n",
    "\n",
    "        out = _ALLOWED_BIN_OPS[type(node.op)](left, right)\n",
    "        if abs(out) > 1e12:\n",
    "            raise ValueError('result too large')\n",
    "        return out\n",
    "\n",
    "    raise ValueError('unsupported expression')\n",
    "\n",
    "\n",
    "def tool_calculator(expression):\n",
    "    expr = expression.strip()\n",
    "    if len(expr) == 0 or len(expr) > 64:\n",
    "        return {'type': 'calc_result', 'error': 'invalid expression length'}\n",
    "\n",
    "    if not re.fullmatch(r'[0-9+\\-*/(). ]+', expr):\n",
    "        return {'type': 'calc_result', 'error': 'invalid expression'}\n",
    "\n",
    "    try:\n",
    "        tree = ast.parse(expr, mode='eval')\n",
    "        value = _safe_eval(tree)\n",
    "    except Exception as e:\n",
    "        return {'type': 'calc_result', 'error': str(e)}\n",
    "\n",
    "    return {'type': 'calc_result', 'value': value}\n",
    "\n",
    "\n",
    "def extract_expression_from_query(user_query):\n",
    "    # クエリ文字列から最も長い算術式っぽい部分を抽出\n",
    "    q = user_query.replace('^', '**')\n",
    "    segments = re.findall(r'[0-9.() +\\-*/]+', q)\n",
    "    candidates = []\n",
    "    for seg in segments:\n",
    "        expr = seg.strip()\n",
    "        if len(expr) < 3:\n",
    "            continue\n",
    "        if re.search(r'\\d', expr) and re.search(r'[+\\-*/]', expr):\n",
    "            candidates.append(expr)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    candidates.sort(key=len, reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def decide_tool(user_query):\n",
    "    q = user_query.lower()\n",
    "\n",
    "    # 日付（例: 2024-01-01）を計算式と誤判定しない\n",
    "    date_like = re.search(r'(?<!\\d)\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}(?!\\d)', q)\n",
    "    calc_intent_terms = ['計算', 'evaluate', '=', 'solve']\n",
    "    has_calc_intent = any(t in q for t in calc_intent_terms)\n",
    "    expr = extract_expression_from_query(user_query)\n",
    "\n",
    "    if has_calc_intent and expr and not date_like:\n",
    "        return {'tool': 'calculator', 'args': {'expression': expr}}\n",
    "\n",
    "    web_action_terms = ['クリック', '押して', 'tap', 'click', '選択', 'open', '開いて']\n",
    "    web_target_terms = ['button', 'ボタン', 'link', 'signin', 'sign in', 'login', 'ログイン', 'html', 'account', 'ページ']\n",
    "    danger_terms = ['delete', 'remove', 'purchase', 'buy', '送金', '削除', '購入']\n",
    "    if any(t in q for t in web_action_terms) and (any(t in q for t in web_target_terms) or any(t in q for t in danger_terms)):\n",
    "        return {'tool': 'web_agent', 'args': {'instruction': user_query}}\n",
    "\n",
    "    return {'tool': 'retrieve', 'args': {'query': user_query}}\n",
    "\n",
    "\n",
    "for q in [\n",
    "    '2+3*4を計算して',\n",
    "    '2*(3+4)を計算して',\n",
    "    '3.5+1.2を計算して',\n",
    "    'ベルマン方程式を説明して',\n",
    "    'Sign In を押して',\n",
    "    '2024-01-01の予定を教えて',\n",
    "]:\n",
    "    print(q, '->', decide_tool(q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Agentの最小例として、HTMLからクリック候補を抽出し、\n",
    "ユーザー指示との一致度が高い要素を選びます。\n",
    "\n",
    "- `Step Success Rate`: 各ステップで正しいアクションを選べた割合\n",
    "- `Success Rate`: 1タスクを最後まで全ステップ正しく完了できた割合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDOMParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = []\n",
    "        self.nodes = []\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.stack.append({\n",
    "            'tag': tag,\n",
    "            'attrs': dict(attrs),\n",
    "            'text_parts': [],\n",
    "        })\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        txt = data.strip()\n",
    "        if not txt:\n",
    "            return\n",
    "\n",
    "        # 祖先すべてに子孫テキストを集約（button > span のような構造に対応）\n",
    "        for node in self.stack:\n",
    "            node['text_parts'].append(txt)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if not self.stack:\n",
    "            return\n",
    "\n",
    "        node = self.stack.pop()\n",
    "        if node['tag'] != tag:\n",
    "            return\n",
    "\n",
    "        if node['tag'] in {'button', 'a'}:\n",
    "            attr = node['attrs']\n",
    "            text = ' '.join(node['text_parts']).strip()\n",
    "            self.nodes.append({\n",
    "                'tag': node['tag'],\n",
    "                'id': attr.get('id', ''),\n",
    "                'class': attr.get('class', ''),\n",
    "                'href': attr.get('href', ''),\n",
    "                'aria_label': attr.get('aria-label', ''),\n",
    "                'title': attr.get('title', ''),\n",
    "                'text': text,\n",
    "            })\n",
    "\n",
    "\n",
    "def tool_web_agent(html, instruction):\n",
    "    parser = SimpleDOMParser()\n",
    "    parser.feed(html)\n",
    "    inst = instruction.lower()\n",
    "\n",
    "    dangerous_terms = ['delete', 'remove', 'purchase', 'buy', '送金', '削除', '購入']\n",
    "    if any(t in inst for t in dangerous_terms):\n",
    "        return {\n",
    "            'type': 'web_action',\n",
    "            'action': 'blocked',\n",
    "            'reason': 'dangerous intent',\n",
    "            'target': None,\n",
    "            'score': 0.0,\n",
    "            'candidates': 0,\n",
    "        }\n",
    "\n",
    "    cand = []\n",
    "    for n in parser.nodes:\n",
    "        score = 0.0\n",
    "\n",
    "        # 行動語が含まれるか（クリック意図）\n",
    "        if any(t in inst for t in ['click', 'クリック', '押して', 'tap', 'open', '開いて']):\n",
    "            score += 0.2\n",
    "\n",
    "        searchable = ' '.join([n['text'], n['id'], n['class'], n['aria_label'], n['title']]).lower()\n",
    "        for key in ['login', 'sign in', 'signin', '検索', '送信', 'next', 'ログイン', 'docs']:\n",
    "            if key in inst and key in searchable:\n",
    "                score += 0.5\n",
    "\n",
    "        if n['id'] and n['id'].lower() in inst:\n",
    "            score += 0.4\n",
    "\n",
    "        cand.append((score, n))\n",
    "\n",
    "    cand.sort(key=lambda x: x[0], reverse=True)\n",
    "    if not cand or cand[0][0] < 0.6:\n",
    "        return {\n",
    "            'type': 'web_action',\n",
    "            'action': 'none',\n",
    "            'target': None,\n",
    "            'score': 0.0,\n",
    "            'candidates': len(cand),\n",
    "        }\n",
    "\n",
    "    best_score, best = cand[0]\n",
    "    return {\n",
    "        'type': 'web_action',\n",
    "        'action': 'click',\n",
    "        'requires_confirmation': True,\n",
    "        'target': best,\n",
    "        'score': best_score,\n",
    "        'candidates': len(cand),\n",
    "    }\n",
    "\n",
    "\n",
    "html = '''\n",
    "<div><button id=\"login-btn\"><span>Sign In</span></button></div>\n",
    "<div><a id=\"docs-link\" href=\"/docs\">Docs</a></div>\n",
    "<div><button id=\"next-btn\">Next</button></div>\n",
    "'''\n",
    "\n",
    "print(tool_web_agent(html, 'Sign In ボタンをクリックして'))\n",
    "print(tool_web_agent(html, 'delete account ボタンをクリックして'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_orchestrator(user_query, html_context=None):\n",
    "    plan = decide_tool(user_query)\n",
    "    if plan['tool'] == 'calculator':\n",
    "        tool_out = tool_calculator(**plan['args'])\n",
    "        final = f\"計算結果: {tool_out.get('value', tool_out.get('error'))}\"\n",
    "        return {'plan': plan, 'tool_output': tool_out, 'final_answer': final}\n",
    "\n",
    "    if plan['tool'] == 'web_agent':\n",
    "        html = html_context or '<div><button id=\"default\">OK</button></div>'\n",
    "        tool_out = tool_web_agent(html, plan['args']['instruction'])\n",
    "        if tool_out['action'] == 'blocked':\n",
    "            final = '危険操作の可能性があるため実行をブロックしました。'\n",
    "        elif tool_out['action'] == 'click':\n",
    "            tgt = tool_out['target']\n",
    "            final = f\"次の操作候補: click(tag={tgt['tag']}, id={tgt['id']}, text={tgt['text']}) ※ユーザー確認後に実行\"\n",
    "        else:\n",
    "            final = '実行可能な操作を特定できませんでした。'\n",
    "        return {'plan': plan, 'tool_output': tool_out, 'final_answer': final}\n",
    "\n",
    "    tool_out = tool_retrieve(**plan['args'])\n",
    "    ranked_for_gen = [\n",
    "        {'doc_id': i['doc_id'], 'chunk_id': i['chunk_id'], 'text': i['text'], 'rerank_score': i['score']}\n",
    "        for i in tool_out['items']\n",
    "    ]\n",
    "    rag_out = generate_with_citations(user_query, ranked_for_gen, max_chunks=len(ranked_for_gen))\n",
    "    final = rag_out['answer_text'] + (' ' + ' '.join(rag_out['refs']) if rag_out['refs'] else '')\n",
    "    return {\n",
    "        'plan': plan,\n",
    "        'tool_output': tool_out,\n",
    "        'rag_output': rag_out,\n",
    "        'final_answer': final,\n",
    "    }\n",
    "\n",
    "\n",
    "demo_queries = [\n",
    "    '2+3*4を計算して',\n",
    "    '2*(3+4)を計算して',\n",
    "    'ベルマン最適方程式を1文で説明して',\n",
    "    'Sign In ボタンをクリックして',\n",
    "    'delete account をクリックして',\n",
    "]\n",
    "\n",
    "for q in demo_queries:\n",
    "    out = tool_orchestrator(q, html_context=html)\n",
    "    print('Q:', q)\n",
    "    print('plan:', out['plan'])\n",
    "    print('final:', out['final_answer'])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価: retrieval / citation / routing\n",
    "# hit@k = 上位k件のどこかに正解docが含まれる割合\n",
    "rag_tests = [\n",
    "    ('ベルマン方程式を説明して', 'doc-rl-1'),\n",
    "    ('LoRAの利点は?', 'doc-llm-1'),\n",
    "    ('RAGの改善方法は?', 'doc-rag-1'),\n",
    "]\n",
    "\n",
    "hit1 = 0\n",
    "hit3 = 0\n",
    "mrr_sum = 0.0\n",
    "for q, expect_doc in rag_tests:\n",
    "    items = tool_retrieve(q, top_k=3)['items']\n",
    "    docs = [it['doc_id'] for it in items]\n",
    "\n",
    "    hit1 += int(len(docs) > 0 and docs[0] == expect_doc)\n",
    "    hit3 += int(expect_doc in docs)\n",
    "\n",
    "    rank = None\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        if d == expect_doc:\n",
    "            rank = i\n",
    "            break\n",
    "    mrr_sum += 0.0 if rank is None else 1.0 / rank\n",
    "\n",
    "print('retrieval hit@1 =', round(hit1 / len(rag_tests), 3))\n",
    "print('retrieval hit@3 =', round(hit3 / len(rag_tests), 3))\n",
    "print('retrieval MRR   =', round(mrr_sum / len(rag_tests), 3))\n",
    "\n",
    "chunk_lookup = {(c['doc_id'], c['chunk_id']): c['text'] for c in chunk_db}\n",
    "\n",
    "\n",
    "def citation_lexical_overlap_toy(answer_text, refs):\n",
    "    # 注意: 厳密な事実性ではなく、回答語と参照チャンク語の重なりを見る簡易指標\n",
    "    if not refs:\n",
    "        return 0.0\n",
    "\n",
    "    a_terms = set(tokenize_ja_like(answer_text))\n",
    "    support = 0\n",
    "    for r in refs:\n",
    "        m = re.match(r'^\\[(.+?):(.+?)\\]$', r)\n",
    "        if not m:\n",
    "            continue\n",
    "        key = (m.group(1), m.group(2))\n",
    "        text = chunk_lookup.get(key, '')\n",
    "        c_terms = set(tokenize_ja_like(text))\n",
    "        if len(a_terms & c_terms) >= 2:\n",
    "            support += 1\n",
    "\n",
    "    return support / max(len(refs), 1)\n",
    "\n",
    "\n",
    "overlap_scores = []\n",
    "for q, _ in rag_tests:\n",
    "    out = tool_orchestrator(q)\n",
    "    rag_out = out.get('rag_output', {'answer_text': '', 'refs': []})\n",
    "    overlap_scores.append(citation_lexical_overlap_toy(rag_out['answer_text'], rag_out['refs']))\n",
    "\n",
    "print('citation lexical overlap (toy) =', round(sum(overlap_scores) / len(overlap_scores), 3))\n",
    "\n",
    "route_tests = [\n",
    "    ('1+2を計算して', 'calculator'),\n",
    "    ('2*(3+4)を計算して', 'calculator'),\n",
    "    ('3.5+1.2を計算して', 'calculator'),\n",
    "    ('Sign In を押して', 'web_agent'),\n",
    "    ('ガードレールを説明して', 'retrieve'),\n",
    "    ('2024-01-01の予定を教えて', 'retrieve'),\n",
    "]\n",
    "route_hit = 0\n",
    "for q, t in route_tests:\n",
    "    route_hit += int(decide_tool(q)['tool'] == t)\n",
    "print('tool routing accuracy =', round(route_hit / len(route_tests), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コスト概算（仮定値）\n",
    "requests_per_day = 900\n",
    "avg_query_tok = 420\n",
    "avg_context_tok = 850   # RAGで追加される文脈\n",
    "avg_output_tok = 180\n",
    "\n",
    "price_in = 0.20   # USD / 1M input tokens\n",
    "price_out = 0.80  # USD / 1M output tokens\n",
    "\n",
    "cost_per_req = ((avg_query_tok + avg_context_tok) / 1e6) * price_in + (avg_output_tok / 1e6) * price_out\n",
    "daily_cost = cost_per_req * requests_per_day\n",
    "\n",
    "print('cost per request (USD):', round(cost_per_req, 6))\n",
    "print('daily cost (USD):', round(daily_cost, 4))\n",
    "\n",
    "# 単純なレイテンシ見積り\n",
    "retrieve_ms = 45\n",
    "rerank_ms = 30\n",
    "gen_ms = 520\n",
    "tool_overhead_ms = 25\n",
    "print('estimated latency (ms):', retrieve_ms + rerank_ms + gen_ms + tool_overhead_ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGとTool Useを組み合わせると、\n",
    "\n",
    "1. 根拠付き回答（RAG）\n",
    "2. 外部操作の明示実行（Tool Use）\n",
    "3. 監査しやすい推論ログ（plan/tool_output）\n",
    "\n",
    "を同じパイプラインで扱えます。\n",
    "ただし、ルーティング誤り・ツール失敗・根拠不足は常に起きるので、評価指標を継続監視する設計が前提です。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
