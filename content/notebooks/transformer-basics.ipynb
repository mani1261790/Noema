{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    torch = None\n",
    "    nn = None\n",
    "    optim = None\n",
    "    TORCH_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer（GPT / ViT / MAE）\n",
    "\n",
    "Transformerの中心は自己注意です。系列中の各要素が、ほかの要素をどれくらい参照すべきかを重みとして計算し、文脈に応じた表現を作ります。\n",
    "このノートでは、まず自己注意の式を最小コードで確認し、次にGPTの因果マスク、ViTのパッチ化、MAEのマスク再構成へ進みます。最後にVAEとの違いも整理します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自己注意では、入力埋め込み `X` から `Q, K, V` を作り、次で重みを計算します。\n",
    "\n",
    "`Attention(Q, K, V) = softmax((QK^T) / sqrt(d_k)) V`\n",
    "\n",
    "ここで `X` の形状を `(T, d_model)`、射影行列 `W_Q, W_K, W_V` を `(d_model, d_k)` とすると、\n",
    "`Q, K, V` の形状は `(T, d_k)` になります。\n",
    "要素で見ると `score_{i,j} = (q_i ・ k_j) / sqrt(d_k)` で、行ごとに `softmax` して `V` を重み付き和します。\n",
    "\n",
    "`Q` は「何を探しているか」、`K` は「何を持っているか」、`V` は「実際に受け渡す情報」と見ると理解しやすくなります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_rowwise(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# 4トークン, 埋め込み次元3の玩具例\n",
    "X = np.array([\n",
    "    [1.0, 0.2, 0.0],   # token 0\n",
    "    [0.9, 0.1, 0.1],   # token 1\n",
    "    [0.1, 0.8, 0.2],   # token 2\n",
    "    [0.0, 0.7, 1.0],   # token 3\n",
    "], dtype=np.float64)\n",
    "\n",
    "W_Q = np.array([[0.8, 0.0, 0.1], [0.2, 0.9, 0.1], [0.1, 0.2, 0.7]])\n",
    "W_K = np.array([[0.7, 0.1, 0.1], [0.1, 0.8, 0.2], [0.2, 0.1, 0.6]])\n",
    "W_V = np.array([[1.0, 0.2, 0.0], [0.1, 0.9, 0.1], [0.0, 0.2, 0.8]])\n",
    "\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "scores = (Q @ K.T) / math.sqrt(Q.shape[-1])\n",
    "weights = softmax_rowwise(scores)\n",
    "context = weights @ V\n",
    "\n",
    "print('attention weights:')\n",
    "print(np.round(weights, 4))\n",
    "print('\\ncontext vectors:')\n",
    "print(np.round(context, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.0, 4.2))\n",
    "plt.imshow(weights, cmap='Blues')\n",
    "plt.colorbar(label='attention weight')\n",
    "plt.xticks(range(len(X)), [f'k{j}' for j in range(len(X))])\n",
    "plt.yticks(range(len(X)), [f'q{i}' for i in range(len(X))])\n",
    "plt.title('Self-Attention Weight Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTのような自己回帰モデルでは、未来トークンを見ないように因果マスク（causal mask）を入れます。\n",
    "`j > i`（未来位置）をマスクし、スコアを `-∞` 相当に落として `softmax` 後の重みをほぼ0にします。\n",
    "\n",
    "下のコードでは、同じ入力で「マスクなし」と「マスクあり」を比較します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_masked_attention(Q, K, V):\n",
    "    T, d = Q.shape\n",
    "    scores = (Q @ K.T) / math.sqrt(d)\n",
    "    mask = np.triu(np.ones((T, T), dtype=bool), k=1)  # future positions\n",
    "    scores_masked = scores.copy()\n",
    "    scores_masked[mask] = -np.inf\n",
    "    w = softmax_rowwise(scores_masked)\n",
    "    return w @ V, w\n",
    "\n",
    "\n",
    "context_full = softmax_rowwise((Q @ K.T) / math.sqrt(Q.shape[-1])) @ V\n",
    "context_causal, w_causal = causal_masked_attention(Q, K, V)\n",
    "\n",
    "print('full attention (token 1):', np.round(context_full[1], 4))\n",
    "print('causal attention (token 1):', np.round(context_causal[1], 4))\n",
    "print('\\ncausal weight matrix:')\n",
    "print(np.round(w_causal, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "位置情報がないと、Transformerは順序を区別できません。\n",
    "よく使われる方法のひとつが正弦波位置エンコーディングです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    pe = np.zeros((seq_len, d_model), dtype=np.float64)\n",
    "    pos = np.arange(seq_len)[:, None]\n",
    "    i = np.arange(d_model)[None, :]\n",
    "    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / d_model)\n",
    "    angles = pos * angle_rates\n",
    "\n",
    "    pe[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return pe\n",
    "\n",
    "\n",
    "pe = sinusoidal_positional_encoding(seq_len=24, d_model=16)\n",
    "print('positional encoding shape:', pe.shape)\n",
    "\n",
    "plt.figure(figsize=(7.2, 3.6))\n",
    "plt.imshow(pe.T, aspect='auto', cmap='coolwarm')\n",
    "plt.colorbar(label='value')\n",
    "plt.xlabel('position')\n",
    "plt.ylabel('channel')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT（Vision Transformer）は画像を小さなパッチ列に分割し、各パッチをトークンとして扱います。\n",
    "以下では 8x8 の画像を 2x2 パッチへ分割し、パッチ埋め込みを作る最小例を示します。\n",
    "\n",
    "実際のViTでは、パッチトークンに位置埋め込みを必ず加えます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(image, patch_size=2):\n",
    "    h, w = image.shape\n",
    "    assert h % patch_size == 0 and w % patch_size == 0\n",
    "    patches = []\n",
    "    for y in range(0, h, patch_size):\n",
    "        for x in range(0, w, patch_size):\n",
    "            p = image[y:y+patch_size, x:x+patch_size].reshape(-1)\n",
    "            patches.append(p)\n",
    "    return np.stack(patches, axis=0)\n",
    "\n",
    "\n",
    "img = np.arange(64, dtype=np.float64).reshape(8, 8) / 63.0\n",
    "patches = patchify(img, patch_size=2)  # (16, 4)\n",
    "\n",
    "W_patch = np.random.default_rng(0).normal(0, 0.4, size=(4, 6))\n",
    "patch_tokens = patches @ W_patch  # (16, 6)\n",
    "\n",
    "# 実際のViTは学習可能な位置埋め込みを加える\n",
    "pos_embed = np.random.default_rng(1).normal(0, 0.1, size=patch_tokens.shape)\n",
    "patch_tokens = patch_tokens + pos_embed\n",
    "\n",
    "# CLSトークンも通常は学習可能ベクトル（ここでは最小例として0初期化）\n",
    "cls_token = np.zeros((1, 6), dtype=np.float64)\n",
    "vit_tokens = np.concatenate([cls_token, patch_tokens], axis=0)  # (17, 6)\n",
    "\n",
    "print('image shape      :', img.shape)\n",
    "print('patches shape    :', patches.shape)\n",
    "print('patch token shape:', patch_tokens.shape)\n",
    "print('ViT token shape  :', vit_tokens.shape, '(CLS + patches)')\n",
    "\n",
    "plt.figure(figsize=(3.8, 3.8))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Toy image (8x8)')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE（Masked Autoencoder）は、パッチの一部を隠して復元する自己教師あり学習です。\n",
    "ViTと違い、学習時には可視パッチだけをエンコーダに入れるため、計算効率を上げやすい設計です。\n",
    "\n",
    "完全な流れは\n",
    "`visible patches -> encoder -> (mask tokenで長さ復元) -> decoder -> masked patchesの再構成誤差`\n",
    "です。下のセルはこの流れを最小化して追うデモです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_mask(n_tokens, mask_ratio=0.75, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_mask = int(n_tokens * mask_ratio)\n",
    "    perm = rng.permutation(n_tokens)\n",
    "    mask_idx = perm[:n_mask]\n",
    "    keep_idx = np.sort(perm[n_mask:])\n",
    "    return keep_idx, np.sort(mask_idx)\n",
    "\n",
    "\n",
    "n_patches = patches.shape[0]\n",
    "keep_idx, mask_idx = make_random_mask(n_patches, mask_ratio=0.75, seed=1)\n",
    "kept_tokens = patch_tokens[keep_idx]\n",
    "\n",
    "print('all patches :', n_patches)\n",
    "print('kept patches:', len(keep_idx), 'indices=', keep_idx)\n",
    "print('masked      :', len(mask_idx), 'indices=', mask_idx)\n",
    "print('encoder input token shape (without CLS):', kept_tokens.shape)\n",
    "\n",
    "# デコーダ入力側で元の長さに戻す（masked位置にはmask tokenを置く）\n",
    "mask_token = np.zeros((len(mask_idx), patch_tokens.shape[1]))\n",
    "decoder_input = np.zeros_like(patch_tokens)\n",
    "decoder_input[keep_idx] = kept_tokens\n",
    "decoder_input[mask_idx] = mask_token\n",
    "\n",
    "# 最小デモ: 線形デコーダでパッチを復元し、masked部分のみ誤差を計算\n",
    "W_rec = np.random.default_rng(2).normal(0, 0.3, size=(patch_tokens.shape[1], patches.shape[1]))\n",
    "recon_patches = decoder_input @ W_rec\n",
    "masked_recon_mse = np.mean((recon_patches[mask_idx] - patches[mask_idx]) ** 2)\n",
    "print('masked reconstruction MSE (toy):', round(float(masked_recon_mse), 6))\n",
    "\n",
    "masked_view = img.copy()\n",
    "patch_size = 2\n",
    "for idx in mask_idx:\n",
    "    gy = idx // (img.shape[1] // patch_size)\n",
    "    gx = idx % (img.shape[1] // patch_size)\n",
    "    y0, x0 = gy * patch_size, gx * patch_size\n",
    "    masked_view[y0:y0+patch_size, x0:x0+patch_size] = 0.0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7.2, 3.4))\n",
    "axes[0].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0].set_title('original')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(masked_view, cmap='gray', vmin=0, vmax=1)\n",
    "axes[1].set_title('MAE masking (75%)')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEも再構成を使いますが、MAEと目的が異なります。\n",
    "VAEは潜在変数 `z` の確率分布を学ぶ生成モデルで、\n",
    "損失は `再構成誤差 + β * KL` です。\n",
    "\n",
    "一方MAEは、主に表現学習を目的に「隠したパッチの再構成」を解きます。\n",
    "下のセルはVAE損失のうち、KL項と再構成項の最小計算デモです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAEの再パラメータ化トリックと損失項の最小計算\n",
    "mu = np.array([0.2, -0.4, 0.1], dtype=np.float64)\n",
    "logvar = np.array([-0.2, 0.3, -0.5], dtype=np.float64)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "eps = rng.normal(0.0, 1.0, size=mu.shape)\n",
    "std = np.exp(0.5 * logvar)\n",
    "z = mu + std * eps\n",
    "\n",
    "# KL項\n",
    "kl = -0.5 * np.sum(1 + logvar - mu**2 - np.exp(logvar))\n",
    "\n",
    "# 再構成項（ここではMSEの玩具例）\n",
    "x_true = np.array([0.9, 0.2, 0.7], dtype=np.float64)\n",
    "x_recon = np.array([0.8, 0.3, 0.6], dtype=np.float64)\n",
    "recon_loss = np.mean((x_true - x_recon) ** 2)\n",
    "\n",
    "beta = 1.0\n",
    "vae_loss = recon_loss + beta * kl\n",
    "\n",
    "print('mu    =', np.round(mu, 4))\n",
    "print('logvar=', np.round(logvar, 4))\n",
    "print('z sample =', np.round(z, 4))\n",
    "print('recon loss =', round(float(recon_loss), 6))\n",
    "print('KL(q(z|x) || p(z)) =', round(float(kl), 6))\n",
    "print('VAE loss = recon + beta*KL =', round(float(vae_loss), 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、PyTorchで小さなDecoder-only Transformer（GPT型）を学習し、\n",
    "「次トークン予測」が実際に動くことを確認します。\n",
    "\n",
    "ここでは次トークンが直前2トークンに依存する列を使い、文脈参照が必要な設定にしています。\n",
    "実装は簡略化のため `TransformerEncoderLayer + 因果マスク` で、GPT型の挙動を再現します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    vocab_size = 20\n",
    "    seq_len = 14\n",
    "    d_model = 48\n",
    "\n",
    "    def sample_batch(batch_size=64):\n",
    "        # Fibonacci-like modulo sequence: x_t = (x_{t-1} + x_{t-2}) mod vocab\n",
    "        seq = torch.zeros(batch_size, seq_len + 1, dtype=torch.long)\n",
    "        seq[:, 0] = torch.randint(0, vocab_size, (batch_size,))\n",
    "        seq[:, 1] = torch.randint(0, vocab_size, (batch_size,))\n",
    "        for t in range(2, seq_len + 1):\n",
    "            seq[:, t] = (seq[:, t - 1] + seq[:, t - 2]) % vocab_size\n",
    "        return seq[:, :-1], seq[:, 1:]\n",
    "\n",
    "    class TinyGPT(nn.Module):\n",
    "        def __init__(self, vocab_size, d_model=48, nhead=4, num_layers=2, seq_len=14):\n",
    "            super().__init__()\n",
    "            self.seq_len = seq_len\n",
    "            self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "            self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "            layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=4 * d_model,\n",
    "                dropout=0.0,\n",
    "                batch_first=True,\n",
    "                activation='gelu',\n",
    "            )\n",
    "            self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "            self.norm = nn.LayerNorm(d_model)\n",
    "            self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            bsz, t = x.shape\n",
    "            h = self.token_emb(x) + self.pos_emb[:, :t, :]\n",
    "            mask = torch.triu(torch.ones(t, t, device=x.device), diagonal=1).bool()\n",
    "            h = self.encoder(h, mask=mask)\n",
    "            h = self.norm(h)\n",
    "            return self.head(h)\n",
    "\n",
    "    model = TinyGPT(vocab_size=vocab_size, d_model=d_model, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(180):\n",
    "        x, y = sample_batch(batch_size=64)\n",
    "        logits = model(x)\n",
    "\n",
    "        # t=0 の予測は初期2トークン中1つ目だけでは不確定なので除外\n",
    "        loss = criterion(logits[:, 1:, :].reshape(-1, vocab_size), y[:, 1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 45 == 0:\n",
    "            print(f'step={step:>3d}, loss={loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    seed = torch.tensor([[3, 7]], dtype=torch.long)\n",
    "    generated = seed.clone()\n",
    "    for _ in range(10):\n",
    "        cur = generated[:, -seq_len:]\n",
    "        logits = model(cur)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    print('generated tokens:', generated.squeeze(0).tolist())\n",
    "else:\n",
    "    print('PyTorch未導入のためGPTミニ実験セルはスキップしました。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerを使うときは、「どのトークン集合を作るか」が設計の中心になります。\n",
    "文章なら単語列、画像ならパッチ列、MAEなら可視パッチ列です。\n",
    "同じ自己注意の枠組みでも、トークン化と目的関数を変えることでGPT/ViT/MAEのように振る舞いが変わります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}