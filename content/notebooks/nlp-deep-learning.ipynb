{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    torch = None\n",
    "    nn = None\n",
    "    optim = None\n",
    "    TORCH_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理（NLP）\n",
    "\n",
    "自然言語処理では、まず文字列を数値列に変換し、その数値列から意味や文脈を学習します。\n",
    "このノートでは、トークン化と語彙作成から始めて、次トークン予測、SFT（Supervised Fine-Tuning）用データ整形、LoRA/QLoRAの計算感覚までを順に確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の関門はトークン化です。\n",
    "同じ文でも、単語単位で切るか、文字単位で切るかで系列長や未知語の扱いが変わります。\n",
    "\n",
    "日本語は空白で単語境界が明示されないため、`split(' ')` は失敗しやすい方法です。\n",
    "ここでは、まず失敗例として空白分割を見たあと、文字単位分割と比較します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\n",
    "    'LLMは文脈に応じて次の単語を予測する。',\n",
    "    'SFTでは指示と回答のペアを教師信号にする。',\n",
    "    'モデル評価では正答率だけでなく出力品質も見る。',\n",
    "    '未知語が多いと語彙外トークンが増えて性能が落ちやすい。',\n",
    "    '日本語でも英語でもトークン化の設計は重要。',\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[。､，,.!?！？]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def whitespace_tokenize(s):\n",
    "    return normalize_text(s).split(' ')\n",
    "\n",
    "\n",
    "def char_tokenize(s):\n",
    "    s = normalize_text(s)\n",
    "    s = s.replace(' ', '')\n",
    "    return list(s)\n",
    "\n",
    "\n",
    "ws_tokenized = [whitespace_tokenize(t) for t in raw_texts]\n",
    "char_tokenized = [char_tokenize(t) for t in raw_texts]\n",
    "\n",
    "for i in range(len(raw_texts)):\n",
    "    print(f'--- sample {i} ---')\n",
    "    print('whitespace:', ws_tokenized[i])\n",
    "    print('char      :', char_tokenized[i][:20], '...')\n",
    "\n",
    "ws_lengths = np.array([len(t) for t in ws_tokenized], dtype=np.int64)\n",
    "char_lengths = np.array([len(t) for t in char_tokenized], dtype=np.int64)\n",
    "\n",
    "print('\\nmean length (whitespace) =', float(ws_lengths.mean()))\n",
    "print('mean length (char)       =', float(char_lengths.mean()))\n",
    "\n",
    "# 以降は外部トークナイザ依存を避けるため、文字単位を使用\n",
    "tokenized = char_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(raw_texts))\n",
    "width = 0.36\n",
    "\n",
    "plt.figure(figsize=(7.2, 3.6))\n",
    "plt.bar(x - width/2, ws_lengths, width=width, label='whitespace split', color='#7aa2ff')\n",
    "plt.bar(x + width/2, char_lengths, width=width, label='char split', color='#8dd3a7')\n",
    "plt.xticks(x, [f's{i}' for i in range(len(raw_texts))])\n",
    "plt.ylabel('token count')\n",
    "plt.title('Token length by tokenization strategy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に語彙（vocabulary）を作って、トークンをIDへ変換します。\n",
    "`<pad>` と `<unk>` を先頭に置くのは実務でもよくある設計です。\n",
    "\n",
    "このノートでは文字単位トークンを使っているので、未知語問題は「未知文字」の形で現れます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(tok for toks in tokenized for tok in toks)\n",
    "special_tokens = ['<pad>', '<unk>']\n",
    "base_vocab = [tok for tok, _ in counter.most_common()]\n",
    "vocab = special_tokens + base_vocab\n",
    "stoi = {tok: i for i, tok in enumerate(vocab)}\n",
    "itos = {i: tok for tok, i in stoi.items()}\n",
    "\n",
    "\n",
    "def encode(tokens):\n",
    "    unk = stoi['<unk>']\n",
    "    return [stoi.get(tok, unk) for tok in tokens]\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    return [itos.get(i, '<unk>') for i in ids]\n",
    "\n",
    "\n",
    "encoded = [encode(toks) for toks in tokenized]\n",
    "print('vocab size =', len(vocab))\n",
    "print('first sample ids =', encoded[0])\n",
    "print('decoded back      =', decode(encoded[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込み（embedding）は、トークンIDを実数ベクトルへ写像する層です。\n",
    "意味の近い語が近いベクトルになる現象は、埋め込みを学習した後に現れます。\n",
    "\n",
    "下のセルでは、\n",
    "1. 学習前（乱数初期化）の類似度\n",
    "2. 小さな共起データで簡易学習した後の類似度\n",
    "を比較します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    mini_sentences = [\n",
    "        'モデル 学習 損失 最適化',\n",
    "        'モデル 訓練 データ 評価',\n",
    "        '文章 単語 文脈 トークン',\n",
    "        '文脈 予測 モデル 生成',\n",
    "        '訓練 最適化 損失 収束',\n",
    "    ]\n",
    "\n",
    "    word_tokens = [s.split(' ') for s in mini_sentences]\n",
    "    w_vocab = sorted(set(tok for toks in word_tokens for tok in toks))\n",
    "    w_stoi = {w: i for i, w in enumerate(w_vocab)}\n",
    "\n",
    "    pairs = []\n",
    "    window = 1\n",
    "    for toks in word_tokens:\n",
    "        ids = [w_stoi[t] for t in toks]\n",
    "        for i, center in enumerate(ids):\n",
    "            for j in range(max(0, i - window), min(len(ids), i + window + 1)):\n",
    "                if i != j:\n",
    "                    pairs.append((center, ids[j]))\n",
    "\n",
    "    emb = nn.Embedding(len(w_vocab), 16)\n",
    "    out = nn.Linear(16, len(w_vocab), bias=False)\n",
    "    opt = optim.Adam(list(emb.parameters()) + list(out.parameters()), lr=5e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    x_train = torch.tensor([c for c, _ in pairs], dtype=torch.long)\n",
    "    y_train = torch.tensor([ctx for _, ctx in pairs], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        init_vec = emb.weight.clone()\n",
    "\n",
    "    for _ in range(220):\n",
    "        h = emb(x_train)\n",
    "        logits = out(h)\n",
    "        loss = criterion(logits, y_train)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    def cos(v1, v2):\n",
    "        return float(torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2) + 1e-12))\n",
    "\n",
    "    i_model = w_stoi['モデル']\n",
    "    i_train = w_stoi['訓練']\n",
    "    init_sim = cos(init_vec[i_model], init_vec[i_train])\n",
    "    trained_sim = cos(emb.weight[i_model].detach(), emb.weight[i_train].detach())\n",
    "\n",
    "    print('vocab:', w_vocab)\n",
    "    print('cos(model, train) before learning =', round(init_sim, 4))\n",
    "    print('cos(model, train) after learning  =', round(trained_sim, 4))\n",
    "else:\n",
    "    rng = np.random.default_rng(0)\n",
    "    emb = rng.normal(0, 0.4, size=(6, 8))\n",
    "    v1, v2 = emb[0], emb[1]\n",
    "    sim = float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12))\n",
    "    print('PyTorch未導入のため学習前ランダム埋め込みのみ表示します。')\n",
    "    print('random cosine =', round(sim, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語モデル学習では、系列 `x_0, x_1, ...` に対して\n",
    "`x_t` までを入力として `x_{t+1}` を予測します。\n",
    "損失にはクロスエントロピーを使うのが標準です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手計算に近い最小クロスエントロピー例\n",
    "logits = np.array([1.2, -0.4, 0.3, 2.0], dtype=np.float64)\n",
    "target_id = 3\n",
    "\n",
    "logits_shift = logits - np.max(logits)\n",
    "probs = np.exp(logits_shift) / np.sum(np.exp(logits_shift))\n",
    "loss = -math.log(probs[target_id] + 1e-12)\n",
    "\n",
    "print('probs =', np.round(probs, 4))\n",
    "print('target id =', target_id)\n",
    "print('cross entropy =', round(loss, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFT（Supervised Fine-Tuning）では、\n",
    "「指示文（instruction）+ 入力（input）+ 望ましい回答（output）」を1本のテキストに整形して学習します。\n",
    "\n",
    "ポイントは、損失をどこに掛けるかです。\n",
    "通常は回答本文に損失を掛け、指示部分は `ignore_index` で除外します。\n",
    "また、言語モデル学習では入力と教師ラベルを1トークン右シフトして計算します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_examples = [\n",
    "    {\n",
    "        'instruction': '次の文を要約してください。',\n",
    "        'input': 'Transformerは系列全体を同時に参照できるため、長距離依存を扱いやすい。',\n",
    "        'output': 'Transformerは長距離依存を扱いやすい。',\n",
    "    },\n",
    "    {\n",
    "        'instruction': '用語を説明してください。',\n",
    "        'input': 'SFT',\n",
    "        'output': '教師ありデータでモデル応答を調整する学習。',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def format_sft(ex):\n",
    "    return (\n",
    "        '<system>あなたは丁寧なAIアシスタントです。</system>\\n'\n",
    "        f\"<user>{ex['instruction']}\\n{ex['input']}</user>\\n\"\n",
    "        f\"<assistant>{ex['output']}</assistant>\"\n",
    "    )\n",
    "\n",
    "\n",
    "formatted = [format_sft(ex) for ex in sft_examples]\n",
    "for i, text in enumerate(formatted):\n",
    "    print(f'--- sample {i} ---')\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字単位の簡易トークナイズで「回答部のみloss」を可視化\n",
    "chars = sorted(set(''.join(formatted)))\n",
    "char_vocab = ['<pad>', '<unk>'] + chars\n",
    "c_stoi = {c: i for i, c in enumerate(char_vocab)}\n",
    "\n",
    "\n",
    "def encode_chars(s):\n",
    "    unk = c_stoi['<unk>']\n",
    "    return [c_stoi.get(ch, unk) for ch in s]\n",
    "\n",
    "\n",
    "ignore_index = -100\n",
    "for i, text in enumerate(formatted):\n",
    "    ids = encode_chars(text)\n",
    "\n",
    "    start_tag = '<assistant>'\n",
    "    end_tag = '</assistant>'\n",
    "    start_pos = text.find(start_tag)\n",
    "    end_pos = text.find(end_tag)\n",
    "\n",
    "    labels = [ignore_index] * len(ids)\n",
    "    if start_pos >= 0 and end_pos > start_pos:\n",
    "        start = start_pos + len(start_tag)\n",
    "        end = end_pos\n",
    "        for j in range(start, end):\n",
    "            labels[j] = ids[j]\n",
    "\n",
    "    # 右シフト後に実際に損失へ入るラベルを計算\n",
    "    input_ids = ids[:-1]\n",
    "    target_ids = labels[1:]\n",
    "\n",
    "    active = sum(1 for v in target_ids if v != ignore_index)\n",
    "    print(f'sample {i}: input_len={len(input_ids)}, supervised_after_shift={active}, ratio={active/max(len(input_ids),1):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、LoRA/QLoRA の計算量感覚を押さえます。\n",
    "大きな重み行列 `W` を丸ごと更新せず、低ランク行列 `A, B` だけを学習するのがLoRAです。\n",
    "\n",
    "`W' = W + (alpha / r) * BA`（`A: r x d_in`, `B: d_out x r`）なので、追加パラメータは `r*(d_in + d_out)` です。\n",
    "QLoRAではベース重みを量子化し、LoRA部分だけ高精度で更新します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_param_count(d_in, d_out, rank):\n",
    "    full = d_in * d_out\n",
    "    lora = rank * (d_in + d_out)\n",
    "    return full, lora\n",
    "\n",
    "\n",
    "for rank in [4, 8, 16, 32]:\n",
    "    full, lora = lora_param_count(d_in=4096, d_out=4096, rank=rank)\n",
    "    print(f'rank={rank:>2d}: full={full:,}, lora={lora:,}, ratio={lora/full:.6f}')\n",
    "\n",
    "# QLoRAの直感: baseを4bit量子化し、adapterは通常精度で学習\n",
    "base_params = 7_000_000_000\n",
    "base_16bit_gb = base_params * 16 / 8 / (1024**3)\n",
    "base_4bit_gb = base_params * 4 / 8 / (1024**3)\n",
    "print('\\nbase model memory (approx, weights only):')\n",
    "print('fp16:', round(base_16bit_gb, 2), 'GB')\n",
    "print('4bit:', round(base_4bit_gb, 2), 'GB')\n",
    "print('note: optimizer state / activations / metadataは別途必要')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、PyTorchで小さな文字レベル言語モデルを学習し、\n",
    "次トークン予測とテキスト生成を体験します。\n",
    "\n",
    "このセルは教育用の最小例で、実際のLLM学習とは規模も最適化手法も異なります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    corpus = [\n",
    "        'transformerは文脈を使って次を予測する',\n",
    "        'sftは指示と回答のペアで学習する',\n",
    "        'loraは追加パラメータを小さくできる',\n",
    "        'token化と語彙設計は性能に効く',\n",
    "    ]\n",
    "\n",
    "    text = '\\n'.join(corpus)\n",
    "    vocab_chars = sorted(set(text))\n",
    "    vocab = ['<unk>'] + vocab_chars\n",
    "    stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "    itos = {i: ch for ch, i in stoi.items()}\n",
    "    unk_id = stoi['<unk>']\n",
    "\n",
    "    data = torch.tensor([stoi.get(ch, unk_id) for ch in text], dtype=torch.long)\n",
    "\n",
    "    block_size = 24\n",
    "    batch_size = 32\n",
    "\n",
    "    def get_batch():\n",
    "        idx = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "        return x, y\n",
    "\n",
    "    class TinyCharLM(nn.Module):\n",
    "        def __init__(self, vocab_size, d_model=64):\n",
    "            super().__init__()\n",
    "            self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "            self.rnn = nn.GRU(d_model, d_model, batch_first=True)\n",
    "            self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h = self.token_emb(x)\n",
    "            out, _ = self.rnn(h)\n",
    "            logits = self.head(out)\n",
    "            return logits\n",
    "\n",
    "    model = TinyCharLM(vocab_size=len(vocab), d_model=64)\n",
    "    opt = optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(220):\n",
    "        xb, yb = get_batch()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits.reshape(-1, len(vocab)), yb.reshape(-1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if step % 55 == 0:\n",
    "            print(f'step={step:>3d}, loss={loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    prompt = 'sftは？'\n",
    "    unknown_count = sum(1 for ch in prompt if ch not in stoi)\n",
    "    ids = [stoi.get(ch, unk_id) for ch in prompt]\n",
    "    print('prompt unknown chars replaced with <unk> =', unknown_count)\n",
    "\n",
    "    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    for _ in range(40):\n",
    "        logits = model(x)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        x = torch.cat([x, next_id], dim=1)\n",
    "\n",
    "    generated = ''.join(itos[i] if i != unk_id else '□' for i in x.squeeze(0).tolist())\n",
    "    print('\\nGenerated text:')\n",
    "    print(generated)\n",
    "else:\n",
    "    print('PyTorch未導入のため、言語モデル実験セルはスキップしました。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLPでは、モデル構造だけでなくデータ整形が性能を大きく左右します。\n",
    "特にSFTでは、テンプレート設計・損失マスク・系列長管理が品質とコストに直結します。\n",
    "\n",
    "このノートで確認した最小実装を基準に、次は評価設計（自動評価+人手評価）へ進むと実務に接続しやすくなります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}