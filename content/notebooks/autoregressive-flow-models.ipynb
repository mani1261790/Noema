{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import statistics\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 自己回帰モデルとフローベースモデル\n",
        "\n",
        "このノートでは、深層生成モデルの中でもよく比較される2系統を扱います。\n",
        "\n",
        "自己回帰モデルは「順番に条件付き分布を掛ける」設計で、フローベースモデルは「可逆変換で分布を写す」設計です。どちらも尤度に基づく学習ができますが、何が得意で何が苦手かはかなり違います。ここでは最小実装を通して、その違いを手で追える形にします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず全体像を式で揃えます。\n",
        "\n",
        "自己回帰の基本は連鎖律です。\n",
        "\n",
        "$$\n",
        "p(x_{1:D}) = \\prod_{d=1}^{D} p(x_d \\mid x_{<d})\n",
        "$$\n",
        "\n",
        "フローベースモデルの基本は変数変換です。可逆写像 $x=f(z)$ と基底分布 $p_Z$ を使って、\n",
        "\n",
        "$$\n",
        "\\log p_X(x) = \\log p_Z(f^{-1}(x)) + \\log \\left|\\det \\frac{\\partial f^{-1}(x)}{\\partial x}\\right|\n",
        "$$\n",
        "\n",
        "と書けます。尤度を直接計算できる点は共通ですが、\n",
        "- 自己回帰: 尤度計算が簡潔、サンプリングは逐次で遅くなりやすい\n",
        "- フロー: 変換がうまく設計できれば、尤度とサンプリングの両方が扱いやすい\n",
        "\n",
        "という違いがあります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず自己回帰モデルから始めます。ここでは2変数データ `(x1, x2)` を用意し、\n",
        "\n",
        "- `p(x1)`\n",
        "- `p(x2 | x1)`\n",
        "\n",
        "を学習して、連鎖律で同時分布を表現する流れを確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(25)\n",
        "\n",
        "def make_ar_dataset(n=1200):\n",
        "    xs = []\n",
        "    for _ in range(n):\n",
        "        x1 = random.gauss(0.7, 1.1)\n",
        "        x2 = 1.35 * x1 - 0.4 + random.gauss(0.0, 0.55)\n",
        "        xs.append((x1, x2))\n",
        "    return xs\n",
        "\n",
        "\n",
        "def corr(xs):\n",
        "    x1 = [v[0] for v in xs]\n",
        "    x2 = [v[1] for v in xs]\n",
        "    m1 = statistics.mean(x1)\n",
        "    m2 = statistics.mean(x2)\n",
        "    c = sum((a - m1) * (b - m2) for a, b in xs) / len(xs)\n",
        "    s1 = statistics.pstdev(x1)\n",
        "    s2 = statistics.pstdev(x2)\n",
        "    return c / (s1 * s2)\n",
        "\n",
        "\n",
        "data_ar = make_ar_dataset()\n",
        "print('dataset size =', len(data_ar))\n",
        "print('mean x1/x2 =', round(statistics.mean(v[0] for v in data_ar), 4), round(statistics.mean(v[1] for v in data_ar), 4))\n",
        "print('std  x1/x2 =', round(statistics.pstdev(v[0] for v in data_ar), 4), round(statistics.pstdev(v[1] for v in data_ar), 4))\n",
        "print('corr(x1,x2)=', round(corr(data_ar), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここで使う最小自己回帰モデルは次です。\n",
        "\n",
        "- `x1 ~ N(mu1, sigma1^2)`\n",
        "- `x2 | x1 ~ N(a * x1 + b, sigma2^2)`\n",
        "\n",
        "パラメータは `theta = [mu1, log_sigma1, a, b, log_sigma2]` とし、負の対数尤度（NLL）を最小化します。`log_sigma` を直接学習するのは、分散が必ず正になるようにするためです。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def clamp_log_sigma(v):\n",
        "    return max(-4.0, min(3.5, v))\n",
        "\n",
        "\n",
        "def sigma_from_log(log_sigma):\n",
        "    return math.exp(clamp_log_sigma(log_sigma))\n",
        "\n",
        "\n",
        "def log_normal(x, mu, sigma):\n",
        "    return -0.5 * math.log(2.0 * math.pi) - math.log(sigma) - 0.5 * ((x - mu) / sigma) ** 2\n",
        "\n",
        "\n",
        "def nll_ar(theta, dataset):\n",
        "    mu1, log_s1, a, b, log_s2 = theta\n",
        "    s1 = sigma_from_log(log_s1)\n",
        "    s2 = sigma_from_log(log_s2)\n",
        "\n",
        "    total = 0.0\n",
        "    for x1, x2 in dataset:\n",
        "        lp1 = log_normal(x1, mu1, s1)\n",
        "        mu2 = a * x1 + b\n",
        "        lp2 = log_normal(x2, mu2, s2)\n",
        "        total += -(lp1 + lp2)\n",
        "    return total / len(dataset)\n",
        "\n",
        "\n",
        "def finite_diff_grad(fn, params, h=1e-4):\n",
        "    grads = []\n",
        "    for i in range(len(params)):\n",
        "        p1 = params[:]\n",
        "        p2 = params[:]\n",
        "        p1[i] += h\n",
        "        p2[i] -= h\n",
        "        grads.append((fn(p1) - fn(p2)) / (2.0 * h))\n",
        "    return grads\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_ar(theta_init, dataset, steps=240, lr=0.04, log_every=30):\n",
        "    theta = theta_init[:]\n",
        "    history = []\n",
        "\n",
        "    for step in range(steps + 1):\n",
        "        grad = finite_diff_grad(lambda th: nll_ar(th, dataset), theta)\n",
        "        theta = [p - lr * g for p, g in zip(theta, grad)]\n",
        "\n",
        "        # log_sigma の発散抑制\n",
        "        theta[1] = clamp_log_sigma(theta[1])\n",
        "        theta[4] = clamp_log_sigma(theta[4])\n",
        "\n",
        "        if step % log_every == 0:\n",
        "            history.append((step, nll_ar(theta, dataset), theta[:]))\n",
        "\n",
        "    return theta, history\n",
        "\n",
        "\n",
        "def sample_ar(theta, n=1500):\n",
        "    mu1, log_s1, a, b, log_s2 = theta\n",
        "    s1 = sigma_from_log(log_s1)\n",
        "    s2 = sigma_from_log(log_s2)\n",
        "    out = []\n",
        "    for _ in range(n):\n",
        "        x1 = random.gauss(mu1, s1)\n",
        "        x2 = random.gauss(a * x1 + b, s2)\n",
        "        out.append((x1, x2))\n",
        "    return out\n",
        "\n",
        "\n",
        "theta0 = [0.0, 0.0, 0.2, 0.0, 0.0]\n",
        "print('initial nll =', round(nll_ar(theta0, data_ar), 5))\n",
        "\n",
        "trained_ar, ar_hist = train_ar(theta0, data_ar)\n",
        "for step, value, th in ar_hist:\n",
        "    print(f'step={step:03d}', 'nll=', round(value, 5), 'theta=', [round(v, 4) for v in th])\n",
        "\n",
        "print('final nll =', round(nll_ar(trained_ar, data_ar), 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(25)\n",
        "synthetic_ar = sample_ar(trained_ar)\n",
        "\n",
        "print('AR data mean x1/x2 =', round(statistics.mean(v[0] for v in data_ar), 4), round(statistics.mean(v[1] for v in data_ar), 4))\n",
        "print('AR gen  mean x1/x2 =', round(statistics.mean(v[0] for v in synthetic_ar), 4), round(statistics.mean(v[1] for v in synthetic_ar), 4))\n",
        "print('AR data std  x1/x2 =', round(statistics.pstdev(v[0] for v in data_ar), 4), round(statistics.pstdev(v[1] for v in data_ar), 4))\n",
        "print('AR gen  std  x1/x2 =', round(statistics.pstdev(v[0] for v in synthetic_ar), 4), round(statistics.pstdev(v[1] for v in synthetic_ar), 4))\n",
        "print('AR data corr       =', round(corr(data_ar), 4))\n",
        "print('AR gen  corr       =', round(corr(synthetic_ar), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここまでで、自己回帰モデルが「条件付き分布を積む」ことで相関構造を学べることを確認できました。\n",
        "\n",
        "次にフローベースモデルへ移ります。フローは「分かりやすい分布 `z` を可逆変換で `x` に写す」設計です。RealNVPで中心になるのは affine coupling で、Jacobian の行列式が計算しやすいように変換を組みます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2次元の最小 affine coupling を使います。\n",
        "\n",
        "$$\n",
        "y_1 = z_1\n",
        "$$\n",
        "$$\n",
        "y_2 = z_2 \\cdot \\exp(s(z_1)) + t(z_1)\n",
        "$$\n",
        "\n",
        "ここで `s(z1)=a_s z1 + b_s`, `t(z1)=a_t z1 + b_t` と置くと、逆変換が明示的に書けます。\n",
        "\n",
        "$$\n",
        "z_1 = y_1,\n",
        "\\quad z_2 = (y_2 - t(y_1)) \\exp(-s(y_1))\n",
        "$$\n",
        "\n",
        "このとき Jacobian が計算しやすいのが重要です。`y1=z1` なので Jacobian は三角構造になり、対角要素は `1` と `exp(s(z1))` になります。したがって行列式は `exp(s(z1))`、log-det は `s(z1)` だけで計算できます。\n",
        "\n",
        "これが RealNVP 系で「可逆だが尤度計算も軽い」理由です。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def standard_normal_logpdf(z1, z2):\n",
        "    return -math.log(2.0 * math.pi) - 0.5 * (z1 * z1 + z2 * z2)\n",
        "\n",
        "\n",
        "def safe_exp(v):\n",
        "    # 数値発散防止\n",
        "    return math.exp(max(-5.0, min(5.0, v)))\n",
        "\n",
        "\n",
        "def coupling_forward(z1, z2, theta):\n",
        "    a_s, b_s, a_t, b_t = theta\n",
        "    s = a_s * z1 + b_s\n",
        "    t = a_t * z1 + b_t\n",
        "    x1 = z1\n",
        "    x2 = z2 * safe_exp(s) + t\n",
        "    log_det = s\n",
        "    return x1, x2, log_det\n",
        "\n",
        "\n",
        "def coupling_inverse(x1, x2, theta):\n",
        "    a_s, b_s, a_t, b_t = theta\n",
        "    s = a_s * x1 + b_s\n",
        "    t = a_t * x1 + b_t\n",
        "    z1 = x1\n",
        "    z2 = (x2 - t) / safe_exp(s)\n",
        "    log_det_inv = -s\n",
        "    return z1, z2, log_det_inv\n",
        "\n",
        "\n",
        "# 可逆性チェック\n",
        "theta_check = [0.55, -0.18, 1.1, 0.35]\n",
        "z1, z2 = 0.7, -1.2\n",
        "x1, x2, ld = coupling_forward(z1, z2, theta_check)\n",
        "z1_hat, z2_hat, ld_inv = coupling_inverse(x1, x2, theta_check)\n",
        "\n",
        "print('forward  :', round(x1, 6), round(x2, 6), 'logdet=', round(ld, 6))\n",
        "print('inverse  :', round(z1_hat, 6), round(z2_hat, 6), 'logdet_inv=', round(ld_inv, 6))\n",
        "print('recovery error =', round(abs(z1 - z1_hat) + abs(z2 - z2_hat), 12))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次のセルでは、既知の変換 `teacher_theta` を使って合成観測データを作ります。意図は、学習後に `theta` が `teacher_theta` に近づくかを確認することです。\n",
        "\n",
        "手順は `z ~ N(0,I)` をサンプルし、`x = f_teacher(z)` を作るだけです。実務では `x` しか観測できませんが、この教材では正解変換を知っている設定にして、フロー学習が「逆写像を通じた尤度最大化」で何をしているかを見やすくしています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(25)\n",
        "\n",
        "# 教師変換で2次元データを作る（実務では観測データに相当）\n",
        "teacher_theta = [0.8, -0.25, 1.35, 0.4]\n",
        "\n",
        "def make_flow_dataset(n=1500):\n",
        "    out = []\n",
        "    for _ in range(n):\n",
        "        z1 = random.gauss(0.0, 1.0)\n",
        "        z2 = random.gauss(0.0, 1.0)\n",
        "        x1, x2, _ = coupling_forward(z1, z2, teacher_theta)\n",
        "        out.append((x1, x2))\n",
        "    return out\n",
        "\n",
        "\n",
        "data_flow = make_flow_dataset()\n",
        "print('flow dataset size =', len(data_flow))\n",
        "print('mean x1/x2 =', round(statistics.mean(v[0] for v in data_flow), 4), round(statistics.mean(v[1] for v in data_flow), 4))\n",
        "print('std  x1/x2 =', round(statistics.pstdev(v[0] for v in data_flow), 4), round(statistics.pstdev(v[1] for v in data_flow), 4))\n",
        "print('corr(x1,x2)=', round(corr(data_flow), 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def nll_flow(theta, dataset):\n",
        "    total = 0.0\n",
        "    for x1, x2 in dataset:\n",
        "        z1, z2, log_det_inv = coupling_inverse(x1, x2, theta)\n",
        "        lp = standard_normal_logpdf(z1, z2) + log_det_inv\n",
        "        total += -lp\n",
        "    return total / len(dataset)\n",
        "\n",
        "\n",
        "def train_flow(theta_init, dataset, steps=260, lr=0.03, log_every=40):\n",
        "    theta = theta_init[:]\n",
        "    history = []\n",
        "\n",
        "    for step in range(steps + 1):\n",
        "        grad = finite_diff_grad(lambda th: nll_flow(th, dataset), theta)\n",
        "        theta = [p - lr * g for p, g in zip(theta, grad)]\n",
        "\n",
        "        # sの線形係数が暴れすぎないように軽く制限\n",
        "        theta[0] = max(-2.5, min(2.5, theta[0]))\n",
        "        theta[1] = max(-2.5, min(2.5, theta[1]))\n",
        "\n",
        "        if step % log_every == 0:\n",
        "            history.append((step, nll_flow(theta, dataset), theta[:]))\n",
        "\n",
        "    return theta, history\n",
        "\n",
        "\n",
        "flow_theta0 = [0.05, 0.0, 0.1, 0.0]\n",
        "print('initial flow nll =', round(nll_flow(flow_theta0, data_flow), 5))\n",
        "\n",
        "trained_flow, flow_hist = train_flow(flow_theta0, data_flow)\n",
        "for step, value, th in flow_hist:\n",
        "    print(f'step={step:03d}', 'nll=', round(value, 5), 'theta=', [round(v, 4) for v in th])\n",
        "\n",
        "print('final flow nll =', round(nll_flow(trained_flow, data_flow), 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sample_flow(theta, n=1500):\n",
        "    out = []\n",
        "    for _ in range(n):\n",
        "        z1 = random.gauss(0.0, 1.0)\n",
        "        z2 = random.gauss(0.0, 1.0)\n",
        "        x1, x2, _ = coupling_forward(z1, z2, theta)\n",
        "        out.append((x1, x2))\n",
        "    return out\n",
        "\n",
        "\n",
        "random.seed(25)\n",
        "synthetic_flow = sample_flow(trained_flow)\n",
        "\n",
        "print('FLOW data mean x1/x2 =', round(statistics.mean(v[0] for v in data_flow), 4), round(statistics.mean(v[1] for v in data_flow), 4))\n",
        "print('FLOW gen  mean x1/x2 =', round(statistics.mean(v[0] for v in synthetic_flow), 4), round(statistics.mean(v[1] for v in synthetic_flow), 4))\n",
        "print('FLOW data std  x1/x2 =', round(statistics.pstdev(v[0] for v in data_flow), 4), round(statistics.pstdev(v[1] for v in data_flow), 4))\n",
        "print('FLOW gen  std  x1/x2 =', round(statistics.pstdev(v[0] for v in synthetic_flow), 4), round(statistics.pstdev(v[1] for v in synthetic_flow), 4))\n",
        "print('FLOW data corr       =', round(corr(data_flow), 4))\n",
        "print('FLOW gen  corr       =', round(corr(synthetic_flow), 4))\n",
        "print('learned theta        =', [round(v, 4) for v in trained_flow])\n",
        "print('teacher theta        =', [round(v, 4) for v in teacher_theta])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここで使った2次元 coupling は、RealNVP の最小断面です。実際の画像タスクでは、\n",
        "\n",
        "- 次元を分割するマスク（checkerboard / channel-wise）\n",
        "- 複数の coupling layer をスタック\n",
        "- squeeze / unsqueeze で空間・チャネルを組み替え\n",
        "\n",
        "という構成で表現力を上げます。重要なのは、どれだけ複雑にしても「可逆性」と「log-det が計算できる設計」を保つことです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここでは RealNVP で使うマスク機構を4次元ベクトルで模擬します。\n",
        "\n",
        "`mask=1` の次元はその層では固定し、`mask=0` の次元だけを affine 変換します。固定側の値から `cond`（条件コンテキスト）を作り、その `cond` を使って scale/shift を決めるのが coupling の基本です。\n",
        "\n",
        "`mask_a` と `mask_b` を交互に使う理由は、毎層で固定される次元を入れ替え、最終的にすべての次元を更新可能にするためです。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def masked_affine_coupling_forward(x, mask, theta):\n",
        "    # x: 4次元ベクトル\n",
        "    # mask=1 の次元は固定、mask=0 の次元を変換\n",
        "    a_s, b_s, a_t, b_t = theta\n",
        "    x_fixed = [x[i] for i in range(4) if mask[i] == 1]\n",
        "    cond = sum(x_fixed) / len(x_fixed)\n",
        "\n",
        "    s = a_s * cond + b_s\n",
        "    t = a_t * cond + b_t\n",
        "    scale = safe_exp(s)\n",
        "\n",
        "    y = x[:]\n",
        "    log_det = 0.0\n",
        "    for i in range(4):\n",
        "        if mask[i] == 0:\n",
        "            y[i] = x[i] * scale + t\n",
        "            log_det += s\n",
        "    return y, log_det\n",
        "\n",
        "\n",
        "def masked_affine_coupling_inverse(y, mask, theta):\n",
        "    a_s, b_s, a_t, b_t = theta\n",
        "    y_fixed = [y[i] for i in range(4) if mask[i] == 1]\n",
        "    cond = sum(y_fixed) / len(y_fixed)\n",
        "\n",
        "    s = a_s * cond + b_s\n",
        "    t = a_t * cond + b_t\n",
        "    scale = safe_exp(s)\n",
        "\n",
        "    x = y[:]\n",
        "    log_det_inv = 0.0\n",
        "    for i in range(4):\n",
        "        if mask[i] == 0:\n",
        "            x[i] = (y[i] - t) / scale\n",
        "            log_det_inv -= s\n",
        "    return x, log_det_inv\n",
        "\n",
        "\n",
        "mask_a = [1, 0, 1, 0]\n",
        "mask_b = [0, 1, 0, 1]\n",
        "vec = [0.3, -1.0, 0.8, 1.2]\n",
        "params = [0.7, -0.1, 0.9, 0.2]\n",
        "\n",
        "y1, ld1 = masked_affine_coupling_forward(vec, mask_a, params)\n",
        "y2, ld2 = masked_affine_coupling_forward(y1, mask_b, params)\n",
        "\n",
        "x1, ild2 = masked_affine_coupling_inverse(y2, mask_b, params)\n",
        "x0, ild1 = masked_affine_coupling_inverse(x1, mask_a, params)\n",
        "\n",
        "print('input vec     =', [round(v, 6) for v in vec])\n",
        "print('after 2 flows =', [round(v, 6) for v in y2])\n",
        "print('recovered vec =', [round(v, 6) for v in x0])\n",
        "print('recovery err  =', round(sum(abs(a - b) for a, b in zip(vec, x0)), 12))\n",
        "print('logdet fwd/inv=', round(ld1 + ld2, 6), round(ild1 + ild2, 6))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ざっくりした計算コスト観察（教材用の粗い比較）\n",
        "def benchmark_ar(theta, num=2000):\n",
        "    t0 = time.time()\n",
        "    _ = sample_ar(theta, n=num)\n",
        "    t1 = time.time()\n",
        "    return t1 - t0\n",
        "\n",
        "\n",
        "def benchmark_flow(theta, num=2000):\n",
        "    t0 = time.time()\n",
        "    _ = sample_flow(theta, n=num)\n",
        "    t1 = time.time()\n",
        "    return t1 - t0\n",
        "\n",
        "\n",
        "ar_sec = benchmark_ar(trained_ar)\n",
        "flow_sec = benchmark_flow(trained_flow)\n",
        "print('sampling time (AR)   =', round(ar_sec, 5), 'sec')\n",
        "print('sampling time (Flow) =', round(flow_sec, 5), 'sec')\n",
        "print('note: この比較は2次元toyなので、実務速度の結論そのものには使わない')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "このノートの要点は3つです。\n",
        "\n",
        "1つ目は、自己回帰モデルは連鎖律で設計が明確で、条件付き関係を素直に書けることです。2つ目は、フローベースモデルは可逆性と log-det 計算可能性を守ることで、尤度とサンプリングの両方を直接扱えることです。3つ目は、どちらが優れているかは問題設定しだいで、モデルの目的（高忠実度サンプリングか、密度推定の安定性か、計算効率か）に合わせて選ぶ必要があることです。\n",
        "\n",
        "RealNVPやMAF/IAFに進むときは、今回の最小実装を「次元を増やしたときに何がボトルネックになるか」という視点で読み替えると、論文の設計意図がかなり理解しやすくなります。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
