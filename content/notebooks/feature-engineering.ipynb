{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 特徴量エンジニアリング\n\nモデルの性能は、アルゴリズム選択だけでなく「どの入力特徴量を作るか」で大きく変わります。\n特徴量エンジニアリングは、元データをモデルが学びやすい表現に変換する作業です。\n\nこのノートでは、欠損値・カテゴリ変数・スケーリング・非線形変換・交互作用を、\n実務で使う `Pipeline` / `ColumnTransformer` と結びつけて確認します。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures, FunctionTransformer\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\n\nsns.set_theme(style=\"whitegrid\", context=\"notebook\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 元データを観察する\n\nまずは「どんな列があるか」を確認します。\n特徴量設計の第一歩は、モデル前にデータの意味と型を把握することです。\n\n以下のデータは教材用の疑似データです。\n- `area`, `rooms`, `age`, `distance_to_station`, `floor` が価格に影響するという仮定で価格を生成\n- `station` は立地プレミアムを表すカテゴリ列\n- `noise` は観測できない要因（内装、周辺環境など）をまとめた誤差\n\nまた、実務に寄せるために数値列とカテゴリ列へ意図的に欠損を入れています。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "rng = np.random.default_rng(42)\nn = 600\n\narea = rng.normal(65, 18, n).clip(20, 150)\nrooms = rng.integers(1, 6, n)\nage = rng.integers(0, 40, n)\ndistance = rng.normal(18, 8, n).clip(1, 45)\nstation = rng.choice([\"A\", \"B\", \"C\", \"D\"], size=n, p=[0.30, 0.30, 0.25, 0.15])\nfloor = rng.integers(1, 16, n)\n\nstation_effect = pd.Series(station).map({\"A\": 180, \"B\": 120, \"C\": 60, \"D\": 20}).to_numpy()\nnoise = rng.normal(0, 35, n)\nprice = 45 + 4.2 * area + 16 * rooms - 2.3 * age - 3.8 * distance + station_effect + 2.8 * floor + noise\n\ndf = pd.DataFrame({\n    \"area\": area,\n    \"rooms\": rooms,\n    \"age\": age,\n    \"distance_to_station\": distance,\n    \"station\": station,\n    \"floor\": floor,\n    \"price\": price,\n})\n\n# 欠損を意図的に作る\nmissing_idx_num = rng.choice(df.index, size=35, replace=False)\nmissing_idx_cat = rng.choice(df.index, size=20, replace=False)\ndf.loc[missing_idx_num, \"distance_to_station\"] = np.nan\ndf.loc[missing_idx_cat, \"station\"] = np.nan\n\ndf.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "`info` と `isna` で型と欠損を確認します。\nこの確認を飛ばすと、後段で変換失敗やリークの原因になります。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print(df.info())\nprint(\"\\n欠損数:\\n\", df.isna().sum())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 変換前後で分布を比べる\n\n歪んだ分布に対して対数変換が有効な場面があります。\nここでは駅距離の分布を `log1p` 変換前後で比較します。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, axes = plt.subplots(1, 2, figsize=(10, 3.6))\n\nsns.histplot(df[\"distance_to_station\"], bins=30, ax=axes[0], kde=True)\naxes[0].set_title(\"original distance\")\n\nsns.histplot(np.log1p(df[\"distance_to_station\"]), bins=30, ax=axes[1], kde=True)\naxes[1].set_title(\"log1p(distance)\")\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. ベースライン（最小限の前処理）\n\n最初に単純なベースラインを作ります。\n比較対象があると、後で追加した特徴量の効果を正しく評価できます。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X = df.drop(columns=[\"price\"])\ny = df[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nnum_cols = [\"area\", \"rooms\", \"age\", \"distance_to_station\", \"floor\"]\ncat_cols = [\"station\"]\n\nbaseline_preprocess = ColumnTransformer([\n    (\"num\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler()),\n    ]), num_cols),\n    (\"cat\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]), cat_cols),\n])\n\nbaseline_model = Pipeline([\n    (\"preprocess\", baseline_preprocess),\n    (\"regressor\", LinearRegression()),\n])\n\nbaseline_model.fit(X_train, y_train)\nbaseline_pred = baseline_model.predict(X_test)\n\nprint(f\"baseline MAE: {mean_absolute_error(y_test, baseline_pred):.2f}\")\nprint(f\"baseline R2 : {r2_score(y_test, baseline_pred):.3f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 特徴量を追加する\n\n次に、ドメイン知識を反映した特徴を追加します。\n\n- `area_per_room`: 1部屋あたり面積。広さと間取りを1つの軸で表せる\n- `is_new`: 築浅フラグ。築年数の閾値効果を捉えやすくする\n- `log_distance`: 駅距離の対数。長い尾を圧縮して線形モデルで扱いやすくする\n\nこうした手作業特徴は、モデルにとって有効な表現を直接与える手段です。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def add_features(frame: pd.DataFrame) -> pd.DataFrame:\n    out = frame.copy()\n    out[\"area_per_room\"] = out[\"area\"] / np.maximum(out[\"rooms\"], 1)\n    out[\"is_new\"] = (out[\"age\"] <= 5).astype(int)\n    out[\"log_distance\"] = np.log1p(out[\"distance_to_station\"])\n    return out\n\nfeature_num_cols = [\n    \"area\", \"rooms\", \"age\", \"distance_to_station\", \"floor\",\n    \"area_per_room\", \"is_new\", \"log_distance\"\n]\nfeature_cat_cols = [\"station\"]\n\nfeature_preprocess = ColumnTransformer([\n    (\"num\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler()),\n    ]), feature_num_cols),\n    (\"cat\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]), feature_cat_cols),\n])\n\nfeature_model = Pipeline([\n    (\"feature_builder\", FunctionTransformer(add_features, validate=False)),\n    (\"preprocess\", feature_preprocess),\n    (\"regressor\", Ridge(alpha=1.0, random_state=42)),\n])\n\nfeature_model.fit(X_train, y_train)\nfeature_pred = feature_model.predict(X_test)\n\nprint(f\"feature MAE: {mean_absolute_error(y_test, feature_pred):.2f}\")\nprint(f\"feature R2 : {r2_score(y_test, feature_pred):.3f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 交互作用特徴を試す\n\n線形モデルでは、列同士の掛け算項（交互作用）を入れると表現力が上がる場合があります。\n`PolynomialFeatures(degree=2)` は、2次の項（例: `area^2`）や交互作用項（例: `area * rooms`）を自動生成します。\n\n次元が増えやすいので、正則化や検証とセットで使います。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "poly_num_cols = [\"area\", \"rooms\", \"age\", \"distance_to_station\", \"floor\"]\n\npoly_preview = PolynomialFeatures(degree=2, include_bias=False)\npoly_preview.fit(X_train[poly_num_cols])\npoly_names = poly_preview.get_feature_names_out(poly_num_cols)\nprint(\"num of polynomial features:\", len(poly_names))\nprint(\"sample names:\", poly_names[:12])\n\npoly_preprocess = ColumnTransformer([\n    (\"num_poly\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n        (\"scaler\", StandardScaler()),\n    ]), poly_num_cols),\n    (\"cat\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]), cat_cols),\n])\n\npoly_model = Pipeline([\n    (\"preprocess\", poly_preprocess),\n    (\"regressor\", Ridge(alpha=2.0, random_state=42)),\n])\n\npoly_model.fit(X_train, y_train)\npoly_pred = poly_model.predict(X_test)\n\nprint(f\"poly MAE: {mean_absolute_error(y_test, poly_pred):.2f}\")\nprint(f\"poly R2 : {r2_score(y_test, poly_pred):.3f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 木モデルとの比較\n\n木ベースモデルはスケーリングに依存しにくく、\n非線形関係も自動で取り込みやすいので、特徴量設計との相性を確認しやすいです。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "tree_preprocess = ColumnTransformer([\n    (\"num\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n    ]), feature_num_cols),\n    (\"cat\", Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]), feature_cat_cols),\n])\n\ntree_model = Pipeline([\n    (\"feature_builder\", FunctionTransformer(add_features, validate=False)),\n    (\"preprocess\", tree_preprocess),\n    (\"regressor\", RandomForestRegressor(\n        n_estimators=300,\n        max_depth=10,\n        min_samples_leaf=2,\n        random_state=42,\n        n_jobs=-1,\n    )),\n])\n\ntree_model.fit(X_train, y_train)\ntree_pred = tree_model.predict(X_test)\n\nprint(f\"random forest MAE: {mean_absolute_error(y_test, tree_pred):.2f}\")\nprint(f\"random forest R2 : {r2_score(y_test, tree_pred):.3f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. リークを避けるための注意\n\n特徴量エンジニアリングで最も危険なのはデータリークです。\n例えば「テストデータを見てから欠損補完値を決める」「目的変数に依存する列を特徴に入れる」は禁止です。\n\n`Pipeline` / `ColumnTransformer` を使って訓練データの手順を固定すれば、\n推論時にも同じ変換を安全に適用できます。\n\n次のセルでは、4つのモデルを 5-fold CV で同一基準比較します。\n`cross_val_score` の `neg_mean_absolute_error` は「大きいほど良い」形式に合わせるため負符号付きで返るため、\n表示時に `-scores.mean()` として通常の MAE（小さいほど良い）へ戻しています。"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "models = {\n    \"baseline_linear\": baseline_model,\n    \"feature_ridge\": feature_model,\n    \"poly_ridge\": poly_model,\n    \"tree_rf\": tree_model,\n}\n\nrows = []\nfor name, model in models.items():\n    # すべての前処理（特徴量追加を含む）をパイプライン内で実行\n    scores = cross_val_score(\n        model,\n        X,\n        y,\n        cv=5,\n        scoring=\"neg_mean_absolute_error\",\n        n_jobs=-1,\n    )\n    rows.append({\n        \"model\": name,\n        \"cv_mae_mean\": -scores.mean(),\n        \"cv_mae_std\": scores.std(),\n    })\n\npd.DataFrame(rows).sort_values(\"cv_mae_mean\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## まとめ\n\n特徴量エンジニアリングでは、\n\n1. データの型と欠損を把握する\n2. ベースラインを作る\n3. 意味のある特徴を追加して比較する\n4. CVで効果を確認する\n\nという順序を崩さないことが重要です。\n\n精度改善だけでなく、リークを避けて再現可能な前処理を作ることが、実務で最も価値のあるポイントです。"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
