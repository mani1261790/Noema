{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "random.seed(21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在変数モデルと混合モデル\n",
    "\n",
    "このノートの主題は、見えていない原因（潜在変数）を導入すると、単純モデルでは説明しにくいデータをどう扱えるようになるか、です。分類ではなく「分布そのもの」を学ぶ生成モデルの視点で、EMアルゴリズムまでを一気につなげます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず「なぜ潜在変数が必要か」を直感で押さえます。\n",
    "\n",
    "同じコイン投げの観測列に見えても、実は2種類のコインが混ざっている場合があります。観測は表裏だけで、どのコインを使ったかは記録されていない。この「見えていないラベル」が潜在変数です。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_coin_mixture(n=40, pi=0.6, p_a=0.8, p_b=0.3):\n",
    "    data = []\n",
    "    latent = []\n",
    "    for _ in range(n):\n",
    "        z = 0 if random.random() < pi else 1  # 0: coin A, 1: coin B\n",
    "        x = 1 if random.random() < (p_a if z == 0 else p_b) else 0\n",
    "        data.append(x)\n",
    "        latent.append(z)\n",
    "    return data, latent\n",
    "\n",
    "\n",
    "data, latent = generate_coin_mixture(n=50, pi=0.65, p_a=0.82, p_b=0.28)\n",
    "print('observed heads ratio =', round(sum(data) / len(data), 3))\n",
    "print('true latent count A/B =', latent.count(0), latent.count(1))\n",
    "print('first 20 observations =', data[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし潜在変数 `z` を無視すると、1種類のコイン確率しか推定できません。すると「2種類が混ざっている」という構造情報を失います。混合モデルは、まさにこの構造を扱うためのモデルです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 混合モデルの数式イメージ\n",
    "\n",
    "混合モデルは「まず潜在クラス `z` を引いて、次に `x` を生成する」という2段階で書けます。\n",
    "\n",
    "- `p(z=k) = pi_k`（混合比）\n",
    "- `p(x|z=k)`（成分分布）\n",
    "- `p(x) = sum_k pi_k p(x|z=k)`（周辺化して観測分布）\n",
    "\n",
    "問題は、観測時には `z` が見えないことです。ここでEMが効きます。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bernoulli(x, p):\n",
    "    p = min(max(p, 1e-9), 1 - 1e-9)\n",
    "    return p if x == 1 else (1 - p)\n",
    "\n",
    "\n",
    "def responsibility_2comp(x, pi, p0, p1):\n",
    "    # gamma0 = p(z=0|x)\n",
    "    num0 = pi * bernoulli(x, p0)\n",
    "    num1 = (1 - pi) * bernoulli(x, p1)\n",
    "    den = num0 + num1 + 1e-12\n",
    "    return num0 / den, num1 / den\n",
    "\n",
    "\n",
    "example_x = [0, 1, 1, 0, 1]\n",
    "for x in example_x:\n",
    "    g0, g1 = responsibility_2comp(x, pi=0.5, p0=0.8, p1=0.2)\n",
    "    print(f'x={x} -> gamma(z=0|x)={g0:.3f}, gamma(z=1|x)={g1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gamma(z=k|x)` を負担率（responsibility）と呼びます。硬いクラスタ割当（0か1か）ではなく、確率として割り当てるのがポイントです。これにより境界点も自然に扱えます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EMアルゴリズム（混合ベルヌーイ）\n",
    "\n",
    "EMは次を反復します。\n",
    "\n",
    "- E-step: 現在のパラメータで負担率 `gamma` を計算\n",
    "- M-step: `gamma` を重みとしてパラメータを更新\n",
    "\n",
    "この流れは「見えない `z` を埋める」→「埋めたと仮定して最尤更新する」の往復です。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def e_step_binary(data, pi, p0, p1):\n",
    "    gammas = []\n",
    "    for x in data:\n",
    "        g0, _ = responsibility_2comp(x, pi, p0, p1)\n",
    "        gammas.append(g0)\n",
    "    return gammas\n",
    "\n",
    "\n",
    "def m_step_binary(data, gammas):\n",
    "    n = len(data)\n",
    "    sum_g = sum(gammas)\n",
    "    sum_1mg = n - sum_g\n",
    "\n",
    "    pi_new = sum_g / n\n",
    "    p0_new = sum(g * x for g, x in zip(gammas, data)) / max(sum_g, 1e-9)\n",
    "    p1_new = sum((1 - g) * x for g, x in zip(gammas, data)) / max(sum_1mg, 1e-9)\n",
    "\n",
    "    # 数値安定\n",
    "    p0_new = min(max(p0_new, 1e-6), 1 - 1e-6)\n",
    "    p1_new = min(max(p1_new, 1e-6), 1 - 1e-6)\n",
    "    pi_new = min(max(pi_new, 1e-6), 1 - 1e-6)\n",
    "    return pi_new, p0_new, p1_new\n",
    "\n",
    "\n",
    "def loglik_binary(data, pi, p0, p1):\n",
    "    ll = 0.0\n",
    "    for x in data:\n",
    "        prob = pi * bernoulli(x, p0) + (1 - pi) * bernoulli(x, p1)\n",
    "        ll += math.log(max(prob, 1e-12))\n",
    "    return ll\n",
    "\n",
    "\n",
    "# 初期値はわざとずらす\n",
    "pi, p0, p1 = 0.5, 0.55, 0.45\n",
    "trace = []\n",
    "for t in range(20):\n",
    "    g = e_step_binary(data, pi, p0, p1)\n",
    "    pi, p0, p1 = m_step_binary(data, g)\n",
    "    ll = loglik_binary(data, pi, p0, p1)\n",
    "    trace.append(ll)\n",
    "    print(f'iter={t:02d} pi={pi:.3f} p0={p0:.3f} p1={p1:.3f} ll={ll:.3f}')\n",
    "\n",
    "print('log-likelihood monotonic non-decrease check =', all(trace[i] <= trace[i+1] + 1e-9 for i in range(len(trace)-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMは局所解に落ちる可能性があるため、初期値を変えて複数回走らせるのが実務では基本です。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_em_binary(data, n_iter=30):\n",
    "    pi = random.uniform(0.2, 0.8)\n",
    "    p0 = random.uniform(0.1, 0.9)\n",
    "    p1 = random.uniform(0.1, 0.9)\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        g = e_step_binary(data, pi, p0, p1)\n",
    "        pi, p0, p1 = m_step_binary(data, g)\n",
    "\n",
    "    ll = loglik_binary(data, pi, p0, p1)\n",
    "    return ll, (pi, p0, p1)\n",
    "\n",
    "\n",
    "trials = []\n",
    "for _ in range(8):\n",
    "    trials.append(run_em_binary(data))\n",
    "\n",
    "trials.sort(key=lambda x: x[0], reverse=True)\n",
    "print('best run ll =', round(trials[0][0], 4), 'params =', tuple(round(v, 4) for v in trials[0][1]))\n",
    "print('worst run ll =', round(trials[-1][0], 4), 'params =', tuple(round(v, 4) for v in trials[-1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで重要な注意があります。上の2コイン例は「1サンプル=1ビット観測」なので、\n",
    "\\(P(x=1)=\\pi p_0 + (1-\\pi)p_1\\) の1本しか観測制約がなく、\\((\\pi,p_0,p_1)\\) は一般に一意に同定できません。\n",
    "\n",
    "そのため、同じ尤度でも異なるパラメータ組が出ます。これはEMのバグではなく、観測情報の不足による同定不能性です。\n",
    "実務では多次元特徴や時系列観測を使って情報量を増やし、同定性を改善します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 多次元の混合ベルヌーイ\n",
    "\n",
    "画像の2値化データのように、観測が多次元になると各次元の確率パラメータを成分ごとに持ちます。ここでは長さ6の2値ベクトルで、MNIST前の練習を行います。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_binary_vector_data(n=120):\n",
    "    # 成分0: 前半が1になりやすい, 成分1: 後半が1になりやすい\n",
    "    mu0 = [0.85, 0.75, 0.7, 0.2, 0.15, 0.1]\n",
    "    mu1 = [0.2, 0.25, 0.3, 0.8, 0.75, 0.7]\n",
    "    pi = 0.55\n",
    "\n",
    "    xs = []\n",
    "    zs = []\n",
    "    for _ in range(n):\n",
    "        z = 0 if random.random() < pi else 1\n",
    "        mu = mu0 if z == 0 else mu1\n",
    "        x = [1 if random.random() < p else 0 for p in mu]\n",
    "        xs.append(x)\n",
    "        zs.append(z)\n",
    "    return xs, zs\n",
    "\n",
    "\n",
    "vec_data, vec_latent = make_binary_vector_data(n=160)\n",
    "print('sample x[0:3] =', vec_data[:3])\n",
    "print('latent count =', vec_latent.count(0), vec_latent.count(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bernoulli_vec_prob(x, mu):\n",
    "    prob = 1.0\n",
    "    for xi, p in zip(x, mu):\n",
    "        p = min(max(p, 1e-8), 1 - 1e-8)\n",
    "        prob *= p if xi == 1 else (1 - p)\n",
    "    return prob\n",
    "\n",
    "\n",
    "def e_step_vec(data, pi, mu0, mu1):\n",
    "    g = []\n",
    "    for x in data:\n",
    "        a = pi * bernoulli_vec_prob(x, mu0)\n",
    "        b = (1 - pi) * bernoulli_vec_prob(x, mu1)\n",
    "        g.append(a / max(a + b, 1e-12))\n",
    "    return g\n",
    "\n",
    "\n",
    "def m_step_vec(data, g):\n",
    "    n = len(data)\n",
    "    d = len(data[0])\n",
    "    sum_g = sum(g)\n",
    "    sum_1mg = n - sum_g\n",
    "\n",
    "    pi_new = sum_g / n\n",
    "    mu0 = []\n",
    "    mu1 = []\n",
    "    for j in range(d):\n",
    "        num0 = sum(g[i] * data[i][j] for i in range(n))\n",
    "        num1 = sum((1 - g[i]) * data[i][j] for i in range(n))\n",
    "        mu0.append(min(max(num0 / max(sum_g, 1e-9), 1e-6), 1 - 1e-6))\n",
    "        mu1.append(min(max(num1 / max(sum_1mg, 1e-9), 1e-6), 1 - 1e-6))\n",
    "    return min(max(pi_new, 1e-6), 1 - 1e-6), mu0, mu1\n",
    "\n",
    "\n",
    "def loglik_vec(data, pi, mu0, mu1):\n",
    "    ll = 0.0\n",
    "    for x in data:\n",
    "        p = pi * bernoulli_vec_prob(x, mu0) + (1 - pi) * bernoulli_vec_prob(x, mu1)\n",
    "        ll += math.log(max(p, 1e-12))\n",
    "    return ll\n",
    "\n",
    "\n",
    "pi = 0.5\n",
    "mu0 = [0.6] * 6\n",
    "mu1 = [0.4] * 6\n",
    "for t in range(25):\n",
    "    g = e_step_vec(vec_data, pi, mu0, mu1)\n",
    "    pi, mu0, mu1 = m_step_vec(vec_data, g)\n",
    "\n",
    "print('estimated pi =', round(pi, 3))\n",
    "print('estimated mu0 =', [round(v, 2) for v in mu0])\n",
    "print('estimated mu1 =', [round(v, 2) for v in mu1])\n",
    "print('final ll =', round(loglik_vec(vec_data, pi, mu0, mu1), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 混合ガウス分布（GMM）とEM\n",
    "\n",
    "連続値データでは混合ガウスが基本になります。E-stepで負担率を計算し、M-stepで混合比・平均・共分散を更新します。DGM第2回で扱う中心テーマです。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_gmm_2d(n=240):\n",
    "    # 2成分の簡易データ\n",
    "    params = [\n",
    "        {'pi': 0.5, 'mu': (-1.8, -1.2), 'sigma': (0.55, 0.45)},\n",
    "        {'pi': 0.5, 'mu': (2.2, 1.8), 'sigma': (0.6, 0.5)},\n",
    "    ]\n",
    "\n",
    "    xs = []\n",
    "    zs = []\n",
    "    for _ in range(n):\n",
    "        z = 0 if random.random() < params[0]['pi'] else 1\n",
    "        p = params[z]\n",
    "        x = (\n",
    "            random.gauss(p['mu'][0], p['sigma'][0]),\n",
    "            random.gauss(p['mu'][1], p['sigma'][1]),\n",
    "        )\n",
    "        xs.append(x)\n",
    "        zs.append(z)\n",
    "    return xs, zs\n",
    "\n",
    "\n",
    "gmm_data, gmm_true_z = sample_gmm_2d(n=300)\n",
    "print('first 3 points =', [tuple(round(v, 3) for v in x) for x in gmm_data[:3]])\n",
    "print('true cluster counts =', gmm_true_z.count(0), gmm_true_z.count(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gaussian_pdf_diag(x, mu, var):\n",
    "    # 対角共分散のみ（教育用に簡略化）\n",
    "    v0 = max(var[0], 1e-6)\n",
    "    v1 = max(var[1], 1e-6)\n",
    "    z0 = (x[0] - mu[0]) ** 2 / v0\n",
    "    z1 = (x[1] - mu[1]) ** 2 / v1\n",
    "    coef = 1.0 / (2 * math.pi * math.sqrt(v0 * v1))\n",
    "    return coef * math.exp(-0.5 * (z0 + z1))\n",
    "\n",
    "\n",
    "def e_step_gmm_diag(data, pis, mus, vars_):\n",
    "    gamma = []\n",
    "    for x in data:\n",
    "        probs = [pis[k] * gaussian_pdf_diag(x, mus[k], vars_[k]) for k in range(2)]\n",
    "        s = sum(probs) + 1e-12\n",
    "        gamma.append([p / s for p in probs])\n",
    "    return gamma\n",
    "\n",
    "\n",
    "def m_step_gmm_diag(data, gamma):\n",
    "    n = len(data)\n",
    "    nk = [sum(g[k] for g in gamma) for k in range(2)]\n",
    "    pis = [nk[k] / n for k in range(2)]\n",
    "\n",
    "    mus = []\n",
    "    vars_ = []\n",
    "    for k in range(2):\n",
    "        mx = sum(gamma[i][k] * data[i][0] for i in range(n)) / max(nk[k], 1e-12)\n",
    "        my = sum(gamma[i][k] * data[i][1] for i in range(n)) / max(nk[k], 1e-12)\n",
    "        mus.append((mx, my))\n",
    "\n",
    "        vx = sum(gamma[i][k] * (data[i][0] - mx) ** 2 for i in range(n)) / max(nk[k], 1e-12)\n",
    "        vy = sum(gamma[i][k] * (data[i][1] - my) ** 2 for i in range(n)) / max(nk[k], 1e-12)\n",
    "        vars_.append((max(vx, 1e-4), max(vy, 1e-4)))\n",
    "\n",
    "    return pis, mus, vars_\n",
    "\n",
    "\n",
    "def loglik_gmm_diag(data, pis, mus, vars_):\n",
    "    ll = 0.0\n",
    "    for x in data:\n",
    "        p = 0.0\n",
    "        for k in range(2):\n",
    "            p += pis[k] * gaussian_pdf_diag(x, mus[k], vars_[k])\n",
    "        ll += math.log(max(p, 1e-12))\n",
    "    return ll\n",
    "\n",
    "\n",
    "pis = [0.5, 0.5]\n",
    "mus = [(-0.5, -2.5), (1.0, 2.5)]\n",
    "vars_ = [(1.2, 1.0), (1.0, 1.3)]\n",
    "ll_trace = []\n",
    "\n",
    "for t in range(30):\n",
    "    gamma = e_step_gmm_diag(gmm_data, pis, mus, vars_)\n",
    "    pis, mus, vars_ = m_step_gmm_diag(gmm_data, gamma)\n",
    "    ll = loglik_gmm_diag(gmm_data, pis, mus, vars_)\n",
    "    ll_trace.append(ll)\n",
    "    if t % 5 == 0 or t == 29:\n",
    "        print(f'iter={t:02d} pis={[round(v,3) for v in pis]} mus={[tuple(round(u,2) for u in m) for m in mus]} ll={ll:.2f}')\n",
    "\n",
    "print('monotonic ll =', all(ll_trace[i] <= ll_trace[i+1] + 1e-8 for i in range(len(ll_trace)-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ソフト割当の例: 境界付近では責任率が0/1に張り付きにくい\n",
    "mid = ((mus[0][0] + mus[1][0]) / 2, (mus[0][1] + mus[1][1]) / 2)\n",
    "probe_points = [\n",
    "    (mid[0], mid[1]),\n",
    "    (mid[0] + 0.4, mid[1] + 0.2),\n",
    "    (mid[0] - 0.4, mid[1] - 0.2),\n",
    "    (mus[0][0], mus[0][1]),\n",
    "    (mus[1][0], mus[1][1]),\n",
    "]\n",
    "for x in probe_points:\n",
    "    g = e_step_gmm_diag([x], pis, mus, vars_)[0]\n",
    "    print('x=', tuple(round(v,2) for v in x), 'responsibility=', [round(v, 3) for v in g])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "責任率が0/1に張り付かない点が重要です。これにより、境界データを無理にハード割当せずに学習できます。K-meansが苦手な場面でGMMが効く理由の1つです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 実務での注意点\n",
    "\n",
    "EMは強力ですが、次の点で失敗しやすいです。\n",
    "\n",
    "- 初期値依存: 局所解に落ちる（複数初期値で比較）\n",
    "- 成分崩壊: 1成分にデータが集中しすぎる\n",
    "- 共分散特異: 分散が極小になって不安定（正則化が必要）\n",
    "- 成分数Kの選択: 大きすぎると過学習\n",
    "\n",
    "AIC/BICや検証データ尤度を使って `K` を決めるのが基本です。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bic(loglik, n_samples, n_params):\n",
    "    return -2 * loglik + n_params * math.log(max(n_samples, 2))\n",
    "\n",
    "\n",
    "n = len(gmm_data)\n",
    "# 2成分・2次元・対角分散の粗いパラメータ数:\n",
    "# pi(1自由度) + mu(2*2) + var(2*2) = 9\n",
    "bic_score = bic(ll_trace[-1], n_samples=n, n_params=9)\n",
    "print('final loglik =', round(ll_trace[-1], 2))\n",
    "print('BIC (rough)  =', round(bic_score, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "潜在変数モデルと混合モデルの核は、「見えない変数を推論しながら分布を学習する」ことです。EMはこの目的に対する最も重要な基本アルゴリズムです。\n",
    "\n",
    "このあとVAEや拡散モデルを学ぶときも、実は同じ構図が続きます。観測されない中間変数をうまく扱うことで、生成の表現力と安定性を上げていく、という流れです。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
