{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM軽量化（圧縮・最適化・効率化）\n",
    "\n",
    "LLMの軽量化は、品質を必要以上に落とさずに、同じ予算でより速く・より長い文脈を扱うための設計です。ここでは、量子化・蒸留・PEFT・推論最適化を、実務で使う判断軸と一緒に整理します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に前提を揃えます。軽量化を考える理由は大きく3つです。\n",
    "\n",
    "1つ目はメモリ制約です。GPUメモリにモデル本体・KV cache・一時バッファが収まらないと、そもそも推論できません。2つ目はレイテンシです。応答が遅いと、対話体験が崩れます。3つ目はコストです。同じ問い合わせ数をさばくなら、1トークンあたりの計算とメモリ転送を下げるほど運用が楽になります。\n",
    "\n",
    "このノートでは「なぜその手法を使うのか」を数字で確認しながら進めます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用語ミニ辞典\n",
    "\n",
    "- KV cache: デコード時に再利用する中間状態。長文になるほどメモリを圧迫しやすい。\n",
    "- PEFT: 一部パラメータだけ更新して学習コストを下げる手法群。\n",
    "- 疎行列カーネル: 0の多い行列を効率計算する実装。無いとpruningが速度に効きにくい。\n",
    "- LLM-as-a-judge: 別モデルで出力品質を採点する手法。自己強化バイアスや位置バイアスがあるため、rubric評価と人手監査を併用する。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. まずはメモリ予算を数字で見る\n",
    "\n",
    "モデルを軽くする話は、だいたい「重みを何bitで持つか」から始まります。まずは重みだけの理想値を計算します。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bytes_for_params(num_params: int, bits: int) -> float:\n",
    "    return num_params * bits / 8\n",
    "\n",
    "\n",
    "def to_gib(num_bytes: float) -> float:\n",
    "    return num_bytes / (1024 ** 3)\n",
    "\n",
    "\n",
    "models = {\n",
    "    '7B': 7_000_000_000,\n",
    "    '13B': 13_000_000_000,\n",
    "}\n",
    "\n",
    "precisions = {\n",
    "    'FP16': 16,\n",
    "    'INT8': 8,\n",
    "    'INT4': 4,\n",
    "}\n",
    "\n",
    "for name, params in models.items():\n",
    "    print(f'--- {name} ({params:,} params) ---')\n",
    "    for p_name, bits in precisions.items():\n",
    "        print(f'{p_name:>4}: {to_gib(bytes_for_params(params, bits)):6.2f} GiB')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ7Bでも、FP16からINT4に落とすと重みメモリは理論上1/4になります。ここで大事なのは、実際の推論では重み以外にKV cacheも効いてくる点です。長文対話ほどKV cacheの影響が大きくなるので、重みだけ見て安心すると詰まります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization（Weight / Activation / KV cache）\n",
    "\n",
    "量子化は、数値の表現bit幅を下げてメモリ帯域と容量を削る手法です。重み量子化が最も普及していますが、長文推論ではKV cache量子化も効きます。さらに学習時にはActivation量子化が効く場面があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kv_cache_bytes(\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    n_layers: int,\n",
    "    n_kv_heads: int,\n",
    "    head_dim: int,\n",
    "    bits: int,\n",
    ") -> float:\n",
    "    # KとVの2本を保持する\n",
    "    elements = batch_size * seq_len * n_layers * n_kv_heads * head_dim * 2\n",
    "    return elements * bits / 8\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'batch_size': 4,\n",
    "    'n_layers': 32,\n",
    "    'n_kv_heads': 8,\n",
    "    'head_dim': 128,\n",
    "}\n",
    "\n",
    "for seq in [1024, 4096, 8192]:\n",
    "    fp16 = to_gib(kv_cache_bytes(seq_len=seq, bits=16, **cfg))\n",
    "    int8 = to_gib(kv_cache_bytes(seq_len=seq, bits=8, **cfg))\n",
    "    int4 = to_gib(kv_cache_bytes(seq_len=seq, bits=4, **cfg))\n",
    "    print(f'seq={seq:>4}: FP16={fp16:5.2f} GiB, INT8={int8:5.2f} GiB, INT4={int4:5.2f} GiB')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def activation_tensor_bytes(batch_size: int, seq_len: int, hidden_size: int, bits: int) -> float:\n",
    "    elements = batch_size * seq_len * hidden_size\n",
    "    return elements * bits / 8\n",
    "\n",
    "\n",
    "act_cfg = {\n",
    "    'batch_size': 8,\n",
    "    'seq_len': 4096,\n",
    "    'hidden_size': 4096,\n",
    "}\n",
    "\n",
    "for bits in [16, 8, 4]:\n",
    "    gib = to_gib(activation_tensor_bytes(bits=bits, **act_cfg))\n",
    "    print(f'activation tensor ({bits:>2}bit) = {gib:5.2f} GiB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation量子化は学習時メモリを下げるのに有効ですが、推論品質への影響が出やすい箇所でもあります。導入時は、レイテンシだけでなく精度劣化を必ず同時評価します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じモデルでも、文脈長を4倍にするとKV cacheもほぼ4倍になります。つまり、長文対応をしたいときの軽量化は「重みだけ」では不十分です。Weight量子化は入口で、運用ではKV cache設計がボトルネックになることが多いです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pruning（Structured / Unstructured）\n",
    "\n",
    "Pruningは使っていない重みを削る手法です。ここで重要なのは、ゼロを増やすだけでは速くならないことです。実行カーネルが疎行列をうまく使えなければ、理論削減と実測速度は一致しません。\n",
    "\n",
    "- Unstructured pruning: 個々の重みを間引く（精度維持しやすいが実装依存）\n",
    "- Structured pruning: 行・列・チャネル単位で削る（速度改善につなげやすい）\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.seed(7)\n",
    "rows, cols = 64, 64\n",
    "weights = [[random.uniform(-1.0, 1.0) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "# Unstructured pruning: 小さい重みを50%ゼロ化\n",
    "flat = sorted(abs(x) for r in weights for x in r)\n",
    "threshold = flat[len(flat) // 2]\n",
    "unstructured_nonzero = sum(1 for r in weights for x in r if abs(x) >= threshold)\n",
    "\n",
    "# Structured pruning: 列ノルムの小さい50%を削除\n",
    "col_norms = []\n",
    "for c in range(cols):\n",
    "    col_norms.append((sum(abs(weights[r][c]) for r in range(rows)), c))\n",
    "col_norms.sort()\n",
    "pruned_cols = set(c for _, c in col_norms[: cols // 2])\n",
    "structured_nonzero = rows * (cols - len(pruned_cols))\n",
    "\n",
    "print('total params        =', rows * cols)\n",
    "print('unstructured kept   =', unstructured_nonzero)\n",
    "print('structured kept     =', structured_nonzero)\n",
    "print('structured speed proxy (active width ratio)=', round((cols - len(pruned_cols)) / cols, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructuredは保持重みを柔軟に選べる一方、ハードウェアでそのまま速くなるとは限りません。Structuredは削る自由度は落ちますが、行列サイズそのものが小さくなるため、実運用で速度向上に結びつけやすくなります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Distillation\n",
    "\n",
    "蒸留は「大きいモデルのふるまい」を小さいモデルに移す方法です。なぜ嬉しいかというと、推論は小型モデルで回しつつ、教師モデルの一般化傾向を取り込めるからです。\n",
    "\n",
    "- `T`（温度）: 分布をなだらかにして、教師の暗黙知を見えやすくする。\n",
    "- `T^2` 係数: 温度を入れたときに勾配スケールが崩れすぎないよう補正する。\n",
    "- `alpha`: 正解ラベル重視（hard）と教師分布重視（soft）の比率を決める。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax(logits: List[float], temperature: float = 1.0) -> List[float]:\n",
    "    scaled = [x / temperature for x in logits]\n",
    "    m = max(scaled)\n",
    "    exps = [math.exp(x - m) for x in scaled]\n",
    "    s = sum(exps)\n",
    "    return [e / s for e in exps]\n",
    "\n",
    "\n",
    "def kl_div(p: List[float], q: List[float]) -> float:\n",
    "    eps = 1e-12\n",
    "    out = 0.0\n",
    "    for pi, qi in zip(p, q):\n",
    "        pi = max(pi, eps)\n",
    "        qi = max(qi, eps)\n",
    "        out += pi * math.log(pi / qi)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cross_entropy(probs: List[float], gold_index: int) -> float:\n",
    "    return -math.log(max(probs[gold_index], 1e-12))\n",
    "\n",
    "\n",
    "teacher_logits = [3.2, 2.4, -0.8, 0.1]\n",
    "student_logits = [2.0, 1.9, -0.1, 0.2]\n",
    "gold = 0\n",
    "\n",
    "T = 2.0\n",
    "alpha = 0.4  # hard target の重み\n",
    "\n",
    "teacher_soft = softmax(teacher_logits, temperature=T)\n",
    "student_soft = softmax(student_logits, temperature=T)\n",
    "student_hard = softmax(student_logits, temperature=1.0)\n",
    "\n",
    "hard_loss = cross_entropy(student_hard, gold)\n",
    "soft_loss = (T ** 2) * kl_div(teacher_soft, student_soft)\n",
    "loss = alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "\n",
    "print('hard_loss =', round(hard_loss, 4))\n",
    "print('soft_loss =', round(soft_loss, 4))\n",
    "print('total_distillation_loss =', round(loss, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蒸留で見ているのは「正解ラベル」だけではありません。教師の確率分布（どの選択肢をどれくらいあり得ると見ているか）も移すことで、学生モデルの判断を滑らかにできます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PEFT（LoRA / QLoRA / DoRA）\n",
    "\n",
    "全重みを更新する代わりに、更新箇所を限定して学習コストを下げるのがPEFTです。\n",
    "\n",
    "- LoRA: 低ランク行列だけ学習\n",
    "- QLoRA: ベース重みを4bit近傍で保持しつつLoRA学習\n",
    "- DoRA: LoRAの表現力を拡張した亜種（方向と大きさの扱いを分離）\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lora_trainable_params(hidden_size: int, rank: int, n_layers: int, n_proj_per_layer: int = 4) -> int:\n",
    "    # 1つの線形層に対して A(in->r) と B(r->out) を持つ\n",
    "    # ここでは in=out=hidden_size として概算\n",
    "    per_proj = 2 * hidden_size * rank\n",
    "    return per_proj * n_proj_per_layer * n_layers\n",
    "\n",
    "\n",
    "def attention_proj_params(hidden_size: int, n_layers: int, n_proj_per_layer: int = 4) -> int:\n",
    "    # q,k,v,o の投影層のみを対象とした概算\n",
    "    per_proj = hidden_size * hidden_size\n",
    "    return per_proj * n_proj_per_layer * n_layers\n",
    "\n",
    "\n",
    "def rough_transformer_params(hidden_size: int, n_layers: int, vocab_size: int = 32_000) -> int:\n",
    "    # 非常に粗い近似: 1層あたり attention(約4h^2) + MLP(約8h^2) = 約12h^2\n",
    "    # + 埋め込み語彙\n",
    "    return n_layers * (12 * hidden_size * hidden_size) + vocab_size * hidden_size\n",
    "\n",
    "\n",
    "hidden = 4096\n",
    "layers = 32\n",
    "rank = 16\n",
    "\n",
    "lora = lora_trainable_params(hidden, rank, layers)\n",
    "attn_only = attention_proj_params(hidden, layers)\n",
    "rough_total = rough_transformer_params(hidden, layers)\n",
    "\n",
    "print('LoRA trainable params            =', f'{lora:,}')\n",
    "print('attention-proj params (partial)  =', f'{attn_only:,}')\n",
    "print('rough total model params         =', f'{rough_total:,}')\n",
    "print('LoRA ratio vs rough total        =', f'{100 * lora / rough_total:.3f}%')\n",
    "\n",
    "# QLoRA風のメモリ感（概算）\n",
    "base_params = 7_000_000_000\n",
    "base_int4_gib = to_gib(bytes_for_params(base_params, 4))\n",
    "adapter_fp16_gib = to_gib(bytes_for_params(lora, 16))\n",
    "\n",
    "# 学習時オーバーヘッドの粗い目安（grad, optimizer state, 一時バッファ）\n",
    "overhead_gib = adapter_fp16_gib * 3.0 + 2.0\n",
    "rough_train_peak = base_int4_gib + adapter_fp16_gib + overhead_gib\n",
    "\n",
    "print('base(INT4) GiB                   =', round(base_int4_gib, 3))\n",
    "print('adapter(FP16) GiB                =', round(adapter_fp16_gib, 3))\n",
    "print('rough training peak GiB (toy)    =', round(rough_train_peak, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでの学びは、学習対象を絞るだけでメモリと学習時間のボトルネックが大きく変わる点です。QLoRAの実務価値は「1枚GPUでも回しやすくなる」ことですが、実際には勾配・optimizer・バッファ分の余裕を見積もってから判断します。INT4の理論値は量子化メタデータや実装差で増減するため、最終判断は実測ピークメモリで行います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. アーキテクチャ最適化（Attention / MLP）\n",
    "\n",
    "軽量化は量子化だけではありません。計算グラフ側を変えると、同じ精度帯で速度を稼げることがあります。特に文脈長が伸びるほど、Attentionの計算量をどう扱うかが効いてきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def attention_complexity(seq_len: int) -> int:\n",
    "    # QK^T の主要項だけを見た O(L^2) のスケール\n",
    "    return seq_len * seq_len\n",
    "\n",
    "\n",
    "def linear_attention_complexity(seq_len: int) -> int:\n",
    "    # 線形近似系を O(L) のスケールとして比較\n",
    "    return seq_len\n",
    "\n",
    "\n",
    "for L in [512, 2048, 8192, 32768]:\n",
    "    quad = attention_complexity(L)\n",
    "    lin = linear_attention_complexity(L)\n",
    "    print(f'L={L:>5}: quadratic/linear ratio = {quad/lin:>7.0f}x')\n",
    "\n",
    "\n",
    "def kv_memory_factor(num_heads: int, num_kv_heads: int) -> float:\n",
    "    return num_kv_heads / num_heads\n",
    "\n",
    "\n",
    "print('\\nKV cache factor examples:')\n",
    "print('MHA  (32/32):', kv_memory_factor(32, 32))\n",
    "print('GQA  (32/8) :', kv_memory_factor(32, 8))\n",
    "print('MQA  (32/1) :', kv_memory_factor(32, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GQA/MQAのような設計は、KV cacheを削って長文推論を実用化しやすくします。MLP側でも、活性化関数や中間次元の設計を見直すと、品質を保ったまま演算量を下げられる余地があります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 推論最適化（PagedAttention / Continuous Batching / Speculative Decoding）\n",
    "\n",
    "推論では、モデルそのものより「リクエストのさばき方」が支配的になる場面があります。ここは運用側の最適化です。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def static_batch_utilization(lengths: List[int], slots: int) -> Tuple[int, float]:\n",
    "    steps = 0\n",
    "    work = 0\n",
    "    for i in range(0, len(lengths), slots):\n",
    "        batch = lengths[i:i+slots]\n",
    "        max_len = max(batch)\n",
    "        steps += max_len\n",
    "        work += sum(batch)\n",
    "    util = work / (steps * slots)\n",
    "    return steps, util\n",
    "\n",
    "\n",
    "def continuous_batch_utilization(lengths: List[int], slots: int) -> Tuple[int, float]:\n",
    "    queue = list(lengths)\n",
    "    active: List[int] = []\n",
    "    steps = 0\n",
    "    work = 0\n",
    "\n",
    "    while queue or active:\n",
    "        while queue and len(active) < slots:\n",
    "            active.append(queue.pop(0))\n",
    "\n",
    "        steps += 1\n",
    "        next_active = []\n",
    "        for remain in active:\n",
    "            remain -= 1\n",
    "            work += 1\n",
    "            if remain > 0:\n",
    "                next_active.append(remain)\n",
    "        active = next_active\n",
    "\n",
    "    util = work / (steps * slots)\n",
    "    return steps, util\n",
    "\n",
    "\n",
    "requests = [120, 80, 40, 20, 60, 55, 40, 35]\n",
    "slots = 4\n",
    "\n",
    "s_steps, s_util = static_batch_utilization(requests, slots)\n",
    "c_steps, c_util = continuous_batch_utilization(requests, slots)\n",
    "\n",
    "print('static steps      =', s_steps, '| utilization =', round(s_util, 3))\n",
    "print('continuous steps  =', c_steps, '| utilization =', round(c_util, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def progress_per_verify_step_toy(draft_block: int, accept_rate: float) -> float:\n",
    "    # 1回の検証で平均して何トークン進むか（近似）\n",
    "    return max(draft_block * accept_rate, 1e-6)\n",
    "\n",
    "\n",
    "def speculative_speedup_toy(draft_block: int, accept_rate: float, draft_cost_ratio: float = 0.25) -> float:\n",
    "    # ごく粗い速度近似:\n",
    "    #   1ラウンドのコスト = teacher検証(1.0) + draft生成(draft_cost_ratio * draft_block)\n",
    "    #   進捗 = draft_block * accept_rate\n",
    "    progress = progress_per_verify_step_toy(draft_block, accept_rate)\n",
    "    cost_per_round = 1.0 + draft_cost_ratio * draft_block\n",
    "    cost_per_token = cost_per_round / progress\n",
    "    return 1.0 / cost_per_token\n",
    "\n",
    "\n",
    "for k in [2, 4, 8]:\n",
    "    for p in [0.4, 0.6, 0.8]:\n",
    "        progress = progress_per_verify_step_toy(k, p)\n",
    "        speed = speculative_speedup_toy(k, p, draft_cost_ratio=0.25)\n",
    "        print(f'k={k}, accept={p:.1f} -> progress/verify={progress:.2f} tok, speedup(toy)=x{speed:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PagedAttentionはKV cacheをページ単位で管理して断片化を減らし、Continuous BatchingはGPUスロットの遊びを減らします。Speculative Decodingは、小さい下書きモデルがどれだけ当たるかで効果が変わります。ここで使った速度式はあくまで近似なので、実運用では必ず実測で検証します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. どの順で導入するか\n",
    "\n",
    "現場では、全部を同時に入れるより段階導入が安全です。次の順序が失敗しにくいです。\n",
    "\n",
    "1. まず推論観測（レイテンシ、GPUメモリ、トークン/秒）を取る\n",
    "2. Weight量子化 + KV cache設計を入れる\n",
    "3. バッチング戦略（Continuous Batching）を入れる\n",
    "4. 学習が必要ならLoRA/QLoRAで適応\n",
    "5. 品質が足りない場合だけ蒸留やアーキ変更に進む\n",
    "\n",
    "この順序にすると、どこで改善したかを切り分けやすく、ロールバックもしやすくなります。しきい値はプロダクト条件で変わるため、次のコードは目安ルールです。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_stack(\n",
    "    vram_gib: float,\n",
    "    latency_ms: int,\n",
    "    quality_priority: str,\n",
    "    draft_accept_rate_estimate: float = 0.5,\n",
    ") -> Dict[str, str]:\n",
    "    # しきい値は単一GPU運用を想定した経験則の目安\n",
    "    plan = {}\n",
    "\n",
    "    if vram_gib < 16:\n",
    "        plan['model_precision'] = 'INT4 or INT8 + KV cache optimization'\n",
    "        plan['fine_tuning'] = 'QLoRA'\n",
    "    else:\n",
    "        plan['model_precision'] = 'INT8 or FP16 (quality-sensitive)'\n",
    "        plan['fine_tuning'] = 'LoRA or full fine-tune (if required)'\n",
    "\n",
    "    if latency_ms < 120 and draft_accept_rate_estimate >= 0.55:\n",
    "        plan['serving'] = 'Continuous batching + speculative decoding'\n",
    "    else:\n",
    "        plan['serving'] = 'Continuous batching first, speculative optional'\n",
    "\n",
    "    if quality_priority == 'high':\n",
    "        plan['quality_guard'] = 'LLM-as-a-judge + rubric eval + bias-check + human spot-check'\n",
    "    else:\n",
    "        plan['quality_guard'] = 'rule-based eval + periodic human check'\n",
    "\n",
    "    return plan\n",
    "\n",
    "\n",
    "print(recommend_stack(vram_gib=12, latency_ms=100, quality_priority='high', draft_accept_rate_estimate=0.6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "軽量化の本質は、単にモデルを小さくすることではありません。限られた計算資源の中で、必要な品質・速度・コストを同時に満たす設計を作ることです。量子化、PEFT、蒸留、推論最適化は、すべてそのための道具です。だからこそ、導入のたびに品質評価と運用計測をセットで回すことが重要です。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
