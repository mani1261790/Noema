{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    torch = None\n",
    "    nn = None\n",
    "    optim = None\n",
    "    TORCH_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前学習（Pre-training）\n",
    "\n",
    "事前学習は、大量の未ラベルテキストから「次にどのトークンが来るか」を予測することで、言語の統計構造を学ぶ段階です。\n",
    "このノートでは、データ準備、N-gramからニューラル言語モデルへの流れ、評価（perplexity）、計算コスト感覚、そして機械論的解釈可能性の入口までを一貫して確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語モデルの基本目的は次の確率を最大化することです。\n",
    "\n",
    "`P(w_1, ..., w_T) = Π_t P(w_t | w_{<t})`\n",
    "\n",
    "ここで `w_{<t}` は `w_1 ... w_{t-1}`（時刻 `t` より前のトークン列）です。\n",
    "実装ではクロスエントロピー損失を最小化します。\n",
    "評価では perplexity（困惑度）を使い、`perplexity = exp(平均クロスエントロピー)`、低いほど予測が当たりやすいことを意味します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小さな日本語コーパス（教育用）\n",
    "raw_docs = [\n",
    "    '事前学習では大量のテキストから次トークン予測を学ぶ。',\n",
    "    '言語モデルは文脈に応じた分布を出力する。',\n",
    "    'トークン化の設計は学習効率と性能に強く影響する。',\n",
    "    '前処理では重複除去や品質フィルタリングが重要になる。',\n",
    "    '評価ではperplexityや下流タスク性能を併用する。',\n",
    "    'モデル規模とデータ規模のバランスが学習の成否を左右する。',\n",
    "    '機械論的解釈可能性は内部回路の理解に役立つ。',\n",
    "    'SFTは事前学習済みモデルを指示追従に調整する工程である。',\n",
    "    '推論時は温度やサンプリング戦略が出力を変える。',\n",
    "    '長文文脈では注意機構の設計が効いてくる。',\n",
    "]\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'[。､，,.!?！？]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def char_tokenize(s):\n",
    "    s = normalize(s).replace(' ', '')\n",
    "    return list(s)\n",
    "\n",
    "\n",
    "docs = [char_tokenize(d) for d in raw_docs]\n",
    "random.seed(0)\n",
    "random.shuffle(docs)\n",
    "split = int(len(docs) * 0.8)\n",
    "train_docs = docs[:split]\n",
    "val_docs = docs[split:]\n",
    "\n",
    "print('train docs:', len(train_docs), 'val docs:', len(val_docs))\n",
    "print('sample train tokens:', ''.join(train_docs[0][:30]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = np.array([len(d) for d in train_docs], dtype=np.int64)\n",
    "val_lengths = np.array([len(d) for d in val_docs], dtype=np.int64)\n",
    "\n",
    "vocab = sorted(set(ch for doc in train_docs for ch in doc))\n",
    "vocab = ['<unk>'] + vocab\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "unk_id = stoi['<unk>']\n",
    "\n",
    "print('vocab size:', len(vocab))\n",
    "print('avg train length:', float(train_lengths.mean()))\n",
    "print('avg val length  :', float(val_lengths.mean()))\n",
    "\n",
    "plt.figure(figsize=(6.8, 3.4))\n",
    "plt.hist(train_lengths, alpha=0.7, label='train', color='#7aa2ff')\n",
    "plt.hist(val_lengths, alpha=0.7, label='val', color='#8dd3a7')\n",
    "plt.xlabel('sequence length (characters)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Length distribution after tokenization')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはN-gram言語モデルを作り、事前学習の最小ベースラインを確認します。\n",
    "\n",
    "- Uni-gram: 直前文脈を使わず出現頻度だけで予測\n",
    "- Bi-gram: 1つ前のトークンを条件に次トークンを予測\n",
    "\n",
    "ここでは加法スムージング（add-k）を入れて、未知遷移でも確率0を避けます。\n",
    "また簡略化のためBOS/EOSは入れず、生成時の開始文字を固定しています（実運用ではBOS開始・EOS停止が一般的）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(docs):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        out.extend(d)\n",
    "    return out\n",
    "\n",
    "\n",
    "def to_ids(doc):\n",
    "    return [stoi.get(ch, unk_id) for ch in doc]\n",
    "\n",
    "\n",
    "train_ids = [to_ids(d) for d in train_docs]\n",
    "val_ids = [to_ids(d) for d in val_docs]\n",
    "\n",
    "unigram_counts = Counter(flatten(train_ids))\n",
    "bigram_counts = defaultdict(Counter)\n",
    "for seq in train_ids:\n",
    "    for a, b in zip(seq[:-1], seq[1:]):\n",
    "        bigram_counts[a][b] += 1\n",
    "\n",
    "V = len(vocab)\n",
    "\n",
    "\n",
    "def unigram_prob(tok, k=0.1):\n",
    "    total = sum(unigram_counts.values())\n",
    "    return (unigram_counts[tok] + k) / (total + k * V)\n",
    "\n",
    "\n",
    "def bigram_prob(prev, tok, k=0.1):\n",
    "    row = bigram_counts[prev]\n",
    "    total = sum(row.values())\n",
    "    return (row[tok] + k) / (total + k * V)\n",
    "\n",
    "\n",
    "def perplexity_unigram(seqs):\n",
    "    nll = 0.0\n",
    "    n = 0\n",
    "    for seq in seqs:\n",
    "        for t in seq:\n",
    "            p = unigram_prob(t)\n",
    "            nll += -math.log(p + 1e-12)\n",
    "            n += 1\n",
    "    return math.exp(nll / max(n, 1))\n",
    "\n",
    "\n",
    "def perplexity_bigram(seqs):\n",
    "    nll = 0.0\n",
    "    n = 0\n",
    "    for seq in seqs:\n",
    "        for prev, t in zip(seq[:-1], seq[1:]):\n",
    "            p = bigram_prob(prev, t)\n",
    "            nll += -math.log(p + 1e-12)\n",
    "            n += 1\n",
    "    return math.exp(nll / max(n, 1))\n",
    "\n",
    "\n",
    "print('val perplexity unigram:', round(perplexity_unigram(val_ids), 4))\n",
    "print('val perplexity bigram :', round(perplexity_bigram(val_ids), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_unigram(max_len=40):\n",
    "    probs = np.array([unigram_prob(i) for i in range(V)], dtype=np.float64)\n",
    "    probs = probs / probs.sum()\n",
    "    ids = np.random.choice(np.arange(V), size=max_len, p=probs)\n",
    "    return ''.join(itos[i] for i in ids if i != unk_id)\n",
    "\n",
    "\n",
    "def sample_bigram(start_id, max_len=40):\n",
    "    out = [start_id]\n",
    "    cur = start_id\n",
    "    for _ in range(max_len - 1):\n",
    "        probs = np.array([bigram_prob(cur, j) for j in range(V)], dtype=np.float64)\n",
    "        probs = probs / probs.sum()\n",
    "        nxt = int(np.random.choice(np.arange(V), p=probs))\n",
    "        out.append(nxt)\n",
    "        cur = nxt\n",
    "    return ''.join(itos[i] for i in out if i != unk_id)\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "start_char = train_ids[0][0]\n",
    "print('unigram sample:', sample_unigram(36))\n",
    "print('bigram sample :', sample_bigram(start_char, 36))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、クロスエントロピー損失を手計算し、ニューラル言語モデル学習へ接続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([0.2, -0.3, 1.1, 0.0], dtype=np.float64)\n",
    "target = 2\n",
    "\n",
    "shift = logits - np.max(logits)\n",
    "probs = np.exp(shift) / np.sum(np.exp(shift))\n",
    "ce = -math.log(probs[target] + 1e-12)\n",
    "\n",
    "print('probs =', np.round(probs, 4))\n",
    "print('cross entropy =', round(float(ce), 6))\n",
    "print('perplexity for this token =', round(float(math.exp(ce)), 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # 連結テキストを作成\n",
    "    train_text = ''.join(''.join(d) + '\\n' for d in train_docs)\n",
    "    val_text = ''.join(''.join(d) + '\\n' for d in val_docs)\n",
    "\n",
    "    train_data = torch.tensor([stoi.get(ch, unk_id) for ch in train_text], dtype=torch.long)\n",
    "    val_data = torch.tensor([stoi.get(ch, unk_id) for ch in val_text], dtype=torch.long)\n",
    "\n",
    "    max_block = min(48, len(train_data) - 1, len(val_data) - 1)\n",
    "    if max_block < 2:\n",
    "        raise ValueError('Dataset too short for neural LM demo after split')\n",
    "\n",
    "    block = max_block\n",
    "    batch_size = 32\n",
    "\n",
    "    def get_batch(data, block_size, bsz):\n",
    "        high = len(data) - block_size - 1\n",
    "        idx = torch.randint(0, high + 1, (bsz,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "        return x, y\n",
    "\n",
    "    class TinyPretrainLM(nn.Module):\n",
    "        def __init__(self, vocab_size, d_model=64):\n",
    "            super().__init__()\n",
    "            self.emb = nn.Embedding(vocab_size, d_model)\n",
    "            self.gru = nn.GRU(d_model, d_model, batch_first=True)\n",
    "            self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h = self.emb(x)\n",
    "            out, _ = self.gru(h)\n",
    "            return self.head(out)\n",
    "\n",
    "    model = TinyPretrainLM(len(vocab), d_model=64)\n",
    "    opt = optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    criterion_mean = nn.CrossEntropyLoss()\n",
    "    criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    for step in range(260):\n",
    "        xb, yb = get_batch(train_data, block, batch_size)\n",
    "        logits = model(xb)\n",
    "        loss = criterion_mean(logits.reshape(-1, len(vocab)), yb.reshape(-1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if step % 65 == 0:\n",
    "            print(f'step={step:>3d}, train loss={loss.item():.4f}')\n",
    "\n",
    "    # バッチ1個ではなく、検証系列全体で近似perplexityを計算\n",
    "    with torch.no_grad():\n",
    "        total_nll = 0.0\n",
    "        total_tok = 0\n",
    "        starts = list(range(0, len(val_data) - 1, block))\n",
    "        for s in starts:\n",
    "            b = min(block, len(val_data) - 1 - s)\n",
    "            if b <= 0:\n",
    "                continue\n",
    "            x = val_data[s:s+b].unsqueeze(0)\n",
    "            y = val_data[s+1:s+b+1].unsqueeze(0)\n",
    "            logits = model(x)\n",
    "            nll = criterion_sum(logits.reshape(-1, len(vocab)), y.reshape(-1)).item()\n",
    "            total_nll += nll\n",
    "            total_tok += y.numel()\n",
    "\n",
    "        val_loss = total_nll / max(total_tok, 1)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "\n",
    "    print('val avg nll =', round(val_loss, 4), 'val perplexity =', round(val_ppl, 4))\n",
    "\n",
    "    # 生成\n",
    "    prompt = '事前学習'\n",
    "    ids = [stoi.get(ch, unk_id) for ch in prompt]\n",
    "    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    for _ in range(40):\n",
    "        logits = model(x)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        x = torch.cat([x, next_id], dim=1)\n",
    "    gen = ''.join(itos[i] if i != unk_id else '□' for i in x.squeeze(0).tolist())\n",
    "    print('generated:', gen)\n",
    "else:\n",
    "    print('PyTorch未導入のため、ニューラルLMセルをスキップしました。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習ではデータ品質が非常に重要です。\n",
    "重複が多いと実効データ量が減り、汚染データがあると望ましくない振る舞いを学習します。\n",
    "下では文字3-gramのJaccard類似度で近重複を簡易検出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(text, k=3):\n",
    "    if len(text) < k:\n",
    "        return {text}\n",
    "    return {text[i:i+k] for i in range(len(text) - k + 1)}\n",
    "\n",
    "\n",
    "def jaccard(a, b):\n",
    "    inter = len(a & b)\n",
    "    union = len(a | b)\n",
    "    return inter / max(union, 1)\n",
    "\n",
    "\n",
    "dedup_candidates = [\n",
    "    'llmの事前学習では大規模テキストを使う',\n",
    "    'llmの事前学習では大規模なテキストを使う',\n",
    "    '画像分類ではラベル付きデータで学習する',\n",
    "]\n",
    "\n",
    "S = [shingles(s, k=3) for s in dedup_candidates]\n",
    "for i in range(len(S)):\n",
    "    for j in range(i + 1, len(S)):\n",
    "        sim = jaccard(S[i], S[j])\n",
    "        print(f'sim({i},{j}) = {sim:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スケーリング則の細部は設定依存ですが、実務では「おおまかな計算量感覚」を先に持つのが重要です。\n",
    "ここではデコーダ型Transformer学習の粗い目安として、\n",
    "\n",
    "`FLOPs ≈ 6 * N_params * N_tokens`\n",
    "\n",
    "を使って見積もります（係数はモデル形状・実装・ハードウェアで変動します）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_flops(params_billion, tokens_billion):\n",
    "    n_params = params_billion * 1e9\n",
    "    n_tokens = tokens_billion * 1e9\n",
    "    return 6.0 * n_params * n_tokens\n",
    "\n",
    "\n",
    "settings = [\n",
    "    (0.1, 20),   # 0.1B params, 20B tokens\n",
    "    (0.7, 100),  # 0.7B params, 100B tokens\n",
    "    (7.0, 300),  # 7B params, 300B tokens\n",
    "]\n",
    "\n",
    "for p_b, t_b in settings:\n",
    "    flops = estimate_flops(p_b, t_b)\n",
    "    print(f'params={p_b:>4.1f}B, tokens={t_b:>4.0f}B -> FLOPs≈{flops:.3e}')\n",
    "\n",
    "# トークン課金の例（仮定値）\n",
    "in_tok, out_tok = 1200, 400\n",
    "price_per_m_in = 0.20\n",
    "price_per_m_out = 0.80\n",
    "cost = (in_tok / 1e6) * price_per_m_in + (out_tok / 1e6) * price_per_m_out\n",
    "print('\\nexample inference cost per request (assumed pricing) =', round(cost, 6), 'USD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習後には、継続事前学習（domain adaptive pretraining）やSFTへ進むことが多いです。\n",
    "混合データでの学習では、一般コーパスとドメインコーパスの損失バランスを監視し、過学習や忘却を防ぎます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2種類のデータ損失を重み付きで合成する簡易例\n",
    "loss_general = np.array([2.20, 2.05, 1.98, 1.95, 1.93])\n",
    "loss_domain = np.array([2.80, 2.30, 2.00, 1.82, 1.74])\n",
    "alpha = 0.6  # general側の重み\n",
    "\n",
    "mixed = alpha * loss_general + (1 - alpha) * loss_domain\n",
    "for i, (g, d, m) in enumerate(zip(loss_general, loss_domain, mixed), 1):\n",
    "    print(f'epoch {i}: general={g:.3f}, domain={d:.3f}, mixed={m:.3f}')\n",
    "\n",
    "plt.figure(figsize=(6.8, 3.5))\n",
    "plt.plot(loss_general, marker='o', label='general loss')\n",
    "plt.plot(loss_domain, marker='s', label='domain loss')\n",
    "plt.plot(mixed, marker='^', label='weighted objective')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Balancing continued pretraining objectives')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械論的解釈可能性（mechanistic interpretability）は、ニューラルモデル内部の回路を理解する試みです。\n",
    "下の可視化は厳密な回路解析そのものではなく、\n",
    "「何が次に出やすいか」を統計遷移として読むための入口デモです。\n",
    "\n",
    "本格的には、注意ヘッドやMLPニューロン活性を直接解析して因果的に検証します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram遷移行列を可視化（解釈可能性の最小例）\n",
    "show_chars = [ch for ch in ['事', '前', '学', '習', 'モ', 'デ', 'ル', '。'] if ch in stoi]\n",
    "show_ids = [stoi[ch] for ch in show_chars]\n",
    "\n",
    "M = np.zeros((len(show_ids), len(show_ids)), dtype=np.float64)\n",
    "for i, src in enumerate(show_ids):\n",
    "    for j, dst in enumerate(show_ids):\n",
    "        M[i, j] = bigram_prob(src, dst)\n",
    "\n",
    "row_sums = M.sum(axis=1, keepdims=True)\n",
    "M_norm = M / np.maximum(row_sums, 1e-12)\n",
    "\n",
    "plt.figure(figsize=(6.0, 4.6))\n",
    "plt.imshow(M_norm, cmap='magma')\n",
    "plt.colorbar(label='P(next | current)')\n",
    "plt.xticks(range(len(show_chars)), show_chars)\n",
    "plt.yticks(range(len(show_chars)), show_chars)\n",
    "plt.xlabel('next token')\n",
    "plt.ylabel('current token')\n",
    "plt.title('Interpretable transition map (toy bigram)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i, src in enumerate(show_chars):\n",
    "    top = np.argsort(M_norm[i])[::-1][:3]\n",
    "    cand = [(show_chars[t], float(M_norm[i, t])) for t in top]\n",
    "    print(src, '->', [(c, round(p, 4)) for c, p in cand])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習では「データ」「目的関数」「計算資源」の3つを同時に設計する必要があります。\n",
    "最初に小さな実験で挙動を掴み、次に本番規模へスケールする手順を徹底すると、失敗コストを下げやすくなります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}