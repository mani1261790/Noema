{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 損失関数と勾配降下法\n\nディープラーニング セクションの学習ステップ 2/8。\n損失関数と勾配降下法は、損失が下がる理由を『式』と『数値変化』の両方で確認する構成です。\n\nこのステップの到達目標: 損失、勾配、最適化、正則化のつながりをコードで確かめ、モデルの挙動を説明できる状態にします。\n前提: ベクトル・行列の計算と、微分の意味を言葉で説明できると理解しやすくなります。\n\n今回の中心語: 「順伝播」、「逆伝播」、「勾配」、「損失」、「正則化」、「損失関数と勾配降下法」\n前ステップ「ニューラルネットワーク」では パーセプトロンの順伝播 → 損失を計算する を確認しました。\nここまでに登場した語: 「順伝播」、「逆伝播」、「勾配」、「損失」、「正則化」、「ニューラルネットワーク」\nセクション全体のゴール: 損失、勾配、最適化、正則化のつながりをコードで確かめ、モデルの挙動を説明できる状態にします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 観察 1: 損失計算の初期値を作る\n\n勾配降下法を理解しやすくするため、損失を計算できる初期状態を固定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\nx = [1.0, -0.5, 0.3]\nw = [0.2, -0.4, 0.6]\nz = sum(xi * wi for xi, wi in zip(x, w)) + 0.12\ny = 1 / (1 + math.exp(-z))\nprint('task = loss-and-gradient-descent', 'pred=', round(y, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "この初期予測を使って、損失の勾配と更新方向を確認します。\n\nこの節では、順伝播 が入出力のどこを決めるかを中心に読める状態になれば十分です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 観察 2: 損失を計算する\n\n次に、予測値に対する損失を計算します。学習は損失を下げる方向に進むので、損失の意味を言葉で説明できることが重要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = 1.0\neps = 1e-9\nloss = -(target * math.log(y + eps) + (1 - target) * math.log(1 - y + eps))\nprint('loss =', round(loss, 6))\nprint('error =', round(target - y, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "損失は『どれだけ外したか』を測る物差しです。物差しがない状態では、改善の方向を決められません。\n\nこの節では、順伝播 が入出力のどこを決めるかを中心に読める状態になれば十分です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 計算の対応表\n\n1. $z = W*x + b$\n2. $theta \\leftarrow  theta - eta * grad_theta L$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 観察 3: 勾配降下法の一歩\n\nここで、重みを一回更新する最小実験を行います。更新前後の損失を比較して、学習の方向が正しいかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = 0.1\ngrad = -(target - y) * y * (1 - y)\nw_new = [wi - lr * grad * xi for wi, xi in zip(w, x)]\nz_new = sum(xi * wi for xi, wi in zip(x, w_new)) + 0.1\ny_new = 1 / (1 + math.exp(-z_new))\nprint('y before/after =', round(y, 6), round(y_new, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "この更新で損失が下がれば、勾配方向が合理的だったと言えます。下がらないなら学習率や符号を疑います。\n\nこの節では、順伝播 が入出力のどこを決めるかを中心に読める状態になれば十分です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 観察 4: 正則化の感覚を作る\n\n次に、重みが大きくなりすぎることを抑える正則化項を追加します。過学習対策をコードで体験するのが狙いです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l2 = 0.01\nweight_norm = sum(wi * wi for wi in w_new)\nloss_reg = loss + l2 * weight_norm\nprint('weight_norm =', round(weight_norm, 6))\nprint('regularized loss =', round(loss_reg, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "正則化は精度を上げる魔法ではなく、汎化の崩れを防ぐ保険です。評価データと合わせて効果を判断してください。\n\nこの節では、順伝播 が入出力のどこを決めるかを中心に読める状態になれば十分です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 観察 5: ミニバッチの考え方\n\n最後に、複数サンプルをまとめて扱う発想を確認します。実務では1件ずつよりバッチ処理が主流です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = [[0.8, -0.4, 0.2], [0.2, 0.1, -0.3], [0.5, -0.2, 0.7]]\ntargets = [1.0, 0.0, 1.0]\npreds = []\nfor bx in batch:\n    z_b = sum(xi * wi for xi, wi in zip(bx, w_new)) + 0.1\n    preds.append(1 / (1 + math.exp(-z_b)))\nprint('preds =', [round(p, 4) for p in preds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ミニバッチを扱えるようになると、計算効率と学習安定性の両面で設計の幅が広がります。\n\nこの節では、順伝播 が入出力のどこを決めるかを中心に読める状態になれば十分です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 要点整理\n\n今回のノートで押さえておくべき誤解しやすい点を整理します。\n\n1. 学習率が大きすぎて発散する\n2. 検証損失の監視をせず過学習を見逃す\n3. 前処理と活性化の相性を無視する\n\n次は学習ステップ 3/8「最適化と正則化」へ進み、今回のコードとの差分を確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "計算の対応表\n\n1. $theta \\leftarrow  theta - eta * grad_theta L$\n\n不足していた式を追記しました。各式がどのコード行に対応するかをメモして確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "計算の対応表\n\n1. $theta \\leftarrow  theta - eta * grad_theta L$\n\n不足していた式を追記しました。各式がどのコード行に対応するかをメモして確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "計算の対応表\n\n1. $theta \\leftarrow  theta - eta * grad_theta L$\n\n不足していた式を追記しました。各式がどのコード行に対応するかをメモして確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "計算の対応表\n\n1. $theta \\leftarrow  theta - eta * grad_theta L$\n\n不足していた式を追記しました。各式がどのコード行に対応するかをメモして確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "計算の対応表\n\n1. $theta \\leftarrow  theta - eta * grad_theta L$\n\n不足していた式を追記しました。各式がどのコード行に対応するかをメモして確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "計算の対応表\n\n1. $theta \\leftarrow  theta - eta * grad_theta L$\n\n不足していた式を追記しました。各式がどのコード行に対応するかをメモして確認してください。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
