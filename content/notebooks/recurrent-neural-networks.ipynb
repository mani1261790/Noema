{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    torch = None\n",
    "    nn = None\n",
    "    optim = None\n",
    "    TORCH_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再帰型ニューラルネットワーク（RNN/LSTM/GRU）\n",
    "\n",
    "時系列データでは、いま見ている値だけでは正解が決まらないことがよくあります。\n",
    "例えば文章なら、次の単語を決めるために前の単語列が必要です。\n",
    "RNN系モデルは、過去の情報を「隠れ状態」として持ち運ぶことで、こうした問題を扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に、最も単純なRNNの更新を実装します。\n",
    "\n",
    "ここでは 1 次元入力を仮定します。\n",
    "`x_t` は時刻 `t` の入力、`h_t` は時刻 `t` の隠れ状態（記憶）です。\n",
    "`W_x` は入力に掛かる重み、`W_h` は前時刻の記憶に掛かる重み、`b` はバイアスです。\n",
    "\n",
    "h_t = tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "\n",
    "この式の意味は「新しい入力 `x_t` と、ひとつ前の記憶 `h_{t-1}` を混ぜて、新しい記憶 `h_t` を作る」です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn_forward(xs, wxh=0.9, whh=0.8, bh=0.0):\n",
    "    h = 0.0\n",
    "    history = []\n",
    "    for t, x_t in enumerate(xs):\n",
    "        # pre = W_x x_t + W_h h_{t-1} + b\n",
    "        pre = wxh * x_t + whh * h + bh\n",
    "        # h_t = tanh(pre)\n",
    "        h = math.tanh(pre)\n",
    "        history.append({'t': t, 'x_t': x_t, 'pre': pre, 'h_t': h})\n",
    "    return h, history\n",
    "\n",
    "\n",
    "sequence = [0.2, -0.1, 0.5, 0.3, -0.4, 0.1]\n",
    "final_h, hist = simple_rnn_forward(sequence, wxh=1.0, whh=0.75, bh=0.0)\n",
    "\n",
    "for row in hist:\n",
    "    print(f\"t={row['t']:>2d}, x_t={row['x_t']:>5.2f}, pre={row['pre']:>7.4f}, h_t={row['h_t']:>7.4f}\")\n",
    "\n",
    "print('final hidden state =', round(final_h, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_axis = [r['t'] for r in hist]\n",
    "x_axis = [r['x_t'] for r in hist]\n",
    "h_axis = [r['h_t'] for r in hist]\n",
    "\n",
    "plt.figure(figsize=(7.2, 3.6))\n",
    "plt.plot(t_axis, x_axis, marker='o', label='input x_t')\n",
    "plt.plot(t_axis, h_axis, marker='s', label='hidden h_t')\n",
    "plt.axhline(0, color='#999999', linewidth=1)\n",
    "plt.xlabel('time step t')\n",
    "plt.ylabel('value')\n",
    "plt.title('Simple RNN: input vs hidden state')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNの弱点として、長い系列になると初期の情報が消えやすい問題があります。\n",
    "逆伝播では、時刻をまたいで勾配が連鎖的に掛け算されるため、係数が1より小さいと急速に小さくなります。\n",
    "この現象が勾配消失（vanishing gradient）です。\n",
    "\n",
    "下の可視化は直感用の近似で、`|W_h|^T` のスカラー連鎖だけを取り出して見ています。\n",
    "実際には `tanh` の導関数やヤコビアン積も効くので、ここでの図は「雰囲気の把握」が目的です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_chain_gain(whh, steps):\n",
    "    return np.abs(whh) ** steps\n",
    "\n",
    "\n",
    "steps = np.arange(1, 61)\n",
    "for whh in [0.5, 0.9, 1.1]:\n",
    "    gains = recurrent_chain_gain(whh, steps)\n",
    "    print(f'whh={whh}: step 1 -> {gains[0]:.6f}, step 30 -> {gains[29]:.6f}, step 60 -> {gains[59]:.6f}')\n",
    "\n",
    "plt.figure(figsize=(7.2, 3.6))\n",
    "for whh in [0.5, 0.9, 1.1]:\n",
    "    plt.plot(steps, recurrent_chain_gain(whh, steps), label=f'|whh|={whh}')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('time distance')\n",
    "plt.ylabel('gradient scale (log)')\n",
    "plt.title('Why long-term dependency is hard for plain RNN')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMはこの問題に対して、記憶セル `c_t` と3つのゲートを導入します。\n",
    "\n",
    "- 忘却ゲート `f_t`: どれだけ過去記憶を残すか\n",
    "- 入力ゲート `i_t`: 新しい情報をどれだけ入れるか\n",
    "- 出力ゲート `o_t`: 記憶をどれだけ外に見せるか\n",
    "- 候補情報 `g_t`: 新しく書き込みたい内容\n",
    "\n",
    "`c_t` を足し算中心で更新することで、勾配が流れやすくなります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def init_lstm_params(input_size=1, hidden_size=2, seed=7):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    concat_size = input_size + hidden_size\n",
    "\n",
    "    def w(shape):\n",
    "        return rng.normal(0.0, 0.35, size=shape)\n",
    "\n",
    "    params = {\n",
    "        'W_i': w((hidden_size, concat_size)),\n",
    "        'b_i': np.zeros(hidden_size),\n",
    "        'W_f': w((hidden_size, concat_size)),\n",
    "        'b_f': np.zeros(hidden_size),\n",
    "        'W_g': w((hidden_size, concat_size)),\n",
    "        'b_g': np.zeros(hidden_size),\n",
    "        'W_o': w((hidden_size, concat_size)),\n",
    "        'b_o': np.zeros(hidden_size),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def lstm_step(x_t, h_prev, c_prev, p):\n",
    "    # x_t:(1,), h_prev/c_prev:(2,), W_*:(2,3), b_*:(2,)\n",
    "    z = np.concatenate([x_t, h_prev])\n",
    "\n",
    "    i_t = sigmoid(p['W_i'] @ z + p['b_i'])\n",
    "    f_t = sigmoid(p['W_f'] @ z + p['b_f'])\n",
    "    g_t = np.tanh(p['W_g'] @ z + p['b_g'])\n",
    "    o_t = sigmoid(p['W_o'] @ z + p['b_o'])\n",
    "\n",
    "    c_t = f_t * c_prev + i_t * g_t\n",
    "    h_t = o_t * np.tanh(c_t)\n",
    "\n",
    "    return h_t, c_t, {'i_t': i_t, 'f_t': f_t, 'g_t': g_t, 'o_t': o_t}\n",
    "\n",
    "\n",
    "params = init_lstm_params(input_size=1, hidden_size=2, seed=7)\n",
    "xs = [np.array([v]) for v in [0.6, -0.2, 0.1, 0.5, -0.4]]\n",
    "h = np.zeros(2)\n",
    "c = np.zeros(2)\n",
    "\n",
    "for t, x_t in enumerate(xs):\n",
    "    h, c, gates = lstm_step(x_t, h, c, params)\n",
    "    print(f't={t}, x={x_t[0]:>5.2f}')\n",
    "    print('  i_t=', np.round(gates['i_t'], 4), 'f_t=', np.round(gates['f_t'], 4), 'o_t=', np.round(gates['o_t'], 4))\n",
    "    print('  c_t=', np.round(c, 4), 'h_t=', np.round(h, 4))\n",
    "\n",
    "print('読み方の目安: f_t が 1 に近いほど過去記憶を残し、0 に近いほど忘れます。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUはLSTMを簡素化した派生モデルです。\n",
    "セル状態 `c_t` を分離せず、隠れ状態 `h_t` だけを更新します。\n",
    "\n",
    "- 更新ゲート `z_t`: 以前の隠れ状態をどれだけ残すか（PyTorch流儀）\n",
    "- リセットゲート `r_t`: 候補状態計算で過去情報をどれだけ使うか\n",
    "\n",
    "一般に、GRUはLSTMよりパラメータが少なく、学習が軽いことがあります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rnn_params_single_bias(input_size, hidden_size):\n",
    "    # 理論の最小形（重み1組 + bias1組）\n",
    "    return hidden_size * (input_size + hidden_size) + hidden_size\n",
    "\n",
    "\n",
    "def count_params_pytorch_style(gates, input_size, hidden_size):\n",
    "    # PyTorchは bias_ih と bias_hh の2本を持つ\n",
    "    return gates * (hidden_size * input_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "\n",
    "\n",
    "input_size = 16\n",
    "hidden_size = 64\n",
    "\n",
    "print('--- single-bias theoretical count ---')\n",
    "print('RNN :', count_rnn_params_single_bias(input_size, hidden_size))\n",
    "print('LSTM:', 4 * count_rnn_params_single_bias(input_size, hidden_size))\n",
    "print('GRU :', 3 * count_rnn_params_single_bias(input_size, hidden_size))\n",
    "\n",
    "print('--- PyTorch parameter count (bias_ih + bias_hh) ---')\n",
    "print('RNN :', count_params_pytorch_style(1, input_size, hidden_size))\n",
    "print('LSTM:', count_params_pytorch_style(4, input_size, hidden_size))\n",
    "print('GRU :', count_params_pytorch_style(3, input_size, hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gru_params(input_size=1, hidden_size=2, seed=9):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    concat_size = input_size + hidden_size\n",
    "\n",
    "    def w(shape):\n",
    "        return rng.normal(0.0, 0.35, size=shape)\n",
    "\n",
    "    return {\n",
    "        'W_z': w((hidden_size, concat_size)),\n",
    "        'b_z': np.zeros(hidden_size),\n",
    "        'W_r': w((hidden_size, concat_size)),\n",
    "        'b_r': np.zeros(hidden_size),\n",
    "        'W_h': w((hidden_size, concat_size)),\n",
    "        'b_h': np.zeros(hidden_size),\n",
    "    }\n",
    "\n",
    "\n",
    "def gru_step(x_t, h_prev, p):\n",
    "    z_in = np.concatenate([x_t, h_prev])\n",
    "    z_t = sigmoid(p['W_z'] @ z_in + p['b_z'])\n",
    "    r_t = sigmoid(p['W_r'] @ z_in + p['b_r'])\n",
    "\n",
    "    h_candidate_in = np.concatenate([x_t, r_t * h_prev])\n",
    "    h_tilde = np.tanh(p['W_h'] @ h_candidate_in + p['b_h'])\n",
    "\n",
    "    # PyTorchと同じ流儀: z_t は過去状態を残す割合\n",
    "    h_t = z_t * h_prev + (1.0 - z_t) * h_tilde\n",
    "    return h_t, {'z_t': z_t, 'r_t': r_t, 'h_tilde': h_tilde}\n",
    "\n",
    "\n",
    "gru_params = init_gru_params(input_size=1, hidden_size=2, seed=9)\n",
    "h = np.zeros(2)\n",
    "for t, x_t in enumerate(xs):\n",
    "    h, gates = gru_step(x_t, h, gru_params)\n",
    "    print(f't={t}, x={x_t[0]:>5.2f}, z_t={np.round(gates[\"z_t\"],4)}, r_t={np.round(gates[\"r_t\"],4)}, h_t={np.round(h,4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからはPyTorchで、RNN/LSTM/GRUの違いを同じ課題で比較します。\n",
    "課題は「系列の最初の値が正か負かを、最後の時刻で判定する」です。\n",
    "途中にノイズが多いため、初期情報を保持できるモデルほど有利です。\n",
    "\n",
    "`logit` は確率に変換する前の値で、`BCEWithLogitsLoss` は2値分類の標準損失です。\n",
    "予測時は `sigmoid(logit) >= 0.5` を陽性判定に使います。\n",
    "\n",
    "期待値としては RNN < (GRU, LSTM) になりやすいですが、データ分布やハイパーパラメータで逆転もあります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    np.random.seed(0)\n",
    "\n",
    "    def build_memory_dataset(n_samples=768, seq_len=28):\n",
    "        x = np.random.randn(n_samples, seq_len, 1).astype(np.float32)\n",
    "        y = (x[:, 0, 0] > 0).astype(np.float32)\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "    x_train, y_train = build_memory_dataset(n_samples=768, seq_len=28)\n",
    "    x_val, y_val = build_memory_dataset(n_samples=256, seq_len=28)\n",
    "\n",
    "    class SequenceClassifier(nn.Module):\n",
    "        def __init__(self, cell_type='RNN', hidden_size=24):\n",
    "            super().__init__()\n",
    "            if cell_type == 'RNN':\n",
    "                self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "            elif cell_type == 'LSTM':\n",
    "                self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "            elif cell_type == 'GRU':\n",
    "                self.rnn = nn.GRU(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "            else:\n",
    "                raise ValueError('Unknown cell_type')\n",
    "            self.head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.rnn(x)\n",
    "            last = out[:, -1, :]\n",
    "            logit = self.head(last).squeeze(-1)\n",
    "            return logit\n",
    "\n",
    "    def train_and_eval(cell_type, seed=0, epochs=6, lr=1e-2):\n",
    "        torch.manual_seed(seed)\n",
    "        model = SequenceClassifier(cell_type=cell_type, hidden_size=24)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        batch_size = 64\n",
    "        for _ in range(epochs):\n",
    "            perm = torch.randperm(x_train.size(0))\n",
    "            for i in range(0, x_train.size(0), batch_size):\n",
    "                idx = perm[i:i+batch_size]\n",
    "                xb, yb = x_train[idx], y_train[idx]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_val)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            acc = (preds == y_val).float().mean().item()\n",
    "        return acc\n",
    "\n",
    "    seeds = [0, 1, 2]\n",
    "    for ct in ['RNN', 'LSTM', 'GRU']:\n",
    "        accs = [train_and_eval(ct, seed=s) for s in seeds]\n",
    "        print(f'{ct} accuracy: mean={np.mean(accs):.4f}, std={np.std(accs):.4f}, each={np.round(accs,4)}')\n",
    "\n",
    "    print('注: hidden_sizeを固定した簡易比較で、パラメータ数を厳密に一致させた比較ではありません。')\n",
    "else:\n",
    "    print('PyTorch未導入のため比較実験セルはスキップしました。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実務での使い分けは、まずGRUかLSTMを基準にし、必要に応じて単純RNNを比較に置くのが安全です。\n",
    "系列が長い、あるいは初期情報の保持が重要な課題では、単純RNNだけで戦うよりLSTM/GRUのほうが安定しやすくなります。\n",
    "\n",
    "一方で、長大な系列や並列計算効率が重要な場面ではTransformer系が選ばれることも多く、RNN系は軽量性やオンライン処理の文脈で依然有効です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}