{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 再帰型ニューラルネットワーク（RNN/LSTM/GRU）\n\n再帰型ニューラルネットワーク（RNN/LSTM/GRU）を、初学者が「概念 -> 理論 -> 実装」の順に理解できるように整理します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 学習目標\n- このテーマの中心概念を説明できる\n- 最低限の数式/アルゴリズムの意味を追える\n- 実装例を実行し、出力を解釈できる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 前提知識\n- 必須: Python基礎（変数・関数・リスト/辞書）\n- 推奨: 線形代数・確率統計の初歩\n- 目安: 数式の各記号を言葉に置き換えられること"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 直感\n一言で言うと: 再帰型ニューラルネットワーク（RNN/LSTM/GRU）は、複雑な対象を分解し、計算可能な形に落とし込む技術です。\n理論を読む前に、まず『何を入力して何を出したいか』を図で考えると理解が速くなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 理論\n- 再帰型ニューラルネットワーク（RNN/LSTM/GRU）で扱う主要な前提・仮定を明確にする。\n- 評価指標または目的関数を明確にし、何を最適化するかを定義する。\n- 実装時にはデータ前処理と境界条件の扱いが結果を大きく左右する。\n- RNNは時系列依存を隠れ状態で保持する。\n- 長期依存にはLSTM/GRUが有効。\n\n### 代表式\n- $z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$\n- $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 実装の流れ\n1. 最小の入力データを準備し、入出力の型・次元を確認する。\n2. 理論式に対応する計算を小さなコードで再現する。\n3. 中間値を可視化/出力して、期待と一致しているか検証する。\n4. 条件を変えたときの挙動を比較し、感度を確認する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\nx = [0.5, -1.2, 0.3]\nw = [0.8, -0.4, 0.2]\nb = 0.1\nz = sum(xi * wi for xi, wi in zip(x, w)) + b\ny = 1 / (1 + math.exp(-z))\nprint('logit=', round(z, 4), 'sigmoid=', round(y, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## よくあるつまずき\n- 式やアルゴリズムを暗記だけで進め、前提条件を見落とす。\n- データのスケール・欠損・外れ値を確認せずに学習/推論を行う。\n- 評価指標を1つだけ見て結論を急ぎ、失敗ケース分析を省略する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 演習\n1. ノートのコードを一部書き換えて、出力がどう変化するか説明する。\n2. 理論式の各項が何を意味するか、自分の言葉で1行ずつ説明する。\n3. 実運用を想定し、入力異常（欠損・外れ値・分布ずれ）への対処案を3つ挙げる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n再帰型ニューラルネットワーク（RNN/LSTM/GRU）の基本を確認しました。次は「Transformer」で、関連テーマを一段深く扱います。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
