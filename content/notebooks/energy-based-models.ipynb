{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import statistics\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# エネルギーベースモデル\n",
        "\n",
        "エネルギーベースモデル（EBM）は、確率分布を「正規化された確率」ではなく「エネルギー関数」で表す考え方です。データらしい点には低いエネルギーを、データらしくない点には高いエネルギーを与えるように学習します。\n",
        "\n",
        "このノートでは、1次元の最小実装で EBM の核を確認します。特に、SGLD と Replay Buffer を使った学習が何をしているかをコードで追います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EBM の基本式は次です。\n",
        "\n",
        "$$\n",
        "p_\\theta(x)=\\frac{\\exp(-E_\\theta(x))}{Z_\\theta},\\qquad Z_\\theta=\\int \\exp(-E_\\theta(x))dx\n",
        "$$\n",
        "\n",
        "ここで難しいのは分配関数 $Z_\\theta$ です。高次元では厳密計算がほぼ不可能なので、学習では\n",
        "\n",
        "- データ点のエネルギーを下げる\n",
        "- モデルサンプル（負例）のエネルギーを上げる\n",
        "\n",
        "という contrastive な差分で更新します。IGEBM系の実装でもこの考え方が中心です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず学習対象データを作ります。2モード混合分布を使い、モード構造を再現できるか確認しやすくします。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(31)\n",
        "\n",
        "def sample_real(n: int):\n",
        "    xs = []\n",
        "    for _ in range(n):\n",
        "        if random.random() < 0.55:\n",
        "            xs.append(random.gauss(-2.0, 0.45))\n",
        "        else:\n",
        "            xs.append(random.gauss(1.8, 0.55))\n",
        "    return xs\n",
        "\n",
        "\n",
        "def describe(xs):\n",
        "    left = sum(1 for x in xs if x < 0) / len(xs)\n",
        "    right = 1.0 - left\n",
        "    return {\n",
        "        'mean': statistics.mean(xs),\n",
        "        'std': statistics.pstdev(xs),\n",
        "        'left': left,\n",
        "        'right': right,\n",
        "    }\n",
        "\n",
        "real_data = sample_real(2400)\n",
        "stats = describe(real_data)\n",
        "print('dataset size =', len(real_data))\n",
        "print('mean/std     =', round(stats['mean'], 4), round(stats['std'], 4))\n",
        "print('left/right   =', round(stats['left'], 4), round(stats['right'], 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "今回は 2 つの井戸（well）を持つエネルギー関数を使います。\n",
        "\n",
        "- `e1(x)=a1(x-m1)^2 + k1`\n",
        "- `e2(x)=a2(x-m2)^2 + k2`\n",
        "- `E(x)=-log(exp(-e1)+exp(-e2))+c`\n",
        "\n",
        "`a1,a2` は正である必要があるため `a=exp(raw_a)` として実装します。`k1,k2` は井戸の深さを調整する項で、モード比率（重み）を表現しやすくする役割があります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def stable_logsumexp(a, b):\n",
        "    m = a if a > b else b\n",
        "    return m + math.log(math.exp(a - m) + math.exp(b - m))\n",
        "\n",
        "\n",
        "def clip_raw_a(v):\n",
        "    return max(-5.0, min(5.0, v))\n",
        "\n",
        "\n",
        "def energy_value(x, theta):\n",
        "    # theta = [raw_a1, m1, k1, raw_a2, m2, k2, c]\n",
        "    raw_a1, m1, k1, raw_a2, m2, k2, c = theta\n",
        "    a1 = math.exp(clip_raw_a(raw_a1))\n",
        "    a2 = math.exp(clip_raw_a(raw_a2))\n",
        "\n",
        "    e1 = a1 * (x - m1) ** 2 + k1\n",
        "    e2 = a2 * (x - m2) ** 2 + k2\n",
        "    return -stable_logsumexp(-e1, -e2) + c\n",
        "\n",
        "\n",
        "def energy_and_grads(x, theta):\n",
        "    raw_a1, m1, k1, raw_a2, m2, k2, c = theta\n",
        "    raw_a1 = clip_raw_a(raw_a1)\n",
        "    raw_a2 = clip_raw_a(raw_a2)\n",
        "    a1 = math.exp(raw_a1)\n",
        "    a2 = math.exp(raw_a2)\n",
        "\n",
        "    d1 = x - m1\n",
        "    d2 = x - m2\n",
        "    e1 = a1 * d1 * d1 + k1\n",
        "    e2 = a2 * d2 * d2 + k2\n",
        "\n",
        "    u1 = math.exp(-e1)\n",
        "    u2 = math.exp(-e2)\n",
        "    den = u1 + u2\n",
        "    w1 = u1 / den\n",
        "    w2 = 1.0 - w1\n",
        "\n",
        "    energy = -math.log(den) + c\n",
        "\n",
        "    grad_x = w1 * (2.0 * a1 * d1) + w2 * (2.0 * a2 * d2)\n",
        "\n",
        "    grad_raw_a1 = w1 * (d1 * d1) * a1\n",
        "    grad_m1 = w1 * (-2.0 * a1 * d1)\n",
        "    grad_k1 = w1\n",
        "\n",
        "    grad_raw_a2 = w2 * (d2 * d2) * a2\n",
        "    grad_m2 = w2 * (-2.0 * a2 * d2)\n",
        "    grad_k2 = w2\n",
        "\n",
        "    grad_c = 1.0\n",
        "\n",
        "    grads = [grad_raw_a1, grad_m1, grad_k1, grad_raw_a2, grad_m2, grad_k2, grad_c]\n",
        "    return energy, grad_x, grads\n",
        "\n",
        "\n",
        "theta_demo = [0.1, -1.2, 0.0, 0.2, 1.3, 0.2, 0.0]\n",
        "for x in [-2.0, -0.2, 1.5]:\n",
        "    e, gx, gp = energy_and_grads(x, theta_demo)\n",
        "    print('x=', x, 'E=', round(e, 5), 'dE/dx=', round(gx, 5), 'dE/dk1=', round(gp[2], 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に SGLD で負例を作ります。\n",
        "\n",
        "`x <- x - step_size * dE/dx + noise_std * N(0,1)`\n",
        "\n",
        "`-dE/dx` はエネルギーを下げる方向なので、低エネルギー領域に集まりやすくなります。ノイズ項は探索性を保つ役割を持ちます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SampleReplayBuffer:\n",
        "    def __init__(self, max_size=12000):\n",
        "        self.max_size = max_size\n",
        "        self.items = []\n",
        "\n",
        "    def add(self, xs):\n",
        "        self.items.extend(xs)\n",
        "        if len(self.items) > self.max_size:\n",
        "            self.items = self.items[-self.max_size :]\n",
        "\n",
        "    def sample(self, n):\n",
        "        if not self.items:\n",
        "            return []\n",
        "        if n >= len(self.items):\n",
        "            return self.items[:]\n",
        "        return random.sample(self.items, n)\n",
        "\n",
        "\n",
        "def sgld_samples(theta, x_init, n_steps=40, step_size=0.06, noise_std=0.08):\n",
        "    xs = x_init[:]\n",
        "    for _ in range(n_steps):\n",
        "        for i in range(len(xs)):\n",
        "            _, grad_x, _ = energy_and_grads(xs[i], theta)\n",
        "            xs[i] = xs[i] - step_size * grad_x + noise_std * random.gauss(0.0, 1.0)\n",
        "            xs[i] = max(-6.0, min(6.0, xs[i]))\n",
        "    return xs\n",
        "\n",
        "\n",
        "buffer = SampleReplayBuffer(max_size=5000)\n",
        "start = [random.uniform(-4.0, 4.0) for _ in range(8)]\n",
        "sgld_out = sgld_samples(theta_demo, start, n_steps=30)\n",
        "print('init  =', [round(v, 3) for v in start])\n",
        "print('sgld  =', [round(v, 3) for v in sgld_out])\n",
        "buffer.add(sgld_out)\n",
        "print('buffer size =', len(buffer.items))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "学習目的は\n",
        "\n",
        "`L(θ)=E_data[Eθ(x)] - E_neg[Eθ(x)] + λ * regularizer`\n",
        "\n",
        "です。これを最小化すると、データ近傍は低エネルギー、負例近傍は高エネルギーになります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def mean_grads(xs, theta):\n",
        "    g = [0.0] * 7\n",
        "    e_sum = 0.0\n",
        "    for x in xs:\n",
        "        e, _, gp = energy_and_grads(x, theta)\n",
        "        e_sum += e\n",
        "        for i in range(7):\n",
        "            g[i] += gp[i]\n",
        "    n = len(xs)\n",
        "    return e_sum / n, [v / n for v in g]\n",
        "\n",
        "\n",
        "def train_ebm(theta_init, real_data, steps=380, batch_size=96, lr=0.016, reg=8e-4, replay_ratio=0.7):\n",
        "    theta = theta_init[:]\n",
        "    replay = SampleReplayBuffer(max_size=14000)\n",
        "    history = []\n",
        "\n",
        "    for step in range(steps + 1):\n",
        "        x_data = random.sample(real_data, batch_size)\n",
        "\n",
        "        n_replay = int(batch_size * replay_ratio)\n",
        "        x_neg0 = replay.sample(n_replay)\n",
        "        if len(x_neg0) < batch_size:\n",
        "            x_neg0 = x_neg0 + [random.uniform(-4.5, 4.5) for _ in range(batch_size - len(x_neg0))]\n",
        "\n",
        "        x_neg = sgld_samples(theta, x_neg0, n_steps=30, step_size=0.055, noise_std=0.085)\n",
        "        replay.add(x_neg)\n",
        "\n",
        "        e_data, g_data = mean_grads(x_data, theta)\n",
        "        e_neg, g_neg = mean_grads(x_neg, theta)\n",
        "\n",
        "        # L = E_data - E_neg + reg * (raw_a1^2 + raw_a2^2 + 0.2*(k1^2+k2^2))\n",
        "        grads = [g_data[i] - g_neg[i] for i in range(7)]\n",
        "        grads[0] += 2.0 * reg * theta[0]\n",
        "        grads[3] += 2.0 * reg * theta[3]\n",
        "        grads[2] += 2.0 * reg * 0.2 * theta[2]\n",
        "        grads[5] += 2.0 * reg * 0.2 * theta[5]\n",
        "\n",
        "        theta = [theta[i] - lr * grads[i] for i in range(7)]\n",
        "\n",
        "        theta[0] = max(-3.0, min(3.0, theta[0]))\n",
        "        theta[3] = max(-3.0, min(3.0, theta[3]))\n",
        "        theta[1] = max(-4.0, min(4.0, theta[1]))\n",
        "        theta[4] = max(-4.0, min(4.0, theta[4]))\n",
        "        theta[2] = max(-3.0, min(3.0, theta[2]))\n",
        "        theta[5] = max(-3.0, min(3.0, theta[5]))\n",
        "\n",
        "        if step % 40 == 0:\n",
        "            gap = e_data - e_neg\n",
        "            history.append((step, e_data, e_neg, gap, theta[:], len(replay.items)))\n",
        "\n",
        "    return theta, history, replay\n",
        "\n",
        "\n",
        "random.seed(31)\n",
        "theta0 = [0.0, -0.2, 0.0, 0.0, 0.2, 0.0, 0.0]\n",
        "trained_theta, history, replay = train_ebm(theta0, real_data)\n",
        "\n",
        "for step, e_d, e_n, gap, th, rs in history:\n",
        "    print(\n",
        "        f'step={step:03d}',\n",
        "        f'E_data={round(e_d,4)}',\n",
        "        f'E_neg={round(e_n,4)}',\n",
        "        f'gap={round(gap,4)}',\n",
        "        f'theta={[round(v,4) for v in th]}',\n",
        "        f'replay={rs}'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def draw_model_samples(theta, n=2600):\n",
        "    init = [random.uniform(-4.5, 4.5) for _ in range(n)]\n",
        "    return sgld_samples(theta, init, n_steps=60, step_size=0.05, noise_std=0.07)\n",
        "\n",
        "\n",
        "random.seed(31)\n",
        "model_samples = draw_model_samples(trained_theta)\n",
        "real_stats = describe(real_data)\n",
        "model_stats = describe(model_samples)\n",
        "\n",
        "print('real  mean/std =', round(real_stats['mean'], 4), round(real_stats['std'], 4))\n",
        "print('model mean/std =', round(model_stats['mean'], 4), round(model_stats['std'], 4))\n",
        "print('real  left/right =', round(real_stats['left'], 4), round(real_stats['right'], 4))\n",
        "print('model left/right =', round(model_stats['left'], 4), round(model_stats['right'], 4))\n",
        "print('trained theta =', [round(v, 4) for v in trained_theta])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def approx_partition(theta, x_min=-6.0, x_max=6.0, n_grid=5000):\n",
        "    dx = (x_max - x_min) / n_grid\n",
        "    s = 0.0\n",
        "    for i in range(n_grid):\n",
        "        x = x_min + (i + 0.5) * dx\n",
        "        s += math.exp(-energy_value(x, theta))\n",
        "    return s * dx\n",
        "\n",
        "\n",
        "def approx_density(theta, x, z_est):\n",
        "    return math.exp(-energy_value(x, theta)) / z_est\n",
        "\n",
        "\n",
        "z_est = approx_partition(trained_theta)\n",
        "print('approx Z =', round(z_est, 6))\n",
        "\n",
        "probe_points = [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
        "for x in probe_points:\n",
        "    p = approx_density(trained_theta, x, z_est)\n",
        "    print('x=', f'{x:>4.1f}', 'E=', round(energy_value(x, trained_theta), 4), 'p~', round(p, 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここまでで、EBMの実装上の核心を確認できました。\n",
        "\n",
        "- 分配関数を直接最適化しない\n",
        "- SGLDで負例を生成する\n",
        "- Replay Bufferで負例初期値を再利用する\n",
        "\n",
        "IGEBMなどの実践モデルでは、ここに CNN/ResNet、スペクトル正規化、強いデータ拡張を組み合わせて高次元へ拡張します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 学習失敗の典型例: SGLDステップ不足\n",
        "\n",
        "def quick_train_with_sgld_steps(real_data, sgld_steps):\n",
        "    random.seed(77 + sgld_steps)\n",
        "    theta = [0.0, -0.1, 0.0, 0.0, 0.1, 0.0, 0.0]\n",
        "    replay = SampleReplayBuffer(max_size=5000)\n",
        "\n",
        "    for _ in range(140):\n",
        "        x_data = random.sample(real_data, 64)\n",
        "        x_neg0 = replay.sample(44)\n",
        "        x_neg0 = x_neg0 + [random.uniform(-4.5, 4.5) for _ in range(64 - len(x_neg0))]\n",
        "        x_neg = sgld_samples(theta, x_neg0, n_steps=sgld_steps, step_size=0.055, noise_std=0.085)\n",
        "        replay.add(x_neg)\n",
        "\n",
        "        _, g_data = mean_grads(x_data, theta)\n",
        "        _, g_neg = mean_grads(x_neg, theta)\n",
        "        grads = [g_data[i] - g_neg[i] for i in range(7)]\n",
        "        theta = [theta[i] - 0.016 * grads[i] for i in range(7)]\n",
        "        theta[0] = max(-3.0, min(3.0, theta[0]))\n",
        "        theta[3] = max(-3.0, min(3.0, theta[3]))\n",
        "\n",
        "    samples = draw_model_samples(theta, n=1200)\n",
        "    return describe(samples)\n",
        "\n",
        "\n",
        "stats_short = quick_train_with_sgld_steps(real_data, sgld_steps=5)\n",
        "stats_long = quick_train_with_sgld_steps(real_data, sgld_steps=35)\n",
        "\n",
        "print('short SGLD steps (5) :', {k: round(v, 4) for k, v in stats_short.items()})\n",
        "print('long  SGLD steps (35):', {k: round(v, 4) for k, v in stats_long.items()})\n",
        "print('real target          :', {k: round(v, 4) for k, v in describe(real_data).items()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SGLDステップが少なすぎると、負例がモデル分布を十分に近似できず、エネルギー地形の更新が偏ります。実務では、SGLDステップ数、ノイズ強度、Replay比率をセットで調整します。\n",
        "\n",
        "EBMの視点を持つと、スコアモデルや拡散モデルで出てくる「勾配で分布へ寄せる」考え方も同じ地図の上で理解しやすくなります。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
