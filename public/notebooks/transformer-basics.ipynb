{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    TORCH_AVAILABLE = True\nexcept ModuleNotFoundError:\n    torch = None\n    nn = None\n    optim = None\n    TORCH_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer（GPT / ViT / MAE）\n\nTransformerの中心は自己注意です。系列中の各要素が、ほかの要素をどれくらい参照すべきかを重みとして計算し、文脈に応じた表現を作ります。\nこのノートでは、まず自己注意の式を最小コードで確認し、次にGPTの因果マスク、ViTのパッチ化、MAEのマスク再構成へ進みます。最後にVAEとの違いも整理します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "自己注意では、入力埋め込み `X` から `Q, K, V` を作り、次で重みを計算します。\n\n`Attention(Q, K, V) = softmax((QK^T) / sqrt(d_k)) V`\n\nここで `X` の形状を `(T, d_model)`、射影行列 `W_Q, W_K, W_V` を `(d_model, d_k)` とすると、\n`Q, K, V` の形状は `(T, d_k)` になります。\n要素で見ると `score_{i,j} = (q_i ・ k_j) / sqrt(d_k)` で、行ごとに `softmax` して `V` を重み付き和します。\n\n`Q` は「何を探しているか」、`K` は「何を持っているか」、`V` は「実際に受け渡す情報」と見ると理解しやすくなります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax_rowwise(x):\n    x = x - np.max(x, axis=-1, keepdims=True)\n    exp_x = np.exp(x)\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\n# 4トークン, 埋め込み次元3の玩具例\nX = np.array([\n    [1.0, 0.2, 0.0],   # token 0\n    [0.9, 0.1, 0.1],   # token 1\n    [0.1, 0.8, 0.2],   # token 2\n    [0.0, 0.7, 1.0],   # token 3\n], dtype=np.float64)\n\nW_Q = np.array([[0.8, 0.0, 0.1], [0.2, 0.9, 0.1], [0.1, 0.2, 0.7]])\nW_K = np.array([[0.7, 0.1, 0.1], [0.1, 0.8, 0.2], [0.2, 0.1, 0.6]])\nW_V = np.array([[1.0, 0.2, 0.0], [0.1, 0.9, 0.1], [0.0, 0.2, 0.8]])\n\nQ = X @ W_Q\nK = X @ W_K\nV = X @ W_V\n\nscores = (Q @ K.T) / math.sqrt(Q.shape[-1])\nweights = softmax_rowwise(scores)\ncontext = weights @ V\n\nprint('attention weights:')\nprint(np.round(weights, 4))\nprint('\\ncontext vectors:')\nprint(np.round(context, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5.0, 4.2))\nplt.imshow(weights, cmap='Blues')\nplt.colorbar(label='attention weight')\nplt.xticks(range(len(X)), [f'k{j}' for j in range(len(X))])\nplt.yticks(range(len(X)), [f'q{i}' for i in range(len(X))])\nplt.title('Self-Attention Weight Matrix')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPTのような自己回帰モデルでは、未来トークンを見ないように因果マスク（causal mask）を入れます。\n`j > i`（未来位置）をマスクし、スコアを `-∞` 相当に落として `softmax` 後の重みをほぼ0にします。\n\n下のコードでは、同じ入力で「マスクなし」と「マスクあり」を比較します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def causal_masked_attention(Q, K, V):\n    T, d = Q.shape\n    scores = (Q @ K.T) / math.sqrt(d)\n    mask = np.triu(np.ones((T, T), dtype=bool), k=1)  # future positions\n    scores_masked = scores.copy()\n    scores_masked[mask] = -np.inf\n    w = softmax_rowwise(scores_masked)\n    return w @ V, w\n\n\ncontext_full = softmax_rowwise((Q @ K.T) / math.sqrt(Q.shape[-1])) @ V\ncontext_causal, w_causal = causal_masked_attention(Q, K, V)\n\nprint('full attention (token 1):', np.round(context_full[1], 4))\nprint('causal attention (token 1):', np.round(context_causal[1], 4))\nprint('\\ncausal weight matrix:')\nprint(np.round(w_causal, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "位置情報がないと、Transformerは順序を区別できません。\nよく使われる方法のひとつが正弦波位置エンコーディングです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sinusoidal_positional_encoding(seq_len, d_model):\n    pe = np.zeros((seq_len, d_model), dtype=np.float64)\n    pos = np.arange(seq_len)[:, None]\n    i = np.arange(d_model)[None, :]\n    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / d_model)\n    angles = pos * angle_rates\n\n    pe[:, 0::2] = np.sin(angles[:, 0::2])\n    pe[:, 1::2] = np.cos(angles[:, 1::2])\n    return pe\n\n\npe = sinusoidal_positional_encoding(seq_len=24, d_model=16)\nprint('positional encoding shape:', pe.shape)\n\nplt.figure(figsize=(7.2, 3.6))\nplt.imshow(pe.T, aspect='auto', cmap='coolwarm')\nplt.colorbar(label='value')\nplt.xlabel('position')\nplt.ylabel('channel')\nplt.title('Sinusoidal Positional Encoding')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ViT（Vision Transformer）は画像を小さなパッチ列に分割し、各パッチをトークンとして扱います。\n以下では 8x8 の画像を 2x2 パッチへ分割し、パッチ埋め込みを作る最小例を示します。\n\n実際のViTでは、パッチトークンに位置埋め込みを必ず加えます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def patchify(image, patch_size=2):\n    h, w = image.shape\n    assert h % patch_size == 0 and w % patch_size == 0\n    patches = []\n    for y in range(0, h, patch_size):\n        for x in range(0, w, patch_size):\n            p = image[y:y+patch_size, x:x+patch_size].reshape(-1)\n            patches.append(p)\n    return np.stack(patches, axis=0)\n\n\nimg = np.arange(64, dtype=np.float64).reshape(8, 8) / 63.0\npatches = patchify(img, patch_size=2)  # (16, 4)\n\nW_patch = np.random.default_rng(0).normal(0, 0.4, size=(4, 6))\npatch_tokens = patches @ W_patch  # (16, 6)\n\n# 実際のViTは学習可能な位置埋め込みを加える\npos_embed = np.random.default_rng(1).normal(0, 0.1, size=patch_tokens.shape)\npatch_tokens = patch_tokens + pos_embed\n\n# CLSトークンも通常は学習可能ベクトル（ここでは最小例として0初期化）\ncls_token = np.zeros((1, 6), dtype=np.float64)\nvit_tokens = np.concatenate([cls_token, patch_tokens], axis=0)  # (17, 6)\n\nprint('image shape      :', img.shape)\nprint('patches shape    :', patches.shape)\nprint('patch token shape:', patch_tokens.shape)\nprint('ViT token shape  :', vit_tokens.shape, '(CLS + patches)')\n\nplt.figure(figsize=(3.8, 3.8))\nplt.imshow(img, cmap='gray')\nplt.title('Toy image (8x8)')\nplt.axis('off')\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MAE（Masked Autoencoder）は、パッチの一部を隠して復元する自己教師あり学習です。\nViTと違い、学習時には可視パッチだけをエンコーダに入れるため、計算効率を上げやすい設計です。\n\n完全な流れは\n`visible patches -> encoder -> (mask tokenで長さ復元) -> decoder -> masked patchesの再構成誤差`\nです。下のセルはこの流れを最小化して追うデモです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_random_mask(n_tokens, mask_ratio=0.75, seed=0):\n    rng = np.random.default_rng(seed)\n    n_mask = int(n_tokens * mask_ratio)\n    perm = rng.permutation(n_tokens)\n    mask_idx = perm[:n_mask]\n    keep_idx = np.sort(perm[n_mask:])\n    return keep_idx, np.sort(mask_idx)\n\n\nn_patches = patches.shape[0]\nkeep_idx, mask_idx = make_random_mask(n_patches, mask_ratio=0.75, seed=1)\nkept_tokens = patch_tokens[keep_idx]\n\nprint('all patches :', n_patches)\nprint('kept patches:', len(keep_idx), 'indices=', keep_idx)\nprint('masked      :', len(mask_idx), 'indices=', mask_idx)\nprint('encoder input token shape (without CLS):', kept_tokens.shape)\n\n# デコーダ入力側で元の長さに戻す（masked位置にはmask tokenを置く）\nmask_token = np.zeros((len(mask_idx), patch_tokens.shape[1]))\ndecoder_input = np.zeros_like(patch_tokens)\ndecoder_input[keep_idx] = kept_tokens\ndecoder_input[mask_idx] = mask_token\n\n# 最小デモ: 線形デコーダでパッチを復元し、masked部分のみ誤差を計算\nW_rec = np.random.default_rng(2).normal(0, 0.3, size=(patch_tokens.shape[1], patches.shape[1]))\nrecon_patches = decoder_input @ W_rec\nmasked_recon_mse = np.mean((recon_patches[mask_idx] - patches[mask_idx]) ** 2)\nprint('masked reconstruction MSE (toy):', round(float(masked_recon_mse), 6))\n\nmasked_view = img.copy()\npatch_size = 2\nfor idx in mask_idx:\n    gy = idx // (img.shape[1] // patch_size)\n    gx = idx % (img.shape[1] // patch_size)\n    y0, x0 = gy * patch_size, gx * patch_size\n    masked_view[y0:y0+patch_size, x0:x0+patch_size] = 0.0\n\nfig, axes = plt.subplots(1, 2, figsize=(7.2, 3.4))\naxes[0].imshow(img, cmap='gray', vmin=0, vmax=1)\naxes[0].set_title('original')\naxes[0].axis('off')\naxes[1].imshow(masked_view, cmap='gray', vmin=0, vmax=1)\naxes[1].set_title('MAE masking (75%)')\naxes[1].axis('off')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VAEも再構成を使いますが、MAEと目的が異なります。\nVAEは潜在変数 `z` の確率分布を学ぶ生成モデルで、\n損失は `再構成誤差 + β * KL` です。\n\n一方MAEは、主に表現学習を目的に「隠したパッチの再構成」を解きます。\n下のセルはVAE損失のうち、KL項と再構成項の最小計算デモです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VAEの再パラメータ化トリックと損失項の最小計算\nmu = np.array([0.2, -0.4, 0.1], dtype=np.float64)\nlogvar = np.array([-0.2, 0.3, -0.5], dtype=np.float64)\n\nrng = np.random.default_rng(42)\neps = rng.normal(0.0, 1.0, size=mu.shape)\nstd = np.exp(0.5 * logvar)\nz = mu + std * eps\n\n# KL項\nkl = -0.5 * np.sum(1 + logvar - mu**2 - np.exp(logvar))\n\n# 再構成項（ここではMSEの玩具例）\nx_true = np.array([0.9, 0.2, 0.7], dtype=np.float64)\nx_recon = np.array([0.8, 0.3, 0.6], dtype=np.float64)\nrecon_loss = np.mean((x_true - x_recon) ** 2)\n\nbeta = 1.0\nvae_loss = recon_loss + beta * kl\n\nprint('mu    =', np.round(mu, 4))\nprint('logvar=', np.round(logvar, 4))\nprint('z sample =', np.round(z, 4))\nprint('recon loss =', round(float(recon_loss), 6))\nprint('KL(q(z|x) || p(z)) =', round(float(kl), 6))\nprint('VAE loss = recon + beta*KL =', round(float(vae_loss), 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "最後に、PyTorchで小さなDecoder-only Transformer（GPT型）を学習し、\n「次トークン予測」が実際に動くことを確認します。\n\nここでは次トークンが直前2トークンに依存する列を使い、文脈参照が必要な設定にしています。\n実装は簡略化のため `TransformerEncoderLayer + 因果マスク` で、GPT型の挙動を再現します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n    torch.manual_seed(0)\n\n    vocab_size = 20\n    seq_len = 14\n    d_model = 48\n\n    def sample_batch(batch_size=64):\n        # Fibonacci-like modulo sequence: x_t = (x_{t-1} + x_{t-2}) mod vocab\n        seq = torch.zeros(batch_size, seq_len + 1, dtype=torch.long)\n        seq[:, 0] = torch.randint(0, vocab_size, (batch_size,))\n        seq[:, 1] = torch.randint(0, vocab_size, (batch_size,))\n        for t in range(2, seq_len + 1):\n            seq[:, t] = (seq[:, t - 1] + seq[:, t - 2]) % vocab_size\n        return seq[:, :-1], seq[:, 1:]\n\n    class TinyGPT(nn.Module):\n        def __init__(self, vocab_size, d_model=48, nhead=4, num_layers=2, seq_len=14):\n            super().__init__()\n            self.seq_len = seq_len\n            self.token_emb = nn.Embedding(vocab_size, d_model)\n            self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n            layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=nhead,\n                dim_feedforward=4 * d_model,\n                dropout=0.0,\n                batch_first=True,\n                activation='gelu',\n            )\n            self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n            self.norm = nn.LayerNorm(d_model)\n            self.head = nn.Linear(d_model, vocab_size)\n\n        def forward(self, x):\n            bsz, t = x.shape\n            h = self.token_emb(x) + self.pos_emb[:, :t, :]\n            mask = torch.triu(torch.ones(t, t, device=x.device), diagonal=1).bool()\n            h = self.encoder(h, mask=mask)\n            h = self.norm(h)\n            return self.head(h)\n\n    model = TinyGPT(vocab_size=vocab_size, d_model=d_model, nhead=4, num_layers=2, seq_len=seq_len)\n    optimizer = optim.AdamW(model.parameters(), lr=3e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    for step in range(180):\n        x, y = sample_batch(batch_size=64)\n        logits = model(x)\n\n        # t=0 の予測は初期2トークン中1つ目だけでは不確定なので除外\n        loss = criterion(logits[:, 1:, :].reshape(-1, vocab_size), y[:, 1:].reshape(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if step % 45 == 0:\n            print(f'step={step:>3d}, loss={loss.item():.4f}')\n\n    model.eval()\n    seed = torch.tensor([[3, 7]], dtype=torch.long)\n    generated = seed.clone()\n    for _ in range(10):\n        cur = generated[:, -seq_len:]\n        logits = model(cur)\n        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n        generated = torch.cat([generated, next_token], dim=1)\n\n    print('generated tokens:', generated.squeeze(0).tolist())\nelse:\n    print('PyTorch未導入のためGPTミニ実験セルはスキップしました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformerを使うときは、「どのトークン集合を作るか」が設計の中心になります。\n文章なら単語列、画像ならパッチ列、MAEなら可視パッチ列です。\n同じ自己注意の枠組みでも、トークン化と目的関数を変えることでGPT/ViT/MAEのように振る舞いが変わります。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
