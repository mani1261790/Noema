{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\nimport random\nimport statistics\nimport time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# エネルギーベースモデル\n\nエネルギーベースモデル（EBM）は、確率分布を「正規化された確率」ではなく「エネルギー関数」で表す考え方です。データらしい点には低いエネルギーを、データらしくない点には高いエネルギーを与えるように学習します。\n\nこのノートでは、1次元の最小実装で EBM の核を確認します。特に、SGLD と Replay Buffer を使った学習が何をしているかをコードで追います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EBM の基本式は次です。\n\n$$\np_\\theta(x)=\\frac{\\exp(-E_\\theta(x))}{Z_\\theta},\\qquad Z_\\theta=\\int \\exp(-E_\\theta(x))dx\n$$\n\nここで難しいのは分配関数 $Z_\\theta$ です。高次元では厳密計算がほぼ不可能なので、学習では\n\n- データ点のエネルギーを下げる\n- モデルサンプル（負例）のエネルギーを上げる\n\nという contrastive な差分で更新します。IGEBM系の実装でもこの考え方が中心です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず学習対象データを作ります。2モード混合分布を使い、モード構造を再現できるか確認しやすくします。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(31)\n\ndef sample_real(n: int):\n    xs = []\n    for _ in range(n):\n        if random.random() < 0.55:\n            xs.append(random.gauss(-2.0, 0.45))\n        else:\n            xs.append(random.gauss(1.8, 0.55))\n    return xs\n\n\ndef describe(xs):\n    left = sum(1 for x in xs if x < 0) / len(xs)\n    right = 1.0 - left\n    return {\n        'mean': statistics.mean(xs),\n        'std': statistics.pstdev(xs),\n        'left': left,\n        'right': right,\n    }\n\nreal_data = sample_real(2400)\nstats = describe(real_data)\nprint('dataset size =', len(real_data))\nprint('mean/std     =', round(stats['mean'], 4), round(stats['std'], 4))\nprint('left/right   =', round(stats['left'], 4), round(stats['right'], 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "今回は 2 つの井戸（well）を持つエネルギー関数を使います。\n\n- `e1(x)=a1(x-m1)^2 + k1`\n- `e2(x)=a2(x-m2)^2 + k2`\n- `E(x)=-log(exp(-e1)+exp(-e2))+c`\n\n`a1,a2` は正である必要があるため `a=exp(raw_a)` として実装します。`k1,k2` は井戸の深さを調整する項で、モード比率（重み）を表現しやすくする役割があります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def stable_logsumexp(a, b):\n    m = a if a > b else b\n    return m + math.log(math.exp(a - m) + math.exp(b - m))\n\n\ndef clip_raw_a(v):\n    return max(-5.0, min(5.0, v))\n\n\ndef energy_value(x, theta):\n    # theta = [raw_a1, m1, k1, raw_a2, m2, k2, c]\n    raw_a1, m1, k1, raw_a2, m2, k2, c = theta\n    a1 = math.exp(clip_raw_a(raw_a1))\n    a2 = math.exp(clip_raw_a(raw_a2))\n\n    e1 = a1 * (x - m1) ** 2 + k1\n    e2 = a2 * (x - m2) ** 2 + k2\n    return -stable_logsumexp(-e1, -e2) + c\n\n\ndef energy_and_grads(x, theta):\n    raw_a1, m1, k1, raw_a2, m2, k2, c = theta\n    raw_a1 = clip_raw_a(raw_a1)\n    raw_a2 = clip_raw_a(raw_a2)\n    a1 = math.exp(raw_a1)\n    a2 = math.exp(raw_a2)\n\n    d1 = x - m1\n    d2 = x - m2\n    e1 = a1 * d1 * d1 + k1\n    e2 = a2 * d2 * d2 + k2\n\n    u1 = math.exp(-e1)\n    u2 = math.exp(-e2)\n    den = u1 + u2\n    w1 = u1 / den\n    w2 = 1.0 - w1\n\n    energy = -math.log(den) + c\n\n    grad_x = w1 * (2.0 * a1 * d1) + w2 * (2.0 * a2 * d2)\n\n    grad_raw_a1 = w1 * (d1 * d1) * a1\n    grad_m1 = w1 * (-2.0 * a1 * d1)\n    grad_k1 = w1\n\n    grad_raw_a2 = w2 * (d2 * d2) * a2\n    grad_m2 = w2 * (-2.0 * a2 * d2)\n    grad_k2 = w2\n\n    grad_c = 1.0\n\n    grads = [grad_raw_a1, grad_m1, grad_k1, grad_raw_a2, grad_m2, grad_k2, grad_c]\n    return energy, grad_x, grads\n\n\ntheta_demo = [0.1, -1.2, 0.0, 0.2, 1.3, 0.2, 0.0]\nfor x in [-2.0, -0.2, 1.5]:\n    e, gx, gp = energy_and_grads(x, theta_demo)\n    print('x=', x, 'E=', round(e, 5), 'dE/dx=', round(gx, 5), 'dE/dk1=', round(gp[2], 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に SGLD で負例を作ります。\n\n`x <- x - step_size * dE/dx + noise_std * N(0,1)`\n\n`-dE/dx` はエネルギーを下げる方向なので、低エネルギー領域に集まりやすくなります。ノイズ項は探索性を保つ役割を持ちます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SampleReplayBuffer:\n    def __init__(self, max_size=12000):\n        self.max_size = max_size\n        self.items = []\n\n    def add(self, xs):\n        self.items.extend(xs)\n        if len(self.items) > self.max_size:\n            self.items = self.items[-self.max_size :]\n\n    def sample(self, n):\n        if not self.items:\n            return []\n        if n >= len(self.items):\n            return self.items[:]\n        return random.sample(self.items, n)\n\n\ndef sgld_samples(theta, x_init, n_steps=40, step_size=0.06, noise_std=0.08):\n    xs = x_init[:]\n    for _ in range(n_steps):\n        for i in range(len(xs)):\n            _, grad_x, _ = energy_and_grads(xs[i], theta)\n            xs[i] = xs[i] - step_size * grad_x + noise_std * random.gauss(0.0, 1.0)\n            xs[i] = max(-6.0, min(6.0, xs[i]))\n    return xs\n\n\nbuffer = SampleReplayBuffer(max_size=5000)\nstart = [random.uniform(-4.0, 4.0) for _ in range(8)]\nsgld_out = sgld_samples(theta_demo, start, n_steps=30)\nprint('init  =', [round(v, 3) for v in start])\nprint('sgld  =', [round(v, 3) for v in sgld_out])\nbuffer.add(sgld_out)\nprint('buffer size =', len(buffer.items))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "学習目的は\n\n`L(θ)=E_data[Eθ(x)] - E_neg[Eθ(x)] + λ * regularizer`\n\nです。これを最小化すると、データ近傍は低エネルギー、負例近傍は高エネルギーになります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def mean_grads(xs, theta):\n    g = [0.0] * 7\n    e_sum = 0.0\n    for x in xs:\n        e, _, gp = energy_and_grads(x, theta)\n        e_sum += e\n        for i in range(7):\n            g[i] += gp[i]\n    n = len(xs)\n    return e_sum / n, [v / n for v in g]\n\n\ndef train_ebm(theta_init, real_data, steps=380, batch_size=96, lr=0.016, reg=8e-4, replay_ratio=0.7):\n    theta = theta_init[:]\n    replay = SampleReplayBuffer(max_size=14000)\n    history = []\n\n    for step in range(steps + 1):\n        x_data = random.sample(real_data, batch_size)\n\n        n_replay = int(batch_size * replay_ratio)\n        x_neg0 = replay.sample(n_replay)\n        if len(x_neg0) < batch_size:\n            x_neg0 = x_neg0 + [random.uniform(-4.5, 4.5) for _ in range(batch_size - len(x_neg0))]\n\n        x_neg = sgld_samples(theta, x_neg0, n_steps=30, step_size=0.055, noise_std=0.085)\n        replay.add(x_neg)\n\n        e_data, g_data = mean_grads(x_data, theta)\n        e_neg, g_neg = mean_grads(x_neg, theta)\n\n        # L = E_data - E_neg + reg * (raw_a1^2 + raw_a2^2 + 0.2*(k1^2+k2^2))\n        grads = [g_data[i] - g_neg[i] for i in range(7)]\n        grads[0] += 2.0 * reg * theta[0]\n        grads[3] += 2.0 * reg * theta[3]\n        grads[2] += 2.0 * reg * 0.2 * theta[2]\n        grads[5] += 2.0 * reg * 0.2 * theta[5]\n\n        theta = [theta[i] - lr * grads[i] for i in range(7)]\n\n        theta[0] = max(-3.0, min(3.0, theta[0]))\n        theta[3] = max(-3.0, min(3.0, theta[3]))\n        theta[1] = max(-4.0, min(4.0, theta[1]))\n        theta[4] = max(-4.0, min(4.0, theta[4]))\n        theta[2] = max(-3.0, min(3.0, theta[2]))\n        theta[5] = max(-3.0, min(3.0, theta[5]))\n\n        if step % 40 == 0:\n            gap = e_data - e_neg\n            history.append((step, e_data, e_neg, gap, theta[:], len(replay.items)))\n\n    return theta, history, replay\n\n\nrandom.seed(31)\ntheta0 = [0.0, -0.2, 0.0, 0.0, 0.2, 0.0, 0.0]\ntrained_theta, history, replay = train_ebm(theta0, real_data)\n\nfor step, e_d, e_n, gap, th, rs in history:\n    print(\n        f'step={step:03d}',\n        f'E_data={round(e_d,4)}',\n        f'E_neg={round(e_n,4)}',\n        f'gap={round(gap,4)}',\n        f'theta={[round(v,4) for v in th]}',\n        f'replay={rs}'\n    )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def draw_model_samples(theta, n=2600):\n    init = [random.uniform(-4.5, 4.5) for _ in range(n)]\n    return sgld_samples(theta, init, n_steps=60, step_size=0.05, noise_std=0.07)\n\n\nrandom.seed(31)\nmodel_samples = draw_model_samples(trained_theta)\nreal_stats = describe(real_data)\nmodel_stats = describe(model_samples)\n\nprint('real  mean/std =', round(real_stats['mean'], 4), round(real_stats['std'], 4))\nprint('model mean/std =', round(model_stats['mean'], 4), round(model_stats['std'], 4))\nprint('real  left/right =', round(real_stats['left'], 4), round(real_stats['right'], 4))\nprint('model left/right =', round(model_stats['left'], 4), round(model_stats['right'], 4))\nprint('trained theta =', [round(v, 4) for v in trained_theta])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def approx_partition(theta, x_min=-6.0, x_max=6.0, n_grid=5000):\n    dx = (x_max - x_min) / n_grid\n    s = 0.0\n    for i in range(n_grid):\n        x = x_min + (i + 0.5) * dx\n        s += math.exp(-energy_value(x, theta))\n    return s * dx\n\n\ndef approx_density(theta, x, z_est):\n    return math.exp(-energy_value(x, theta)) / z_est\n\n\nz_est = approx_partition(trained_theta)\nprint('approx Z =', round(z_est, 6))\n\nprobe_points = [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\nfor x in probe_points:\n    p = approx_density(trained_theta, x, z_est)\n    print('x=', f'{x:>4.1f}', 'E=', round(energy_value(x, trained_theta), 4), 'p~', round(p, 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここまでで、EBMの実装上の核心を確認できました。\n\n- 分配関数を直接最適化しない\n- SGLDで負例を生成する\n- Replay Bufferで負例初期値を再利用する\n\nIGEBMなどの実践モデルでは、ここに CNN/ResNet、スペクトル正規化、強いデータ拡張を組み合わせて高次元へ拡張します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 学習失敗の典型例: SGLDステップ不足\n\ndef quick_train_with_sgld_steps(real_data, sgld_steps):\n    random.seed(77 + sgld_steps)\n    theta = [0.0, -0.1, 0.0, 0.0, 0.1, 0.0, 0.0]\n    replay = SampleReplayBuffer(max_size=5000)\n\n    for _ in range(140):\n        x_data = random.sample(real_data, 64)\n        x_neg0 = replay.sample(44)\n        x_neg0 = x_neg0 + [random.uniform(-4.5, 4.5) for _ in range(64 - len(x_neg0))]\n        x_neg = sgld_samples(theta, x_neg0, n_steps=sgld_steps, step_size=0.055, noise_std=0.085)\n        replay.add(x_neg)\n\n        _, g_data = mean_grads(x_data, theta)\n        _, g_neg = mean_grads(x_neg, theta)\n        grads = [g_data[i] - g_neg[i] for i in range(7)]\n        theta = [theta[i] - 0.016 * grads[i] for i in range(7)]\n        theta[0] = max(-3.0, min(3.0, theta[0]))\n        theta[3] = max(-3.0, min(3.0, theta[3]))\n\n    samples = draw_model_samples(theta, n=1200)\n    return describe(samples)\n\n\nstats_short = quick_train_with_sgld_steps(real_data, sgld_steps=5)\nstats_long = quick_train_with_sgld_steps(real_data, sgld_steps=35)\n\nprint('short SGLD steps (5) :', {k: round(v, 4) for k, v in stats_short.items()})\nprint('long  SGLD steps (35):', {k: round(v, 4) for k, v in stats_long.items()})\nprint('real target          :', {k: round(v, 4) for k, v in describe(real_data).items()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SGLDステップが少なすぎると、負例がモデル分布を十分に近似できず、エネルギー地形の更新が偏ります。実務では、SGLDステップ数、ノイズ強度、Replay比率をセットで調整します。\n\nEBMの視点を持つと、スコアモデルや拡散モデルで出てくる「勾配で分布へ寄せる」考え方も同じ地図の上で理解しやすくなります。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
