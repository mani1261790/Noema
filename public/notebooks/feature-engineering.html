<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>feature-engineering</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="特徴量エンジニアリング">特徴量エンジニアリング</h1>
<p>モデルの性能は、アルゴリズム選択だけでなく「どの入力特徴量を作るか」で大きく変わります。<br>
特徴量エンジニアリングは、元データをモデルが学びやすい表現に変換する作業です。</p>
<p>このノートでは、欠損値・カテゴリ変数・スケーリング・非線形変換・交互作用を、<br>
実務で使う <code>Pipeline</code> / <code>ColumnTransformer</code> と結びつけて確認します。</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures, FunctionTransformer
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor

sns.set_theme(style=&quot;whitegrid&quot;, context=&quot;notebook&quot;)</code></pre>
<h2 id="1-元データを観察する">1. 元データを観察する</h2>
<p>まずは「どんな列があるか」を確認します。<br>
特徴量設計の第一歩は、モデル前にデータの意味と型を把握することです。</p>
<p>以下のデータは教材用の疑似データです。</p>
<ul>
<li><code>area</code>, <code>rooms</code>, <code>age</code>, <code>distance_to_station</code>, <code>floor</code> が価格に影響するという仮定で価格を生成</li>
<li><code>station</code> は立地プレミアムを表すカテゴリ列</li>
<li><code>noise</code> は観測できない要因（内装、周辺環境など）をまとめた誤差</li>
</ul>
<p>また、実務に寄せるために数値列とカテゴリ列へ意図的に欠損を入れています。</p>

<pre><code class="language-python">rng = np.random.default_rng(42)
n = 600

area = rng.normal(65, 18, n).clip(20, 150)
rooms = rng.integers(1, 6, n)
age = rng.integers(0, 40, n)
distance = rng.normal(18, 8, n).clip(1, 45)
station = rng.choice([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;], size=n, p=[0.30, 0.30, 0.25, 0.15])
floor = rng.integers(1, 16, n)

station_effect = pd.Series(station).map({&quot;A&quot;: 180, &quot;B&quot;: 120, &quot;C&quot;: 60, &quot;D&quot;: 20}).to_numpy()
noise = rng.normal(0, 35, n)
price = 45 + 4.2 * area + 16 * rooms - 2.3 * age - 3.8 * distance + station_effect + 2.8 * floor + noise

df = pd.DataFrame({
    &quot;area&quot;: area,
    &quot;rooms&quot;: rooms,
    &quot;age&quot;: age,
    &quot;distance_to_station&quot;: distance,
    &quot;station&quot;: station,
    &quot;floor&quot;: floor,
    &quot;price&quot;: price,
})

# 欠損を意図的に作る
missing_idx_num = rng.choice(df.index, size=35, replace=False)
missing_idx_cat = rng.choice(df.index, size=20, replace=False)
df.loc[missing_idx_num, &quot;distance_to_station&quot;] = np.nan
df.loc[missing_idx_cat, &quot;station&quot;] = np.nan

df.head()</code></pre>
<p><code>info</code> と <code>isna</code> で型と欠損を確認します。<br>
この確認を飛ばすと、後段で変換失敗やリークの原因になります。</p>

<pre><code class="language-python">print(df.info())
print(&quot;\n欠損数:\n&quot;, df.isna().sum())</code></pre>
<h2 id="2-変換前後で分布を比べる">2. 変換前後で分布を比べる</h2>
<p>歪んだ分布に対して対数変換が有効な場面があります。<br>
ここでは駅距離の分布を <code>log1p</code> 変換前後で比較します。</p>

<pre><code class="language-python">fig, axes = plt.subplots(1, 2, figsize=(10, 3.6))

sns.histplot(df[&quot;distance_to_station&quot;], bins=30, ax=axes[0], kde=True)
axes[0].set_title(&quot;original distance&quot;)

sns.histplot(np.log1p(df[&quot;distance_to_station&quot;]), bins=30, ax=axes[1], kde=True)
axes[1].set_title(&quot;log1p(distance)&quot;)

plt.tight_layout()
plt.show()</code></pre>
<h2 id="3-ベースライン-最小限の前処理">3. ベースライン（最小限の前処理）</h2>
<p>最初に単純なベースラインを作ります。<br>
比較対象があると、後で追加した特徴量の効果を正しく評価できます。</p>

<pre><code class="language-python">X = df.drop(columns=[&quot;price&quot;])
y = df[&quot;price&quot;]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

num_cols = [&quot;area&quot;, &quot;rooms&quot;, &quot;age&quot;, &quot;distance_to_station&quot;, &quot;floor&quot;]
cat_cols = [&quot;station&quot;]

baseline_preprocess = ColumnTransformer([
    (&quot;num&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)),
        (&quot;scaler&quot;, StandardScaler()),
    ]), num_cols),
    (&quot;cat&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;most_frequent&quot;)),
        (&quot;onehot&quot;, OneHotEncoder(handle_unknown=&quot;ignore&quot;)),
    ]), cat_cols),
])

baseline_model = Pipeline([
    (&quot;preprocess&quot;, baseline_preprocess),
    (&quot;regressor&quot;, LinearRegression()),
])

baseline_model.fit(X_train, y_train)
baseline_pred = baseline_model.predict(X_test)

print(f&quot;baseline MAE: {mean_absolute_error(y_test, baseline_pred):.2f}&quot;)
print(f&quot;baseline R2 : {r2_score(y_test, baseline_pred):.3f}&quot;)</code></pre>
<h2 id="4-特徴量を追加する">4. 特徴量を追加する</h2>
<p>次に、ドメイン知識を反映した特徴を追加します。</p>
<ul>
<li><code>area_per_room</code>: 1部屋あたり面積。広さと間取りを1つの軸で表せる</li>
<li><code>is_new</code>: 築浅フラグ。築年数の閾値効果を捉えやすくする</li>
<li><code>log_distance</code>: 駅距離の対数。長い尾を圧縮して線形モデルで扱いやすくする</li>
</ul>
<p>こうした手作業特徴は、モデルにとって有効な表現を直接与える手段です。</p>

<pre><code class="language-python">def add_features(frame: pd.DataFrame) -&gt; pd.DataFrame:
    out = frame.copy()
    out[&quot;area_per_room&quot;] = out[&quot;area&quot;] / np.maximum(out[&quot;rooms&quot;], 1)
    out[&quot;is_new&quot;] = (out[&quot;age&quot;] &lt;= 5).astype(int)
    out[&quot;log_distance&quot;] = np.log1p(out[&quot;distance_to_station&quot;])
    return out

feature_num_cols = [
    &quot;area&quot;, &quot;rooms&quot;, &quot;age&quot;, &quot;distance_to_station&quot;, &quot;floor&quot;,
    &quot;area_per_room&quot;, &quot;is_new&quot;, &quot;log_distance&quot;
]
feature_cat_cols = [&quot;station&quot;]

feature_preprocess = ColumnTransformer([
    (&quot;num&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)),
        (&quot;scaler&quot;, StandardScaler()),
    ]), feature_num_cols),
    (&quot;cat&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;most_frequent&quot;)),
        (&quot;onehot&quot;, OneHotEncoder(handle_unknown=&quot;ignore&quot;)),
    ]), feature_cat_cols),
])

feature_model = Pipeline([
    (&quot;feature_builder&quot;, FunctionTransformer(add_features, validate=False)),
    (&quot;preprocess&quot;, feature_preprocess),
    (&quot;regressor&quot;, Ridge(alpha=1.0, random_state=42)),
])

feature_model.fit(X_train, y_train)
feature_pred = feature_model.predict(X_test)

print(f&quot;feature MAE: {mean_absolute_error(y_test, feature_pred):.2f}&quot;)
print(f&quot;feature R2 : {r2_score(y_test, feature_pred):.3f}&quot;)</code></pre>
<h2 id="5-交互作用特徴を試す">5. 交互作用特徴を試す</h2>
<p>線形モデルでは、列同士の掛け算項（交互作用）を入れると表現力が上がる場合があります。<br>
<code>PolynomialFeatures(degree=2)</code> は、2次の項（例: <code>area^2</code>）や交互作用項（例: <code>area * rooms</code>）を自動生成します。</p>
<p>次元が増えやすいので、正則化や検証とセットで使います。</p>

<pre><code class="language-python">poly_num_cols = [&quot;area&quot;, &quot;rooms&quot;, &quot;age&quot;, &quot;distance_to_station&quot;, &quot;floor&quot;]

poly_preview = PolynomialFeatures(degree=2, include_bias=False)
poly_preview.fit(X_train[poly_num_cols])
poly_names = poly_preview.get_feature_names_out(poly_num_cols)
print(&quot;num of polynomial features:&quot;, len(poly_names))
print(&quot;sample names:&quot;, poly_names[:12])

poly_preprocess = ColumnTransformer([
    (&quot;num_poly&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)),
        (&quot;poly&quot;, PolynomialFeatures(degree=2, include_bias=False)),
        (&quot;scaler&quot;, StandardScaler()),
    ]), poly_num_cols),
    (&quot;cat&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;most_frequent&quot;)),
        (&quot;onehot&quot;, OneHotEncoder(handle_unknown=&quot;ignore&quot;)),
    ]), cat_cols),
])

poly_model = Pipeline([
    (&quot;preprocess&quot;, poly_preprocess),
    (&quot;regressor&quot;, Ridge(alpha=2.0, random_state=42)),
])

poly_model.fit(X_train, y_train)
poly_pred = poly_model.predict(X_test)

print(f&quot;poly MAE: {mean_absolute_error(y_test, poly_pred):.2f}&quot;)
print(f&quot;poly R2 : {r2_score(y_test, poly_pred):.3f}&quot;)</code></pre>
<h2 id="6-木モデルとの比較">6. 木モデルとの比較</h2>
<p>木ベースモデルはスケーリングに依存しにくく、<br>
非線形関係も自動で取り込みやすいので、特徴量設計との相性を確認しやすいです。</p>

<pre><code class="language-python">tree_preprocess = ColumnTransformer([
    (&quot;num&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)),
    ]), feature_num_cols),
    (&quot;cat&quot;, Pipeline([
        (&quot;imputer&quot;, SimpleImputer(strategy=&quot;most_frequent&quot;)),
        (&quot;onehot&quot;, OneHotEncoder(handle_unknown=&quot;ignore&quot;)),
    ]), feature_cat_cols),
])

tree_model = Pipeline([
    (&quot;feature_builder&quot;, FunctionTransformer(add_features, validate=False)),
    (&quot;preprocess&quot;, tree_preprocess),
    (&quot;regressor&quot;, RandomForestRegressor(
        n_estimators=300,
        max_depth=10,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
    )),
])

tree_model.fit(X_train, y_train)
tree_pred = tree_model.predict(X_test)

print(f&quot;random forest MAE: {mean_absolute_error(y_test, tree_pred):.2f}&quot;)
print(f&quot;random forest R2 : {r2_score(y_test, tree_pred):.3f}&quot;)</code></pre>
<h2 id="7-リークを避けるための注意">7. リークを避けるための注意</h2>
<p>特徴量エンジニアリングで最も危険なのはデータリークです。<br>
例えば「テストデータを見てから欠損補完値を決める」「目的変数に依存する列を特徴に入れる」は禁止です。</p>
<p><code>Pipeline</code> / <code>ColumnTransformer</code> を使って訓練データの手順を固定すれば、<br>
推論時にも同じ変換を安全に適用できます。</p>
<p>次のセルでは、4つのモデルを 5-fold CV で同一基準比較します。<br>
<code>cross_val_score</code> の <code>neg_mean_absolute_error</code> は「大きいほど良い」形式に合わせるため負符号付きで返るため、<br>
表示時に <code>-scores.mean()</code> として通常の MAE（小さいほど良い）へ戻しています。</p>

<pre><code class="language-python">models = {
    &quot;baseline_linear&quot;: baseline_model,
    &quot;feature_ridge&quot;: feature_model,
    &quot;poly_ridge&quot;: poly_model,
    &quot;tree_rf&quot;: tree_model,
}

rows = []
for name, model in models.items():
    # すべての前処理（特徴量追加を含む）をパイプライン内で実行
    scores = cross_val_score(
        model,
        X,
        y,
        cv=5,
        scoring=&quot;neg_mean_absolute_error&quot;,
        n_jobs=-1,
    )
    rows.append({
        &quot;model&quot;: name,
        &quot;cv_mae_mean&quot;: -scores.mean(),
        &quot;cv_mae_std&quot;: scores.std(),
    })

pd.DataFrame(rows).sort_values(&quot;cv_mae_mean&quot;)</code></pre>
<h2 id="まとめ">まとめ</h2>
<p>特徴量エンジニアリングでは、</p>
<ol>
<li>データの型と欠損を把握する</li>
<li>ベースラインを作る</li>
<li>意味のある特徴を追加して比較する</li>
<li>CVで効果を確認する</li>
</ol>
<p>という順序を崩さないことが重要です。</p>
<p>精度改善だけでなく、リークを避けて再現可能な前処理を作ることが、実務で最も価値のあるポイントです。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>