{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\nimport random\nimport re\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    TORCH_AVAILABLE = True\nexcept ModuleNotFoundError:\n    torch = None\n    nn = None\n    optim = None\n    TORCH_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 事前学習（Pre-training）\n\n事前学習は、大量の未ラベルテキストから「次にどのトークンが来るか」を予測することで、言語の統計構造を学ぶ段階です。\nこのノートでは、データ準備、N-gramからニューラル言語モデルへの流れ、評価（perplexity）、計算コスト感覚、そして機械論的解釈可能性の入口までを一貫して確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "言語モデルの基本目的は次の確率を最大化することです。\n\n`P(w_1, ..., w_T) = Π_t P(w_t | w_{<t})`\n\nここで `w_{<t}` は `w_1 ... w_{t-1}`（時刻 `t` より前のトークン列）です。\n実装ではクロスエントロピー損失を最小化します。\n評価では perplexity（困惑度）を使い、`perplexity = exp(平均クロスエントロピー)`、低いほど予測が当たりやすいことを意味します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 小さな日本語コーパス（教育用）\nraw_docs = [\n    '事前学習では大量のテキストから次トークン予測を学ぶ。',\n    '言語モデルは文脈に応じた分布を出力する。',\n    'トークン化の設計は学習効率と性能に強く影響する。',\n    '前処理では重複除去や品質フィルタリングが重要になる。',\n    '評価ではperplexityや下流タスク性能を併用する。',\n    'モデル規模とデータ規模のバランスが学習の成否を左右する。',\n    '機械論的解釈可能性は内部回路の理解に役立つ。',\n    'SFTは事前学習済みモデルを指示追従に調整する工程である。',\n    '推論時は温度やサンプリング戦略が出力を変える。',\n    '長文文脈では注意機構の設計が効いてくる。',\n]\n\n\ndef normalize(s):\n    s = s.lower().strip()\n    s = re.sub(r'[。､，,.!?！？]', ' ', s)\n    s = re.sub(r'\\s+', ' ', s)\n    return s\n\n\ndef char_tokenize(s):\n    s = normalize(s).replace(' ', '')\n    return list(s)\n\n\ndocs = [char_tokenize(d) for d in raw_docs]\nrandom.seed(0)\nrandom.shuffle(docs)\nsplit = int(len(docs) * 0.8)\ntrain_docs = docs[:split]\nval_docs = docs[split:]\n\nprint('train docs:', len(train_docs), 'val docs:', len(val_docs))\nprint('sample train tokens:', ''.join(train_docs[0][:30]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_lengths = np.array([len(d) for d in train_docs], dtype=np.int64)\nval_lengths = np.array([len(d) for d in val_docs], dtype=np.int64)\n\nvocab = sorted(set(ch for doc in train_docs for ch in doc))\nvocab = ['<unk>'] + vocab\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for ch, i in stoi.items()}\nunk_id = stoi['<unk>']\n\nprint('vocab size:', len(vocab))\nprint('avg train length:', float(train_lengths.mean()))\nprint('avg val length  :', float(val_lengths.mean()))\n\nplt.figure(figsize=(6.8, 3.4))\nplt.hist(train_lengths, alpha=0.7, label='train', color='#7aa2ff')\nplt.hist(val_lengths, alpha=0.7, label='val', color='#8dd3a7')\nplt.xlabel('sequence length (characters)')\nplt.ylabel('count')\nplt.title('Length distribution after tokenization')\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まずはN-gram言語モデルを作り、事前学習の最小ベースラインを確認します。\n\n- Uni-gram: 直前文脈を使わず出現頻度だけで予測\n- Bi-gram: 1つ前のトークンを条件に次トークンを予測\n\nここでは加法スムージング（add-k）を入れて、未知遷移でも確率0を避けます。\nまた簡略化のためBOS/EOSは入れず、生成時の開始文字を固定しています（実運用ではBOS開始・EOS停止が一般的）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten(docs):\n    out = []\n    for d in docs:\n        out.extend(d)\n    return out\n\n\ndef to_ids(doc):\n    return [stoi.get(ch, unk_id) for ch in doc]\n\n\ntrain_ids = [to_ids(d) for d in train_docs]\nval_ids = [to_ids(d) for d in val_docs]\n\nunigram_counts = Counter(flatten(train_ids))\nbigram_counts = defaultdict(Counter)\nfor seq in train_ids:\n    for a, b in zip(seq[:-1], seq[1:]):\n        bigram_counts[a][b] += 1\n\nV = len(vocab)\n\n\ndef unigram_prob(tok, k=0.1):\n    total = sum(unigram_counts.values())\n    return (unigram_counts[tok] + k) / (total + k * V)\n\n\ndef bigram_prob(prev, tok, k=0.1):\n    row = bigram_counts[prev]\n    total = sum(row.values())\n    return (row[tok] + k) / (total + k * V)\n\n\ndef perplexity_unigram(seqs):\n    nll = 0.0\n    n = 0\n    for seq in seqs:\n        for t in seq:\n            p = unigram_prob(t)\n            nll += -math.log(p + 1e-12)\n            n += 1\n    return math.exp(nll / max(n, 1))\n\n\ndef perplexity_bigram(seqs):\n    nll = 0.0\n    n = 0\n    for seq in seqs:\n        for prev, t in zip(seq[:-1], seq[1:]):\n            p = bigram_prob(prev, t)\n            nll += -math.log(p + 1e-12)\n            n += 1\n    return math.exp(nll / max(n, 1))\n\n\nprint('val perplexity unigram:', round(perplexity_unigram(val_ids), 4))\nprint('val perplexity bigram :', round(perplexity_bigram(val_ids), 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_unigram(max_len=40):\n    probs = np.array([unigram_prob(i) for i in range(V)], dtype=np.float64)\n    probs = probs / probs.sum()\n    ids = np.random.choice(np.arange(V), size=max_len, p=probs)\n    return ''.join(itos[i] for i in ids if i != unk_id)\n\n\ndef sample_bigram(start_id, max_len=40):\n    out = [start_id]\n    cur = start_id\n    for _ in range(max_len - 1):\n        probs = np.array([bigram_prob(cur, j) for j in range(V)], dtype=np.float64)\n        probs = probs / probs.sum()\n        nxt = int(np.random.choice(np.arange(V), p=probs))\n        out.append(nxt)\n        cur = nxt\n    return ''.join(itos[i] for i in out if i != unk_id)\n\n\nnp.random.seed(1)\nstart_char = train_ids[0][0]\nprint('unigram sample:', sample_unigram(36))\nprint('bigram sample :', sample_bigram(start_char, 36))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、クロスエントロピー損失を手計算し、ニューラル言語モデル学習へ接続します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = np.array([0.2, -0.3, 1.1, 0.0], dtype=np.float64)\ntarget = 2\n\nshift = logits - np.max(logits)\nprobs = np.exp(shift) / np.sum(np.exp(shift))\nce = -math.log(probs[target] + 1e-12)\n\nprint('probs =', np.round(probs, 4))\nprint('cross entropy =', round(float(ce), 6))\nprint('perplexity for this token =', round(float(math.exp(ce)), 6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n    torch.manual_seed(0)\n\n    # 連結テキストを作成\n    train_text = ''.join(''.join(d) + '\\n' for d in train_docs)\n    val_text = ''.join(''.join(d) + '\\n' for d in val_docs)\n\n    train_data = torch.tensor([stoi.get(ch, unk_id) for ch in train_text], dtype=torch.long)\n    val_data = torch.tensor([stoi.get(ch, unk_id) for ch in val_text], dtype=torch.long)\n\n    max_block = min(48, len(train_data) - 1, len(val_data) - 1)\n    if max_block < 2:\n        raise ValueError('Dataset too short for neural LM demo after split')\n\n    block = max_block\n    batch_size = 32\n\n    def get_batch(data, block_size, bsz):\n        high = len(data) - block_size - 1\n        idx = torch.randint(0, high + 1, (bsz,))\n        x = torch.stack([data[i:i+block_size] for i in idx])\n        y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n        return x, y\n\n    class TinyPretrainLM(nn.Module):\n        def __init__(self, vocab_size, d_model=64):\n            super().__init__()\n            self.emb = nn.Embedding(vocab_size, d_model)\n            self.gru = nn.GRU(d_model, d_model, batch_first=True)\n            self.head = nn.Linear(d_model, vocab_size)\n\n        def forward(self, x):\n            h = self.emb(x)\n            out, _ = self.gru(h)\n            return self.head(out)\n\n    model = TinyPretrainLM(len(vocab), d_model=64)\n    opt = optim.AdamW(model.parameters(), lr=3e-3)\n    criterion_mean = nn.CrossEntropyLoss()\n    criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n\n    for step in range(260):\n        xb, yb = get_batch(train_data, block, batch_size)\n        logits = model(xb)\n        loss = criterion_mean(logits.reshape(-1, len(vocab)), yb.reshape(-1))\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        if step % 65 == 0:\n            print(f'step={step:>3d}, train loss={loss.item():.4f}')\n\n    # バッチ1個ではなく、検証系列全体で近似perplexityを計算\n    with torch.no_grad():\n        total_nll = 0.0\n        total_tok = 0\n        starts = list(range(0, len(val_data) - 1, block))\n        for s in starts:\n            b = min(block, len(val_data) - 1 - s)\n            if b <= 0:\n                continue\n            x = val_data[s:s+b].unsqueeze(0)\n            y = val_data[s+1:s+b+1].unsqueeze(0)\n            logits = model(x)\n            nll = criterion_sum(logits.reshape(-1, len(vocab)), y.reshape(-1)).item()\n            total_nll += nll\n            total_tok += y.numel()\n\n        val_loss = total_nll / max(total_tok, 1)\n        val_ppl = math.exp(val_loss)\n\n    print('val avg nll =', round(val_loss, 4), 'val perplexity =', round(val_ppl, 4))\n\n    # 生成\n    prompt = '事前学習'\n    ids = [stoi.get(ch, unk_id) for ch in prompt]\n    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    for _ in range(40):\n        logits = model(x)\n        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n        x = torch.cat([x, next_id], dim=1)\n    gen = ''.join(itos[i] if i != unk_id else '□' for i in x.squeeze(0).tolist())\n    print('generated:', gen)\nelse:\n    print('PyTorch未導入のため、ニューラルLMセルをスキップしました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "事前学習ではデータ品質が非常に重要です。\n重複が多いと実効データ量が減り、汚染データがあると望ましくない振る舞いを学習します。\n下では文字3-gramのJaccard類似度で近重複を簡易検出します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shingles(text, k=3):\n    if len(text) < k:\n        return {text}\n    return {text[i:i+k] for i in range(len(text) - k + 1)}\n\n\ndef jaccard(a, b):\n    inter = len(a & b)\n    union = len(a | b)\n    return inter / max(union, 1)\n\n\ndedup_candidates = [\n    'llmの事前学習では大規模テキストを使う',\n    'llmの事前学習では大規模なテキストを使う',\n    '画像分類ではラベル付きデータで学習する',\n]\n\nS = [shingles(s, k=3) for s in dedup_candidates]\nfor i in range(len(S)):\n    for j in range(i + 1, len(S)):\n        sim = jaccard(S[i], S[j])\n        print(f'sim({i},{j}) = {sim:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "スケーリング則の細部は設定依存ですが、実務では「おおまかな計算量感覚」を先に持つのが重要です。\nここではデコーダ型Transformer学習の粗い目安として、\n\n`FLOPs ≈ 6 * N_params * N_tokens`\n\nを使って見積もります（係数はモデル形状・実装・ハードウェアで変動します）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_flops(params_billion, tokens_billion):\n    n_params = params_billion * 1e9\n    n_tokens = tokens_billion * 1e9\n    return 6.0 * n_params * n_tokens\n\n\nsettings = [\n    (0.1, 20),   # 0.1B params, 20B tokens\n    (0.7, 100),  # 0.7B params, 100B tokens\n    (7.0, 300),  # 7B params, 300B tokens\n]\n\nfor p_b, t_b in settings:\n    flops = estimate_flops(p_b, t_b)\n    print(f'params={p_b:>4.1f}B, tokens={t_b:>4.0f}B -> FLOPs≈{flops:.3e}')\n\n# トークン課金の例（仮定値）\nin_tok, out_tok = 1200, 400\nprice_per_m_in = 0.20\nprice_per_m_out = 0.80\ncost = (in_tok / 1e6) * price_per_m_in + (out_tok / 1e6) * price_per_m_out\nprint('\\nexample inference cost per request (assumed pricing) =', round(cost, 6), 'USD')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "事前学習後には、継続事前学習（domain adaptive pretraining）やSFTへ進むことが多いです。\n混合データでの学習では、一般コーパスとドメインコーパスの損失バランスを監視し、過学習や忘却を防ぎます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2種類のデータ損失を重み付きで合成する簡易例\nloss_general = np.array([2.20, 2.05, 1.98, 1.95, 1.93])\nloss_domain = np.array([2.80, 2.30, 2.00, 1.82, 1.74])\nalpha = 0.6  # general側の重み\n\nmixed = alpha * loss_general + (1 - alpha) * loss_domain\nfor i, (g, d, m) in enumerate(zip(loss_general, loss_domain, mixed), 1):\n    print(f'epoch {i}: general={g:.3f}, domain={d:.3f}, mixed={m:.3f}')\n\nplt.figure(figsize=(6.8, 3.5))\nplt.plot(loss_general, marker='o', label='general loss')\nplt.plot(loss_domain, marker='s', label='domain loss')\nplt.plot(mixed, marker='^', label='weighted objective')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Balancing continued pretraining objectives')\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "機械論的解釈可能性（mechanistic interpretability）は、ニューラルモデル内部の回路を理解する試みです。\n下の可視化は厳密な回路解析そのものではなく、\n「何が次に出やすいか」を統計遷移として読むための入口デモです。\n\n本格的には、注意ヘッドやMLPニューロン活性を直接解析して因果的に検証します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bi-gram遷移行列を可視化（解釈可能性の最小例）\nshow_chars = [ch for ch in ['事', '前', '学', '習', 'モ', 'デ', 'ル', '。'] if ch in stoi]\nshow_ids = [stoi[ch] for ch in show_chars]\n\nM = np.zeros((len(show_ids), len(show_ids)), dtype=np.float64)\nfor i, src in enumerate(show_ids):\n    for j, dst in enumerate(show_ids):\n        M[i, j] = bigram_prob(src, dst)\n\nrow_sums = M.sum(axis=1, keepdims=True)\nM_norm = M / np.maximum(row_sums, 1e-12)\n\nplt.figure(figsize=(6.0, 4.6))\nplt.imshow(M_norm, cmap='magma')\nplt.colorbar(label='P(next | current)')\nplt.xticks(range(len(show_chars)), show_chars)\nplt.yticks(range(len(show_chars)), show_chars)\nplt.xlabel('next token')\nplt.ylabel('current token')\nplt.title('Interpretable transition map (toy bigram)')\nplt.tight_layout()\nplt.show()\n\nfor i, src in enumerate(show_chars):\n    top = np.argsort(M_norm[i])[::-1][:3]\n    cand = [(show_chars[t], float(M_norm[i, t])) for t in top]\n    print(src, '->', [(c, round(p, 4)) for c, p in cand])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "事前学習では「データ」「目的関数」「計算資源」の3つを同時に設計する必要があります。\n最初に小さな実験で挙動を掴み、次に本番規模へスケールする手順を徹底すると、失敗コストを下げやすくなります。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
