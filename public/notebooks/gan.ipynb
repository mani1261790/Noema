{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\nimport random\nimport statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GAN\n\nGAN（Generative Adversarial Network）は、生成器 `G` と識別器 `D` を競わせることでデータ分布を学習する手法です。ここでは1次元データを使い、数式とコードを行き来しながら、学習がうまくいく条件と失敗する条件を確認します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "実データ分布を `p_data(x)`、生成分布を `p_g(x)` と書くと、目標は `p_g ≈ p_data` です。GANでは `D` が「本物か偽物か」を見分ける能力を上げ、`G` がその `D` をだます能力を上げます。\n\nこの設計の利点は、尤度を直接書きにくい問題でも、識別問題として学習を回せることです。一方で、学習は不安定になりやすく、目的関数とモデル設計の意図を理解していないと改善が難しくなります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず、2つの山を持つ1次元分布を実データとして作ります。2峰性のデータを使う理由は、mode collapse（片方の山しか出せなくなる失敗）を検出しやすくするためです。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(21)\n\ndef sample_real(n: int):\n    out = []\n    for _ in range(n):\n        if random.random() < 0.5:\n            out.append(random.gauss(-2.0, 0.35))\n        else:\n            out.append(random.gauss(2.0, 0.35))\n    return out\n\nreal_preview = sample_real(2000)\nprint('real mean =', round(statistics.mean(real_preview), 4))\nprint('real stdev =', round(statistics.pstdev(real_preview), 4))\nprint('left ratio =', round(sum(1 for x in real_preview if x < 0) / len(real_preview), 4))\nprint('right ratio=', round(sum(1 for x in real_preview if x >= 0) / len(real_preview), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GANの代表的な目的関数は次です。\n\n$$\n\\max_D\\; \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p(z)}[\\log(1-D(G(z)))]\n$$\n\n$$\n\\max_G\\; \\mathbb{E}_{z\\sim p(z)}[\\log D(G(z))]\\quad\\text{(non-saturating 版)}\n$$\n\n教科書で出る `\\min_G \\mathbb{E}[\\log(1-D(G(z)))]`（minimax 版）は、初期段階で勾配が弱くなりやすいので、実装では non-saturating 版がよく使われます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sigmoid(t: float) -> float:\n    if t >= 0:\n        e = math.exp(-t)\n        return 1.0 / (1.0 + e)\n    e = math.exp(t)\n    return e / (1.0 + e)\n\n\ndef clamp_prob(p: float, eps: float = 1e-8) -> float:\n    return min(1.0 - eps, max(eps, p))\n\n\ndef sample_z(n: int):\n    return [random.gauss(0.0, 1.0) for _ in range(n)]\n\n\n# 線形の最小モデル\n# G(z) = a z + b\n# D(x) = sigmoid(c x + d)\ndef generator_linear(z: float, theta):\n    a, b = theta\n    return a * z + b\n\n\ndef discriminator(x: float, phi):\n    c, d = phi\n    return sigmoid(c * x + d)\n\n\ndef losses_on_batch(theta, phi, x_real, z_batch, generator_fn):\n    x_fake = [generator_fn(z, theta) for z in z_batch]\n\n    d_real = [clamp_prob(discriminator(x, phi)) for x in x_real]\n    d_fake = [clamp_prob(discriminator(x, phi)) for x in x_fake]\n\n    ld = sum(math.log(p) for p in d_real) / len(d_real) + sum(math.log(1.0 - p) for p in d_fake) / len(d_fake)\n    lg_nonsat = sum(math.log(p) for p in d_fake) / len(d_fake)          # maximize\n    lg_minimax = sum(math.log(1.0 - p) for p in d_fake) / len(d_fake)   # minimize\n\n    return ld, lg_nonsat, lg_minimax, x_fake\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "theta0 = [0.15, 0.0]\nphi0 = [0.25, 0.0]\n\nx_real = sample_real(256)\nz_batch = sample_z(256)\nld0, lg0, lg_min0, x_fake0 = losses_on_batch(theta0, phi0, x_real, z_batch, generator_linear)\n\nprint('initial L_D (maximize)        =', round(ld0, 5))\nprint('initial L_G non-saturating    =', round(lg0, 5))\nprint('initial L_G minimax objective =', round(lg_min0, 5))\nprint('fake preview mean/stdev =', round(statistics.mean(x_fake0), 4), round(statistics.pstdev(x_fake0), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に学習を回します。今回は自動微分ではなく有限差分を使います。深層学習の本番では自動微分を使うべきですが、有限差分だと「どの目的を最大化・最小化しているか」が見えやすく、概念理解には有効です。\n\nこのノートでは `L_D` と `L_G` をどちらも「最大化」する書き方に統一しているため、更新式は `params = params + lr * grad` になります。一般的な最小化実装（`params - lr * grad`）と符号が逆に見えるのは、最適化している向きが違うためです。\n\nログに出す `W1` は実分布と生成分布の距離なので、小さいほど良い値です。`left` と `right` は左右モードの比率で、両者が 0.5 付近なら mode collapse が起きにくい状態と解釈できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def finite_diff_grad(fn, params, h: float = 1e-4):\n    grads = []\n    for i in range(len(params)):\n        plus = params[:]\n        minus = params[:]\n        plus[i] += h\n        minus[i] -= h\n        grads.append((fn(plus) - fn(minus)) / (2.0 * h))\n    return grads\ndef empirical_w1_1d(xs, ys):\n    n = min(len(xs), len(ys))\n    xs_sorted = sorted(xs)[:n]\n    ys_sorted = sorted(ys)[:n]\n    return sum(abs(a - b) for a, b in zip(xs_sorted, ys_sorted)) / n\ndef evaluate_distribution(theta, generator_fn, n=4000):\n    real = sample_real(n)\n    fake = [generator_fn(z, theta) for z in sample_z(n)]\n    left = sum(1 for x in fake if x < 0) / len(fake)\n    right = 1.0 - left\n    return {\n        'real_mean': statistics.mean(real),\n        'real_std': statistics.pstdev(real),\n        'fake_mean': statistics.mean(fake),\n        'fake_std': statistics.pstdev(fake),\n        'w1': empirical_w1_1d(real, fake),\n        'left': left,\n        'right': right,\n    }\ndef train_gan(theta_init, phi_init, generator_fn, steps=320, batch_size=128, lr_g=0.04, lr_d=0.04, d_updates=2, log_every=40):\n    theta = theta_init[:]\n    phi = phi_init[:]\n    history = []\n    for step in range(steps + 1):\n        for _ in range(d_updates):\n            x_real = sample_real(batch_size)\n            z = sample_z(batch_size)\n            def d_objective(phi_try):\n                ld, _, _, _ = losses_on_batch(theta, phi_try, x_real, z, generator_fn)\n                return ld\n            g_phi = finite_diff_grad(d_objective, phi)\n            # L_D を最大化するので、勾配上昇（+）で更新\n            phi = [p + lr_d * gp for p, gp in zip(phi, g_phi)]\n        x_real = sample_real(batch_size)\n        z = sample_z(batch_size)\n        def g_objective(theta_try):\n            _, lg, _, _ = losses_on_batch(theta_try, phi, x_real, z, generator_fn)\n            return lg\n        g_theta = finite_diff_grad(g_objective, theta)\n        # L_G（non-saturating）を最大化するので、勾配上昇（+）で更新\n        theta = [t + lr_g * gt for t, gt in zip(theta, g_theta)]\n        if step % log_every == 0:\n            x_eval = sample_real(1000)\n            z_eval = sample_z(1000)\n            ld_eval, lg_eval, _, _ = losses_on_batch(theta, phi, x_eval, z_eval, generator_fn)\n            stats = evaluate_distribution(theta, generator_fn, n=2000)\n            history.append((step, theta[:], phi[:], ld_eval, lg_eval, stats['w1'], stats['left'], stats['right']))\n    return theta, phi, history\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(21)\nlin_theta, lin_phi, lin_history = train_gan(theta0, phi0, generator_linear)\n\nfor step, th, ph, ld, lg, w1, left, right in lin_history:\n    print(\n        f'step={step:03d}',\n        f'theta={[round(v,4) for v in th]}',\n        f'phi={[round(v,4) for v in ph]}',\n        f'L_D={round(ld,4)}',\n        f'L_G={round(lg,4)}',\n        f'W1={round(w1,4)}',\n        f'left={round(left,3)}',\n        f'right={round(right,3)}'\n    )\n\nlin_stats = evaluate_distribution(lin_theta, generator_linear)\nprint()\nprint('linear generator final stats:')\nfor k, v in lin_stats.items():\n    print(k, '=', round(v, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここで重要なのは、学習アルゴリズム以前に表現力の限界があることです。`z~N(0,1)` に対して線形写像 `a z + b` を使うと、`p_g` は必ず単峰ガウスになります。したがって、2峰の実分布を正確には表現できません。\n\nつまり「学習が遅い」だけでなく、「モデルがその分布族を持っていない」ことが失敗の根本原因です。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2分岐生成器: z<0 と z>=0 で別の線形写像を使う\n# G(z) = a_l z + b_l  (z<0),  a_r z + b_r (z>=0)\ndef generator_piecewise(z: float, theta):\n    a_l, b_l, a_r, b_r = theta\n    if z < 0.0:\n        return a_l * z + b_l\n    return a_r * z + b_r\n\n\nrandom.seed(21)\n# 2峰を作りやすい初期値を置く（実務ではここも設計対象）\ntheta_pw0 = [0.35, -1.8, 0.35, 1.8]\nphi_pw0 = [0.25, 0.0]\n\npw_theta, pw_phi, pw_history = train_gan(\n    theta_pw0,\n    phi_pw0,\n    generator_piecewise,\n    steps=400,\n    lr_g=0.02,\n    lr_d=0.03,\n    d_updates=2,\n)\n\nfor step, th, ph, ld, lg, w1, left, right in pw_history:\n    print(\n        f'step={step:03d}',\n        f'theta={[round(v,4) for v in th]}',\n        f'phi={[round(v,4) for v in ph]}',\n        f'L_D={round(ld,4)}',\n        f'L_G={round(lg,4)}',\n        f'W1={round(w1,4)}',\n        f'left={round(left,3)}',\n        f'right={round(right,3)}'\n    )\n\npw_stats = evaluate_distribution(pw_theta, generator_piecewise)\nprint()\nprint('piecewise generator final stats:')\nfor k, v in pw_stats.items():\n    print(k, '=', round(v, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('comparison (linear vs piecewise):')\nprint('W1            =', round(lin_stats['w1'], 4), 'vs', round(pw_stats['w1'], 4))\nprint('fake stdev    =', round(lin_stats['fake_std'], 4), 'vs', round(pw_stats['fake_std'], 4))\nprint('mode balance  =', round(min(lin_stats['left'], lin_stats['right']) / max(lin_stats['left'], lin_stats['right']), 4),\n      'vs',\n      round(min(pw_stats['left'], pw_stats['right']) / max(pw_stats['left'], pw_stats['right']), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "non-saturating 目的を使う理由も確認します。`s` を識別器ロジット（`D=σ(s)`）とすると、\n\n- minimax 目的 `log(1-σ(s))` の導関数は `-σ(s)`\n- non-saturating 目的 `-log(σ(s))` の導関数は `-(1-σ(s))`\n\nです。`D(G(z))` が小さい初期段階では、minimax の勾配は小さくなりやすく、non-saturating のほうが更新信号を確保しやすくなります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def d_minimax_ds(p):\n    return -p\n\n\ndef d_nonsat_ds(p):\n    return -(1.0 - p)\n\nfor p in [0.001, 0.01, 0.05, 0.1, 0.5, 0.9]:\n    print(\n        f'D(fake)={p:>5}',\n        f'|d(minimax)/ds|={abs(d_minimax_ds(p)):.4f}',\n        f'|d(non-sat)/ds|={abs(d_nonsat_ds(p)):.4f}'\n    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GAN派生の代表例として LSGAN と WGAN の方向性を整理します。LSGAN は二乗誤差で勾配をなめらかにし、WGAN は Wasserstein 距離に基づく評価で学習安定化を狙います。どちらも「単に新しい損失」ではなく、失敗モードに対応した設計です。\n\n次のセルでは次を観察してください。\n- LSGAN の `loss_D` と `loss_G` は最小化対象で、値の振れが大きすぎないかを見ます。\n- WGAN の重みクリップは、識別器（critic）の過剰な鋭さを抑えるための直感的な操作です。クリップ前後の値の変化を見て、制約がどう効くかを確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def lsgan_losses(theta, phi, x_real, z_batch, generator_fn):\n    x_fake = [generator_fn(z, theta) for z in z_batch]\n    d_real = [discriminator(x, phi) for x in x_real]\n    d_fake = [discriminator(x, phi) for x in x_fake]\n\n    # 典型的な最小化形式\n    loss_d = 0.5 * (\n        sum((p - 1.0) ** 2 for p in d_real) / len(d_real)\n        + sum((p - 0.0) ** 2 for p in d_fake) / len(d_fake)\n    )\n    loss_g = 0.5 * sum((p - 1.0) ** 2 for p in d_fake) / len(d_fake)\n    return loss_d, loss_g\n\n\nx_ref = sample_real(256)\nz_ref = sample_z(256)\nld_gan, lg_gan, _, _ = losses_on_batch(pw_theta, pw_phi, x_ref, z_ref, generator_piecewise)\nld_ls, lg_ls = lsgan_losses(pw_theta, pw_phi, x_ref, z_ref, generator_piecewise)\n\nprint('GAN objective (maximize L_D, L_G):', round(ld_gan, 5), round(lg_gan, 5))\nprint('LSGAN loss (minimize loss_D, loss_G):', round(ld_ls, 5), round(lg_ls, 5))\n\ncritic_weight = 1.7\nclip_value = 0.1\ncritic_weight_clipped = max(-clip_value, min(clip_value, critic_weight))\nprint('WGAN intuition: raw critic weight =', critic_weight, '-> clipped =', critic_weight_clipped)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GANで精度を上げるときは、損失関数だけでなく、生成器の分布表現力・識別器の強さ・学習率バランスを同時に設計する必要があります。このノートで見た通り、同じGANでも「分布をそもそも表現できるかどうか」で結果は大きく変わります。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
