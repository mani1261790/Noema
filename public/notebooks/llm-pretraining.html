<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>llm-pretraining</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<pre><code class="language-python">import math
import random
import re
from collections import Counter, defaultdict

import numpy as np
import matplotlib.pyplot as plt

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ModuleNotFoundError:
    torch = None
    nn = None
    optim = None
    TORCH_AVAILABLE = False</code></pre>
<h1 id="事前学習-pre-training">事前学習（Pre-training）</h1>
<p>事前学習は、大量の未ラベルテキストから「次にどのトークンが来るか」を予測することで、言語の統計構造を学ぶ段階です。<br>
このノートでは、データ準備、N-gramからニューラル言語モデルへの流れ、評価（perplexity）、計算コスト感覚、そして機械論的解釈可能性の入口までを一貫して確認します。</p>

<p>言語モデルの基本目的は次の確率を最大化することです。</p>
<p><code>P(w_1, ..., w_T) = Π_t P(w_t | w_{&lt;t})</code></p>
<p>ここで <code>w_{&lt;t}</code> は <code>w_1 ... w_{t-1}</code>（時刻 <code>t</code> より前のトークン列）です。<br>
実装ではクロスエントロピー損失を最小化します。<br>
評価では perplexity（困惑度）を使い、<code>perplexity = exp(平均クロスエントロピー)</code>、低いほど予測が当たりやすいことを意味します。</p>

<pre><code class="language-python"># 小さな日本語コーパス（教育用）
raw_docs = [
    &#39;事前学習では大量のテキストから次トークン予測を学ぶ。&#39;,
    &#39;言語モデルは文脈に応じた分布を出力する。&#39;,
    &#39;トークン化の設計は学習効率と性能に強く影響する。&#39;,
    &#39;前処理では重複除去や品質フィルタリングが重要になる。&#39;,
    &#39;評価ではperplexityや下流タスク性能を併用する。&#39;,
    &#39;モデル規模とデータ規模のバランスが学習の成否を左右する。&#39;,
    &#39;機械論的解釈可能性は内部回路の理解に役立つ。&#39;,
    &#39;SFTは事前学習済みモデルを指示追従に調整する工程である。&#39;,
    &#39;推論時は温度やサンプリング戦略が出力を変える。&#39;,
    &#39;長文文脈では注意機構の設計が効いてくる。&#39;,
]


def normalize(s):
    s = s.lower().strip()
    s = re.sub(r&#39;[。､，,.!?！？]&#39;, &#39; &#39;, s)
    s = re.sub(r&#39;\s+&#39;, &#39; &#39;, s)
    return s


def char_tokenize(s):
    s = normalize(s).replace(&#39; &#39;, &#39;&#39;)
    return list(s)


docs = [char_tokenize(d) for d in raw_docs]
random.seed(0)
random.shuffle(docs)
split = int(len(docs) * 0.8)
train_docs = docs[:split]
val_docs = docs[split:]

print(&#39;train docs:&#39;, len(train_docs), &#39;val docs:&#39;, len(val_docs))
print(&#39;sample train tokens:&#39;, &#39;&#39;.join(train_docs[0][:30]))</code></pre>
<pre><code class="language-python">train_lengths = np.array([len(d) for d in train_docs], dtype=np.int64)
val_lengths = np.array([len(d) for d in val_docs], dtype=np.int64)

vocab = sorted(set(ch for doc in train_docs for ch in doc))
vocab = [&#39;&lt;unk&gt;&#39;] + vocab
stoi = {ch: i for i, ch in enumerate(vocab)}
itos = {i: ch for ch, i in stoi.items()}
unk_id = stoi[&#39;&lt;unk&gt;&#39;]

print(&#39;vocab size:&#39;, len(vocab))
print(&#39;avg train length:&#39;, float(train_lengths.mean()))
print(&#39;avg val length  :&#39;, float(val_lengths.mean()))

plt.figure(figsize=(6.8, 3.4))
plt.hist(train_lengths, alpha=0.7, label=&#39;train&#39;, color=&#39;#7aa2ff&#39;)
plt.hist(val_lengths, alpha=0.7, label=&#39;val&#39;, color=&#39;#8dd3a7&#39;)
plt.xlabel(&#39;sequence length (characters)&#39;)
plt.ylabel(&#39;count&#39;)
plt.title(&#39;Length distribution after tokenization&#39;)
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>まずはN-gram言語モデルを作り、事前学習の最小ベースラインを確認します。</p>
<ul>
<li>Uni-gram: 直前文脈を使わず出現頻度だけで予測</li>
<li>Bi-gram: 1つ前のトークンを条件に次トークンを予測</li>
</ul>
<p>ここでは加法スムージング（add-k）を入れて、未知遷移でも確率0を避けます。<br>
また簡略化のためBOS/EOSは入れず、生成時の開始文字を固定しています（実運用ではBOS開始・EOS停止が一般的）。</p>

<pre><code class="language-python">def flatten(docs):
    out = []
    for d in docs:
        out.extend(d)
    return out


def to_ids(doc):
    return [stoi.get(ch, unk_id) for ch in doc]


train_ids = [to_ids(d) for d in train_docs]
val_ids = [to_ids(d) for d in val_docs]

unigram_counts = Counter(flatten(train_ids))
bigram_counts = defaultdict(Counter)
for seq in train_ids:
    for a, b in zip(seq[:-1], seq[1:]):
        bigram_counts[a][b] += 1

V = len(vocab)


def unigram_prob(tok, k=0.1):
    total = sum(unigram_counts.values())
    return (unigram_counts[tok] + k) / (total + k * V)


def bigram_prob(prev, tok, k=0.1):
    row = bigram_counts[prev]
    total = sum(row.values())
    return (row[tok] + k) / (total + k * V)


def perplexity_unigram(seqs):
    nll = 0.0
    n = 0
    for seq in seqs:
        for t in seq:
            p = unigram_prob(t)
            nll += -math.log(p + 1e-12)
            n += 1
    return math.exp(nll / max(n, 1))


def perplexity_bigram(seqs):
    nll = 0.0
    n = 0
    for seq in seqs:
        for prev, t in zip(seq[:-1], seq[1:]):
            p = bigram_prob(prev, t)
            nll += -math.log(p + 1e-12)
            n += 1
    return math.exp(nll / max(n, 1))


print(&#39;val perplexity unigram:&#39;, round(perplexity_unigram(val_ids), 4))
print(&#39;val perplexity bigram :&#39;, round(perplexity_bigram(val_ids), 4))</code></pre>
<pre><code class="language-python">def sample_unigram(max_len=40):
    probs = np.array([unigram_prob(i) for i in range(V)], dtype=np.float64)
    probs = probs / probs.sum()
    ids = np.random.choice(np.arange(V), size=max_len, p=probs)
    return &#39;&#39;.join(itos[i] for i in ids if i != unk_id)


def sample_bigram(start_id, max_len=40):
    out = [start_id]
    cur = start_id
    for _ in range(max_len - 1):
        probs = np.array([bigram_prob(cur, j) for j in range(V)], dtype=np.float64)
        probs = probs / probs.sum()
        nxt = int(np.random.choice(np.arange(V), p=probs))
        out.append(nxt)
        cur = nxt
    return &#39;&#39;.join(itos[i] for i in out if i != unk_id)


np.random.seed(1)
start_char = train_ids[0][0]
print(&#39;unigram sample:&#39;, sample_unigram(36))
print(&#39;bigram sample :&#39;, sample_bigram(start_char, 36))</code></pre>
<p>次に、クロスエントロピー損失を手計算し、ニューラル言語モデル学習へ接続します。</p>

<pre><code class="language-python">logits = np.array([0.2, -0.3, 1.1, 0.0], dtype=np.float64)
target = 2

shift = logits - np.max(logits)
probs = np.exp(shift) / np.sum(np.exp(shift))
ce = -math.log(probs[target] + 1e-12)

print(&#39;probs =&#39;, np.round(probs, 4))
print(&#39;cross entropy =&#39;, round(float(ce), 6))
print(&#39;perplexity for this token =&#39;, round(float(math.exp(ce)), 6))</code></pre>
<pre><code class="language-python">if TORCH_AVAILABLE:
    torch.manual_seed(0)

    # 連結テキストを作成
    train_text = &#39;&#39;.join(&#39;&#39;.join(d) + &#39;\n&#39; for d in train_docs)
    val_text = &#39;&#39;.join(&#39;&#39;.join(d) + &#39;\n&#39; for d in val_docs)

    train_data = torch.tensor([stoi.get(ch, unk_id) for ch in train_text], dtype=torch.long)
    val_data = torch.tensor([stoi.get(ch, unk_id) for ch in val_text], dtype=torch.long)

    max_block = min(48, len(train_data) - 1, len(val_data) - 1)
    if max_block &lt; 2:
        raise ValueError(&#39;Dataset too short for neural LM demo after split&#39;)

    block = max_block
    batch_size = 32

    def get_batch(data, block_size, bsz):
        high = len(data) - block_size - 1
        idx = torch.randint(0, high + 1, (bsz,))
        x = torch.stack([data[i:i+block_size] for i in idx])
        y = torch.stack([data[i+1:i+block_size+1] for i in idx])
        return x, y

    class TinyPretrainLM(nn.Module):
        def __init__(self, vocab_size, d_model=64):
            super().__init__()
            self.emb = nn.Embedding(vocab_size, d_model)
            self.gru = nn.GRU(d_model, d_model, batch_first=True)
            self.head = nn.Linear(d_model, vocab_size)

        def forward(self, x):
            h = self.emb(x)
            out, _ = self.gru(h)
            return self.head(out)

    model = TinyPretrainLM(len(vocab), d_model=64)
    opt = optim.AdamW(model.parameters(), lr=3e-3)
    criterion_mean = nn.CrossEntropyLoss()
    criterion_sum = nn.CrossEntropyLoss(reduction=&#39;sum&#39;)

    for step in range(260):
        xb, yb = get_batch(train_data, block, batch_size)
        logits = model(xb)
        loss = criterion_mean(logits.reshape(-1, len(vocab)), yb.reshape(-1))

        opt.zero_grad()
        loss.backward()
        opt.step()

        if step % 65 == 0:
            print(f&#39;step={step:&gt;3d}, train loss={loss.item():.4f}&#39;)

    # バッチ1個ではなく、検証系列全体で近似perplexityを計算
    with torch.no_grad():
        total_nll = 0.0
        total_tok = 0
        starts = list(range(0, len(val_data) - 1, block))
        for s in starts:
            b = min(block, len(val_data) - 1 - s)
            if b &lt;= 0:
                continue
            x = val_data[s:s+b].unsqueeze(0)
            y = val_data[s+1:s+b+1].unsqueeze(0)
            logits = model(x)
            nll = criterion_sum(logits.reshape(-1, len(vocab)), y.reshape(-1)).item()
            total_nll += nll
            total_tok += y.numel()

        val_loss = total_nll / max(total_tok, 1)
        val_ppl = math.exp(val_loss)

    print(&#39;val avg nll =&#39;, round(val_loss, 4), &#39;val perplexity =&#39;, round(val_ppl, 4))

    # 生成
    prompt = &#39;事前学習&#39;
    ids = [stoi.get(ch, unk_id) for ch in prompt]
    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)
    for _ in range(40):
        logits = model(x)
        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
        x = torch.cat([x, next_id], dim=1)
    gen = &#39;&#39;.join(itos[i] if i != unk_id else &#39;□&#39; for i in x.squeeze(0).tolist())
    print(&#39;generated:&#39;, gen)
else:
    print(&#39;PyTorch未導入のため、ニューラルLMセルをスキップしました。&#39;)</code></pre>
<p>事前学習ではデータ品質が非常に重要です。<br>
重複が多いと実効データ量が減り、汚染データがあると望ましくない振る舞いを学習します。<br>
下では文字3-gramのJaccard類似度で近重複を簡易検出します。</p>

<pre><code class="language-python">def shingles(text, k=3):
    if len(text) &lt; k:
        return {text}
    return {text[i:i+k] for i in range(len(text) - k + 1)}


def jaccard(a, b):
    inter = len(a &amp; b)
    union = len(a | b)
    return inter / max(union, 1)


dedup_candidates = [
    &#39;llmの事前学習では大規模テキストを使う&#39;,
    &#39;llmの事前学習では大規模なテキストを使う&#39;,
    &#39;画像分類ではラベル付きデータで学習する&#39;,
]

S = [shingles(s, k=3) for s in dedup_candidates]
for i in range(len(S)):
    for j in range(i + 1, len(S)):
        sim = jaccard(S[i], S[j])
        print(f&#39;sim({i},{j}) = {sim:.4f}&#39;)</code></pre>
<p>スケーリング則の細部は設定依存ですが、実務では「おおまかな計算量感覚」を先に持つのが重要です。<br>
ここではデコーダ型Transformer学習の粗い目安として、</p>
<p><code>FLOPs ≈ 6 * N_params * N_tokens</code></p>
<p>を使って見積もります（係数はモデル形状・実装・ハードウェアで変動します）。</p>

<pre><code class="language-python">def estimate_flops(params_billion, tokens_billion):
    n_params = params_billion * 1e9
    n_tokens = tokens_billion * 1e9
    return 6.0 * n_params * n_tokens


settings = [
    (0.1, 20),   # 0.1B params, 20B tokens
    (0.7, 100),  # 0.7B params, 100B tokens
    (7.0, 300),  # 7B params, 300B tokens
]

for p_b, t_b in settings:
    flops = estimate_flops(p_b, t_b)
    print(f&#39;params={p_b:&gt;4.1f}B, tokens={t_b:&gt;4.0f}B -&gt; FLOPs≈{flops:.3e}&#39;)

# トークン課金の例（仮定値）
in_tok, out_tok = 1200, 400
price_per_m_in = 0.20
price_per_m_out = 0.80
cost = (in_tok / 1e6) * price_per_m_in + (out_tok / 1e6) * price_per_m_out
print(&#39;\nexample inference cost per request (assumed pricing) =&#39;, round(cost, 6), &#39;USD&#39;)</code></pre>
<p>事前学習後には、継続事前学習（domain adaptive pretraining）やSFTへ進むことが多いです。<br>
混合データでの学習では、一般コーパスとドメインコーパスの損失バランスを監視し、過学習や忘却を防ぎます。</p>

<pre><code class="language-python"># 2種類のデータ損失を重み付きで合成する簡易例
loss_general = np.array([2.20, 2.05, 1.98, 1.95, 1.93])
loss_domain = np.array([2.80, 2.30, 2.00, 1.82, 1.74])
alpha = 0.6  # general側の重み

mixed = alpha * loss_general + (1 - alpha) * loss_domain
for i, (g, d, m) in enumerate(zip(loss_general, loss_domain, mixed), 1):
    print(f&#39;epoch {i}: general={g:.3f}, domain={d:.3f}, mixed={m:.3f}&#39;)

plt.figure(figsize=(6.8, 3.5))
plt.plot(loss_general, marker=&#39;o&#39;, label=&#39;general loss&#39;)
plt.plot(loss_domain, marker=&#39;s&#39;, label=&#39;domain loss&#39;)
plt.plot(mixed, marker=&#39;^&#39;, label=&#39;weighted objective&#39;)
plt.xlabel(&#39;epoch&#39;)
plt.ylabel(&#39;loss&#39;)
plt.title(&#39;Balancing continued pretraining objectives&#39;)
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>機械論的解釈可能性（mechanistic interpretability）は、ニューラルモデル内部の回路を理解する試みです。<br>
下の可視化は厳密な回路解析そのものではなく、<br>
「何が次に出やすいか」を統計遷移として読むための入口デモです。</p>
<p>本格的には、注意ヘッドやMLPニューロン活性を直接解析して因果的に検証します。</p>

<pre><code class="language-python"># Bi-gram遷移行列を可視化（解釈可能性の最小例）
show_chars = [ch for ch in [&#39;事&#39;, &#39;前&#39;, &#39;学&#39;, &#39;習&#39;, &#39;モ&#39;, &#39;デ&#39;, &#39;ル&#39;, &#39;。&#39;] if ch in stoi]
show_ids = [stoi[ch] for ch in show_chars]

M = np.zeros((len(show_ids), len(show_ids)), dtype=np.float64)
for i, src in enumerate(show_ids):
    for j, dst in enumerate(show_ids):
        M[i, j] = bigram_prob(src, dst)

row_sums = M.sum(axis=1, keepdims=True)
M_norm = M / np.maximum(row_sums, 1e-12)

plt.figure(figsize=(6.0, 4.6))
plt.imshow(M_norm, cmap=&#39;magma&#39;)
plt.colorbar(label=&#39;P(next | current)&#39;)
plt.xticks(range(len(show_chars)), show_chars)
plt.yticks(range(len(show_chars)), show_chars)
plt.xlabel(&#39;next token&#39;)
plt.ylabel(&#39;current token&#39;)
plt.title(&#39;Interpretable transition map (toy bigram)&#39;)
plt.tight_layout()
plt.show()

for i, src in enumerate(show_chars):
    top = np.argsort(M_norm[i])[::-1][:3]
    cand = [(show_chars[t], float(M_norm[i, t])) for t in top]
    print(src, &#39;-&gt;&#39;, [(c, round(p, 4)) for c, p in cand])</code></pre>
<p>事前学習では「データ」「目的関数」「計算資源」の3つを同時に設計する必要があります。<br>
最初に小さな実験で挙動を掴み、次に本番規模へスケールする手順を徹底すると、失敗コストを下げやすくなります。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>