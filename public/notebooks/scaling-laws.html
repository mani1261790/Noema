<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>scaling-laws</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<pre><code class="language-python">import math
import numpy as np
import matplotlib.pyplot as plt</code></pre>
<h1 id="スケーリング則">スケーリング則</h1>
<p>LLMの事前学習では、モデルサイズ <code>N</code>、学習トークン数 <code>D</code>、計算資源 <code>C</code> を大きくすると、検証損失がべき乗的に下がる傾向があります。<br>
このノートでは、実験データからべき乗則を推定し、同一計算資源（isoflops）での最適配分を計算する流れを確認します。</p>

<p>よく使う近似は次の形です。</p>
<p><code>L(N, D) ≈ L_inf + a * N^{-alpha} + b * D^{-beta}</code></p>
<ul>
<li><code>L_inf</code>: これ以上は下げにくい下限（不可約損失）</li>
<li><code>alpha</code>, <code>beta</code>: モデル拡大・データ拡大の効き方</li>
</ul>
<p>まずは <code>N</code> と <code>D</code> をそれぞれ変えた観測データを作り、<code>alpha</code>, <code>beta</code> を推定します。</p>

<pre><code class="language-python"># 観測例（教育用の合成データ）
N_million = np.array([30, 60, 120, 240, 480, 960], dtype=np.float64)   # million params
D_billion = np.array([5, 10, 20, 40, 80, 160], dtype=np.float64)        # billion tokens

# 実際の単位へ変換
N_params = N_million * 1e6
D_tokens = D_billion * 1e9

# 生成側の真値（未知だと思って推定する）
L_inf_true = 1.60
A_true, alpha_true = 3.2, 0.37
B_true, beta_true = 2.1, 0.29

# 損失生成では見やすさのため M/B 単位でべき乗を作る
rng = np.random.default_rng(7)
L_of_N = L_inf_true + A_true * (N_million ** (-alpha_true)) + rng.normal(0, 0.01, size=N_million.shape)
L_of_D = L_inf_true + B_true * (D_billion ** (-beta_true)) + rng.normal(0, 0.01, size=D_billion.shape)

print(&#39;N sweep losses:&#39;, np.round(L_of_N, 4))
print(&#39;D sweep losses:&#39;, np.round(L_of_D, 4))
print(&#39;unit note: N uses params count, D uses token count in isoflops section&#39;)</code></pre>
<pre><code class="language-python">plt.figure(figsize=(7.2, 3.6))
plt.subplot(1, 2, 1)
plt.plot(N_million, L_of_N, marker=&#39;o&#39;)
plt.xscale(&#39;log&#39;)
plt.xlabel(&#39;N (million params, log)&#39;)
plt.ylabel(&#39;validation loss&#39;)
plt.title(&#39;Loss vs Model Size&#39;)

plt.subplot(1, 2, 2)
plt.plot(D_billion, L_of_D, marker=&#39;o&#39;, color=&#39;#d77f00&#39;)
plt.xscale(&#39;log&#39;)
plt.xlabel(&#39;D (billion tokens, log)&#39;)
plt.ylabel(&#39;validation loss&#39;)
plt.title(&#39;Loss vs Data Size&#39;)
plt.tight_layout()
plt.show()</code></pre>
<p><code>L_inf</code> が未知なので、候補値を走査しながら<br>
<code>log(L - L_inf)</code> と <code>log(x)</code> の一次回帰で指数を推定します。</p>
<p><code>L_inf</code> は観測損失の最小値より少し小さい値にしかなりえないので、<br>
その近傍を候補として探索します。</p>

<pre><code class="language-python">def fit_power_with_fixed_floor(x, y, floor):
    # y ~= floor + A * x^{-exponent}
    z = y - floor
    if np.any(z &lt;= 0):
        return None

    lx = np.log(x)
    lz = np.log(z)
    # lz = c + m*lx, where m=-exponent
    m, c = np.polyfit(lx, lz, 1)
    pred = floor + np.exp(c) * (x ** m)
    mse = float(np.mean((pred - y) ** 2))
    return {
        &#39;floor&#39;: float(floor),
        &#39;A&#39;: float(np.exp(c)),
        &#39;exponent&#39;: float(-m),
        &#39;mse&#39;: mse,
        &#39;pred&#39;: pred,
    }


min_obs = float(min(np.min(L_of_N), np.min(L_of_D)))
floor_candidates = np.linspace(min_obs - 0.25, min_obs - 1e-4, 400)

best = None
for floor in floor_candidates:
    fN = fit_power_with_fixed_floor(N_million, L_of_N, floor)
    fD = fit_power_with_fixed_floor(D_billion, L_of_D, floor)
    if fN is None or fD is None:
        continue
    total_mse = fN[&#39;mse&#39;] + fD[&#39;mse&#39;]
    if best is None or total_mse &lt; best[&#39;total_mse&#39;]:
        best = {
            &#39;L_inf&#39;: float(floor),
            &#39;N_fit&#39;: fN,
            &#39;D_fit&#39;: fD,
            &#39;total_mse&#39;: float(total_mse),
        }

fit_joint = best
print(&#39;Shared-floor fit summary:&#39;)
print(&#39;L_inf =&#39;, round(fit_joint[&#39;L_inf&#39;], 5), &#39;total_mse =&#39;, round(fit_joint[&#39;total_mse&#39;], 7))
print(&#39;N_fit =&#39;, {k: round(v, 5) for k, v in fit_joint[&#39;N_fit&#39;].items() if k != &#39;pred&#39;})
print(&#39;D_fit =&#39;, {k: round(v, 5) for k, v in fit_joint[&#39;D_fit&#39;].items() if k != &#39;pred&#39;})</code></pre>
<pre><code class="language-python">plt.figure(figsize=(7.2, 3.5))
plt.subplot(1, 2, 1)
plt.scatter(N_million, L_of_N, label=&#39;observed&#39;)
plt.plot(N_million, fit_joint[&#39;N_fit&#39;][&#39;pred&#39;], label=&#39;power fit&#39;, color=&#39;#cc3344&#39;)
plt.xscale(&#39;log&#39;)
plt.xlabel(&#39;N (M params)&#39;)
plt.ylabel(&#39;loss&#39;)
plt.title(f&quot;alpha≈{fit_joint[&#39;N_fit&#39;][&#39;exponent&#39;]:.3f}&quot;)
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(D_billion, L_of_D, label=&#39;observed&#39;)
plt.plot(D_billion, fit_joint[&#39;D_fit&#39;][&#39;pred&#39;], label=&#39;power fit&#39;, color=&#39;#cc3344&#39;)
plt.xscale(&#39;log&#39;)
plt.xlabel(&#39;D (B tokens)&#39;)
plt.ylabel(&#39;loss&#39;)
plt.title(f&quot;beta≈{fit_joint[&#39;D_fit&#39;][&#39;exponent&#39;]:.3f}&quot;)
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>次に isoflops を考えます。<br>
デコーダ型学習の粗い近似として <code>C ≈ 6ND</code>（<code>N</code>: パラメータ数, <code>D</code>: 学習トークン数）を使います。</p>
<p><code>D = C/(6N)</code> を <code>L(N, D)</code> に代入すると<br>
<code>f(N) = aN^{-alpha} + b(C/6)^{-beta}N^{beta}</code> になり、<br>
これを <code>df/dN = 0</code> で解くと <code>N*</code> と <code>D*</code> が得られます。</p>

<pre><code class="language-python"># 共有L_infで推定した係数を使って、L(N,D)=L_inf+aN^-alpha+bD^-beta を最適化
L_inf = fit_joint[&#39;L_inf&#39;]
a_fit, alpha = fit_joint[&#39;N_fit&#39;][&#39;A&#39;], fit_joint[&#39;N_fit&#39;][&#39;exponent&#39;]
b_fit, beta = fit_joint[&#39;D_fit&#39;][&#39;A&#39;], fit_joint[&#39;D_fit&#39;][&#39;exponent&#39;]

# a_fit,b_fit は N(M params), D(B tokens) の単位で推定されているので
# isoflops (N: params, D: tokens) へ合わせて係数を変換する
# (N/1e6)^-alpha = (1e6^alpha) * N^-alpha
# (D/1e9)^-beta  = (1e9^beta)  * D^-beta
a_raw = a_fit * (1e6 ** alpha)
b_raw = b_fit * (1e9 ** beta)


def optimal_N_D_for_compute(C, a, alpha, b, beta):
    # C = 6ND -&gt; D = C/(6N)
    # minimize f(N)=aN^-alpha + b(C/6)^-beta N^beta
    numer = a * alpha
    denom = b * beta
    N_star = (numer / denom) ** (1.0 / (alpha + beta)) * (C / 6.0) ** (beta / (alpha + beta))
    D_star = C / (6.0 * N_star)
    return N_star, D_star


# C の単位は FLOPs 相当の抽象値（ここでは比較目的）
C_values = np.logspace(18, 22, 9)
N_star = []
D_star = []
for C in C_values:
    n, d = optimal_N_D_for_compute(C, a_raw, alpha, b_raw, beta)
    N_star.append(n)
    D_star.append(d)
N_star = np.array(N_star)
D_star = np.array(D_star)

print(&#39;first 4 optimal pairs (N*, D*):&#39;)
for i in range(4):
    print(f&quot;C={C_values[i]:.1e} -&gt; N*={N_star[i]/1e6:.2f}M params, D*={D_star[i]/1e9:.2f}B tokens&quot;)</code></pre>
<pre><code class="language-python">plt.figure(figsize=(7.2, 3.6))
plt.plot(C_values, N_star / 1e6, marker=&#39;o&#39;, label=&#39;optimal N* (M params)&#39;)
plt.plot(C_values, D_star / 1e9, marker=&#39;s&#39;, label=&#39;optimal D* (B tokens)&#39;)
plt.xscale(&#39;log&#39;)
plt.yscale(&#39;log&#39;)
plt.xlabel(&#39;Compute C (log)&#39;)
plt.ylabel(&#39;Optimal scale (log)&#39;)
plt.title(&#39;Isoflops-optimal allocation&#39;)
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>実務でありがちな失敗は、計算資源が同じなのに</p>
<ul>
<li>モデルだけ大きくしてデータ不足（undertrained）</li>
<li>データだけ増やしてモデル不足（underparameterized）</li>
</ul>
<p>になることです。下で同一 <code>C</code> に対する損失差を比較します。</p>

<pre><code class="language-python">def approx_loss(N, D, L_inf, a, alpha, b, beta):
    return L_inf + a * (N ** (-alpha)) + b * (D ** (-beta))


ratios = [0.5, 1.0, 2.0]  # Nを最適比の何倍にするか
example_C = 1e20
n_opt, d_opt = optimal_N_D_for_compute(example_C, a_raw, alpha, b_raw, beta)

print(f&#39;compute C={example_C:.1e}&#39;)
print(f&#39;optimal N={n_opt/1e6:.2f}M params, D={d_opt/1e9:.2f}B tokens&#39;)

for r in ratios:
    n = n_opt * r
    d = example_C / (6.0 * n)
    L = approx_loss(n, d, L_inf, a_raw, alpha, b_raw, beta)
    label = &#39;optimal&#39; if abs(r - 1.0) &lt; 1e-9 else f&#39;N x {r}&#39;
    print(f&#39;{label:8s}: N={n/1e6:8.2f}M, D={d/1e9:8.2f}B, approx loss={L:.5f}&#39;)</code></pre>
<p>スケーリング則は万能ではありません。<br>
データ品質、ドメインミスマッチ、最適化設定、アーキテクチャ変更で指数や下限は変わります。<br>
それでも、実験計画の初期段階で「どこに計算資源を使うか」を決める強力な指針になります。</p>

<pre><code class="language-python"># 価格の粗い見積もり（仮定値）
train_flops = 3.0e22
hardware_tflops = 250.0   # 1 GPUあたり
num_gpus = 64
utilization = 0.35

seconds = train_flops / (hardware_tflops * 1e12 * num_gpus * utilization)
hours = seconds / 3600

print(&#39;Estimated wall-clock hours:&#39;, round(hours, 2))

usd_per_gpu_hour = 1.8
cost = hours * num_gpus * usd_per_gpu_hour
print(&#39;Estimated training cost (USD, rough):&#39;, round(cost, 2))</code></pre>
<p>このノートで押さえたい実務ポイント:</p>
<ol>
<li>まず小規模スイープで <code>alpha, beta, L_inf</code> を推定する</li>
<li>その推定に基づき isoflops で <code>N</code> と <code>D</code> を配分する</li>
<li>本番では品質劣化要因（データ品質・最適化不安定）を別監視する</li>
</ol>
<p>この3段階を回すと、計算予算の無駄打ちを減らしやすくなります。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>