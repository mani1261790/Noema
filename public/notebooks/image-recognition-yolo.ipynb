{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 画像認識（YOLOを含む）\n\n画像認識には大きく3種類あります。\n\n- 画像分類: 画像全体に1つのラベルを付ける\n- 物体検出: 物体の位置（バウンディングボックス）とクラスを出す\n- セグメンテーション: 画素ごとにクラスを出す\n\nこのノートでは、物体検出を中心に、YOLOで必要になる計算を順に確認します。\n最後に、CNNバックボーンとViTバックボーンの関係も整理します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    TORCH_AVAILABLE = True\nexcept ModuleNotFoundError:\n    torch = None\n    nn = None\n    optim = None\n    TORCH_AVAILABLE = False\n\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まずは、バウンディングボックスの表現を統一します。\n\nこのノートでは画像座標を使います。原点は左上 `(0, 0)`、`x` は右向き、`y` は下向きに増えます。\n`xyxy` は `(x1, y1, x2, y2)`、`xywh` は `(cx, cy, w, h)` です。\n\nここでは `xyxy`（左上x, 左上y, 右下x, 右下y）と `xywh`（中心x, 中心y, 幅, 高さ）を相互変換する関数を用意します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def xywh_to_xyxy(box_xywh):\n    cx, cy, w, h = box_xywh\n    x1 = cx - w / 2\n    y1 = cy - h / 2\n    x2 = cx + w / 2\n    y2 = cy + h / 2\n    return np.array([x1, y1, x2, y2], dtype=np.float64)\n\n\ndef xyxy_to_xywh(box_xyxy):\n    x1, y1, x2, y2 = box_xyxy\n    cx = (x1 + x2) / 2\n    cy = (y1 + y2) / 2\n    w = max(0.0, x2 - x1)\n    h = max(0.0, y2 - y1)\n    return np.array([cx, cy, w, h], dtype=np.float64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_xywh = np.array([40.0, 30.0, 28.0, 18.0])\nconverted = xywh_to_xyxy(sample_xywh)\nback = xyxy_to_xywh(converted)\n\nprint('xywh -> xyxy:', np.round(converted, 3))\nprint('xyxy -> xywh:', np.round(back, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IoU（Intersection over Union）は、予測ボックスと正解ボックスの重なり具合を測る指標です。\n\n`IoU = 重なり面積 / (予測面積 + 正解面積 - 重なり面積)` で定義します。\n`IoU=1` に近いほど一致、`IoU=0` に近いほど不一致です。\n物体検出では「当たったかどうか」の判定や AP/mAP 計算で中心的に使います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iou_xyxy(box_a, box_b):\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n\n    inter_w = max(0.0, inter_x2 - inter_x1)\n    inter_h = max(0.0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n\n    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n    union = area_a + area_b - inter_area\n\n    return 0.0 if union <= 0 else inter_area / union\n\n\ngt = np.array([20.0, 20.0, 60.0, 52.0])\np1 = np.array([18.0, 24.0, 62.0, 54.0])\np2 = np.array([45.0, 20.0, 80.0, 48.0])\n\nprint('IoU(pred1, gt):', round(iou_xyxy(p1, gt), 4))\nprint('IoU(pred2, gt):', round(iou_xyxy(p2, gt), 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "canvas = np.zeros((80, 100), dtype=np.float64)\n\nfig, ax = plt.subplots(figsize=(6.6, 4.0))\nax.imshow(canvas, cmap='gray', vmin=0, vmax=1)\n\n\ndef draw_box(ax, b, color, label):\n    x1, y1, x2, y2 = b\n    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=color, linewidth=2)\n    ax.add_patch(rect)\n    ax.text(x1, y1 - 2, label, color=color, fontsize=9)\n\n\ndraw_box(ax, gt, 'lime', 'gt')\ndraw_box(ax, p1, 'cyan', 'pred1')\ndraw_box(ax, p2, 'orange', 'pred2')\nax.set_title('Bounding Boxes and Overlap')\nax.axis('off')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "物体検出では近い位置に重複予測が出やすいため、NMS（Non-Maximum Suppression）で重複を間引きます。\nスコア順に見て、IoUが高いボックスを抑制するのが基本です。\n\n下は単一クラスを想定した最小実装です。マルチクラスではクラスごとにNMSを行います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nms(boxes, scores, iou_thresh=0.5):\n    boxes = np.asarray(boxes, dtype=np.float64)\n    scores = np.asarray(scores, dtype=np.float64)\n\n    order = np.argsort(scores)[::-1]\n    keep = []\n\n    while len(order) > 0:\n        i = order[0]\n        keep.append(i)\n\n        rest = order[1:]\n        remaining = []\n        for j in rest:\n            if iou_xyxy(boxes[i], boxes[j]) < iou_thresh:\n                remaining.append(j)\n        order = np.array(remaining, dtype=int)\n\n    return keep\n\n\nboxes = np.array([\n    [16, 18, 58, 50],\n    [18, 20, 60, 52],\n    [44, 18, 79, 47],\n    [10, 40, 36, 70],\n], dtype=np.float64)\n\nscores = np.array([0.86, 0.91, 0.63, 0.58])\nkeep_idx = nms(boxes, scores, iou_thresh=0.45)\n\nprint('keep index:', keep_idx)\nprint('kept boxes:')\nprint(boxes[keep_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に YOLO の出力テンソルを見ます。\n\nここでは簡略版として、`S x S` グリッドの各セルに `B` 個のボックスと `C` クラス確率を持つ形式を使います。\n例えば `image_size=96, S=6` なら1セルは `16x16` ピクセルです。\n`B=2, C=3` なら各セルの出力次元は `2*5+3=13` なので、全体出力は `(6, 6, 13)` です。\n\nテンソルは「数字を並べた箱」、アンカーは「各セルに用意した基準ボックスの型」です。\n物体中心が入ったセルを担当セルと呼び、そのセル内で最も適したアンカー1本に教師信号を与えます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iou_on_wh(box_wh, anchor_wh):\n    # 中心を一致させ、幅と高さだけで形状の近さを測るIoU（アンカー選択用）\n    bw, bh = box_wh\n    aw, ah = anchor_wh\n    inter = min(bw, aw) * min(bh, ah)\n    union = bw * bh + aw * ah - inter\n    return inter / (union + 1e-12)\n\n\ndef encode_yolo_target(gt_box_xyxy, class_id, image_size=96, S=6, B=2, C=3, anchors_wh=None):\n    # target shape: (S, S, B*5 + C)\n    # per anchor: (tx, ty, tw, th, obj)\n    target = np.zeros((S, S, B * 5 + C), dtype=np.float64)\n\n    # anchors_wh are normalized by image size (w,h in 0~1)\n    if anchors_wh is None:\n        if B == 2:\n            anchors_wh = np.array([[0.25, 0.25], [0.45, 0.45]], dtype=np.float64)\n        else:\n            sizes = np.linspace(0.2, 0.5, B)\n            anchors_wh = np.stack([sizes, sizes], axis=1)\n\n    gt_xywh = xyxy_to_xywh(gt_box_xyxy)\n    cx, cy, w, h = gt_xywh\n\n    cell_size = image_size / S\n    col = min(S - 1, int(cx // cell_size))\n    row = min(S - 1, int(cy // cell_size))\n\n    # cell_x, cell_y: relative position inside the cell (0~1)\n    # cell_w, cell_h: normalized by image_size (0~1)\n    # NOTE: この教材では tw/th の anchor-relative 変換(log比)を省略した簡略形を使う\n    cell_x = (cx / cell_size) - col\n    cell_y = (cy / cell_size) - row\n    cell_w = w / image_size\n    cell_h = h / image_size\n\n    ious = np.array([iou_on_wh((cell_w, cell_h), a) for a in anchors_wh], dtype=np.float64)\n    anchor_idx = int(np.argmax(ious))\n\n    start = anchor_idx * 5\n    target[row, col, start:start + 5] = np.array([cell_x, cell_y, cell_w, cell_h, 1.0])\n    target[row, col, B * 5 + class_id] = 1.0\n\n    return target, (row, col, anchor_idx), anchors_wh\n\n\ngt_box = np.array([24.0, 30.0, 58.0, 66.0])\ntarget, responsible, anchors = encode_yolo_target(gt_box, class_id=1, image_size=96, S=6, B=2, C=3)\n\nrow, col, anchor_idx = responsible\nprint('responsible (row, col, anchor):', responsible)\nprint('anchors (normalized w,h):', np.round(anchors, 3))\nprint('cell vector:', np.round(target[row, col], 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOLO学習では、座標・objectness（そのボックスに物体が存在する確率）・クラスの3種類の誤差を同時に最小化します。\n\n下の簡易損失は実装理解用で、最新YOLOの実装そのものではありません。\n形状は `pred_boxes: (S, S, B, 5)`, `obj_mask: (S, S, B)` を使います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def yolo_loss_components(pred, target, S=6, B=2, C=3, lambda_coord=5.0, lambda_noobj=0.5):\n    pred = pred.copy()\n    target = target.copy()\n\n    # pred_boxes/tgt_boxes: (S,S,B,5)  where 5=(tx,ty,tw,th,obj)\n    pred_boxes = pred[..., :B * 5].reshape(S, S, B, 5)\n    tgt_boxes = target[..., :B * 5].reshape(S, S, B, 5)\n\n    # class logits/targets: (S,S,C)\n    pred_cls = pred[..., B * 5:]\n    tgt_cls = target[..., B * 5:]\n\n    # Per-anchor masks\n    obj_mask = tgt_boxes[..., 4]                 # (S,S,B)\n    noobj_mask = 1.0 - obj_mask                  # (S,S,B)\n\n    # Coordinate and objectness losses apply on responsible anchors\n    coord_loss = np.sum(obj_mask[..., None] * (pred_boxes[..., :4] - tgt_boxes[..., :4]) ** 2)\n    obj_loss = np.sum(obj_mask * (pred_boxes[..., 4] - tgt_boxes[..., 4]) ** 2)\n\n    # No-object penalty applies to non-responsible anchors too\n    noobj_loss = np.sum(noobj_mask * (pred_boxes[..., 4] - tgt_boxes[..., 4]) ** 2)\n\n    # Class loss is cell-level (if any anchor in that cell has object)\n    cell_obj_mask = np.max(obj_mask, axis=2, keepdims=True)  # (S,S,1)\n    cls_loss = np.sum(cell_obj_mask * (pred_cls - tgt_cls) ** 2)\n\n    total = lambda_coord * coord_loss + obj_loss + lambda_noobj * noobj_loss + cls_loss\n\n    return {\n        'coord_loss': float(coord_loss),\n        'obj_loss': float(obj_loss),\n        'noobj_loss': float(noobj_loss),\n        'cls_loss': float(cls_loss),\n        'total': float(total),\n    }\n\n\nS, B, C = 6, 2, 3\n# これは学習ループではなく、損失の性質を比較するためのダミー予測\npred_far = np.random.uniform(low=0.0, high=1.0, size=(S, S, B * 5 + C))\npred_near = np.clip(target + np.random.normal(0.0, 0.05, size=(S, S, B * 5 + C)), 0.0, 1.0)\n\nloss_far = yolo_loss_components(pred_far, target, S=S, B=B, C=C)\nloss_near = yolo_loss_components(pred_near, target, S=S, B=B, C=C)\n\nprint('--- far prediction ---')\nfor k, v in loss_far.items():\n    print(k, round(v, 4))\n\nprint('--- near-to-target prediction ---')\nfor k, v in loss_near.items():\n    print(k, round(v, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、検出評価で使う AP（Average Precision）の最小実装を確認します。\nここでは1クラス・1画像群の簡易版で、IoU閾値を超えて未対応GTならTPとします。\n\nこの実装は教育目的の VOC2007 11点補間APです。\n実務では、複数クラス平均かつ IoU `0.50:0.95` の COCO mAP がよく使われます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_precision_from_pr(precision, recall):\n    # VOC2007 11-point interpolation (educational)\n    ap = 0.0\n    for t in np.linspace(0, 1, 11):\n        p = np.max(precision[recall >= t]) if np.any(recall >= t) else 0.0\n        ap += p / 11.0\n    return float(ap)\n\n\ndef evaluate_ap_single_class(pred_boxes, pred_scores, gt_boxes, iou_thresh=0.5):\n    order = np.argsort(pred_scores)[::-1]\n    matched = np.zeros(len(gt_boxes), dtype=bool)\n\n    tp = []\n    fp = []\n\n    for idx in order:\n        pb = pred_boxes[idx]\n        best_iou = 0.0\n        best_gt = -1\n        for g_i, gb in enumerate(gt_boxes):\n            iou = iou_xyxy(pb, gb)\n            if iou > best_iou:\n                best_iou = iou\n                best_gt = g_i\n\n        if best_iou >= iou_thresh and best_gt >= 0 and not matched[best_gt]:\n            tp.append(1)\n            fp.append(0)\n            matched[best_gt] = True\n        else:\n            tp.append(0)\n            fp.append(1)\n\n    tp = np.cumsum(tp)\n    fp = np.cumsum(fp)\n    precision = tp / np.maximum(tp + fp, 1e-12)\n    recall = tp / max(len(gt_boxes), 1)\n\n    ap = average_precision_from_pr(precision, recall)\n    return precision, recall, ap\n\n\ngt_boxes_eval = np.array([\n    [20, 18, 54, 50],\n    [58, 26, 86, 58],\n], dtype=np.float64)\n\npred_boxes_eval = np.array([\n    [19, 19, 52, 49],  # TP\n    [60, 24, 88, 60],  # TP\n    [15, 12, 30, 26],  # FP\n    [57, 24, 85, 57],  # duplicate near second GT -> FP\n], dtype=np.float64)\n\npred_scores_eval = np.array([0.93, 0.87, 0.44, 0.73])\n\nprecision, recall, ap = evaluate_ap_single_class(pred_boxes_eval, pred_scores_eval, gt_boxes_eval, iou_thresh=0.5)\nprint('precision:', np.round(precision, 4))\nprint('recall   :', np.round(recall, 4))\nprint('VOC07 AP@0.5 (11-point):', round(ap, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここまでの計算を踏まえ、実モデル側の設計を整理します。\n\n- 初期YOLO〜YOLOv5系: CNNバックボーン中心で高速\n- 最近の派生: ViT/Transformer系バックボーンを組み合わせる例も増加\n\nViTは長距離依存を扱いやすく、CNNは局所性と計算効率に強みがあります。\n実務では精度・速度・メモリ制約で使い分けます。\n\n次の最小PyTorch例は、YOLO本体そのものではなく、\n「バックボーン特徴を `S*S*(B*5+C)` に写像してYOLO形式に整形する流れ」を示すデモです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n    torch.manual_seed(42)\n\n    class TinyYoloHeadDemo(nn.Module):\n        def __init__(self, S=6, B=2, C=3):\n            super().__init__()\n            self.S, self.B, self.C = S, B, C\n            self.backbone = nn.Sequential(\n                nn.Conv2d(1, 8, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(8, 16, 3, padding=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d((1, 1)),\n            )\n            self.head = nn.Linear(16, S * S * (B * 5 + C))\n\n        def forward(self, x):\n            feat = self.backbone(x).flatten(1)\n            out = self.head(feat)\n            return out.view(-1, self.S, self.S, self.B * 5 + self.C)\n\n    model = TinyYoloHeadDemo(S=6, B=2, C=3)\n    n_params = sum(p.numel() for p in model.parameters())\n    print('TinyYoloHeadDemo params:', n_params)\n\n    x_dummy = torch.randn(4, 1, 64, 64)\n    out_dummy = model(x_dummy)\n    print('output shape:', tuple(out_dummy.shape))\n    print('NOTE: 形状理解用デモのため、実YOLOの空間ヘッド実装とは異なります。')\nelse:\n    print('PyTorch未導入のため、TinyYoloHeadDemo例はスキップしました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOLOを学ぶときは、次の順で確認すると迷いにくくなります。\n\n1. ボックス表現（`xyxy` / `xywh`）\n2. IoU と NMS\n3. ターゲットエンコード（担当セル）\n4. 損失の内訳（座標・objectness・クラス）\n5. AP/mAP の評価\n\nこの5点を押さえると、YOLO系論文や実装の差分を追いやすくなります。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
