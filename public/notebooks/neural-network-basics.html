<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>neural-network-basics</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="ニューラルネットワーク">ニューラルネットワーク</h1>
<p>ニューラルネットワークの理解で重要なのは、式を暗記することより「何を入力し、どこで誤差が生まれ、どう更新するか」を追えることです。<br>
このノートでは、単一ニューロン（ロジスティック回帰）から始めて、XORでの失敗、2層MLPによる改善、勾配チェック、ミニバッチ学習、PyTorch実装へ進みます。</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ModuleNotFoundError:
    torch = None
    nn = None
    optim = None
    TORCH_AVAILABLE = False

np.random.seed(42)</code></pre>
<p>まずは単一ニューロンで OR 問題を学習します。<br>
ここで使う損失（BCE）は、正解に高い確率を出すほど小さくなります。</p>
<ul>
<li>正解が 1 のとき: 予測確率 <code>p</code> が 1 に近いほど損失は小さい</li>
<li>正解が 0 のとき: <code>p</code> が 0 に近いほど損失は小さい</li>
</ul>
<p><code>grad_w</code>, <code>grad_b</code> は「どちらへ重みを動かすと損失が下がるか」を表す量です。</p>
<p><code>logits</code> は sigmoid を通す前の生のスコアで、まだ確率ではありません。</p>

<pre><code class="language-python">X_or = np.array([
    [0.0, 0.0],
    [0.0, 1.0],
    [1.0, 0.0],
    [1.0, 1.0],
])

y_or = np.array([[0.0], [1.0], [1.0], [1.0]])


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def bce_from_logits(y_true, logits):
    # log(1 + exp(logits)) - y*logits
    return float(np.mean(np.logaddexp(0.0, logits) - y_true * logits))


def train_logistic(X, y, lr=0.5, epochs=800):
    w = np.zeros((X.shape[1], 1))
    b = 0.0
    loss_history = []

    for _ in range(epochs):
        logits = X @ w + b
        prob = sigmoid(logits)
        loss = bce_from_logits(y, logits)

        grad_w = (X.T @ (prob - y)) / len(X)
        grad_b = float(np.mean(prob - y))

        w -= lr * grad_w
        b -= lr * grad_b

        loss_history.append(loss)

    return w, b, loss_history</code></pre>
<pre><code class="language-python">w_or, b_or, loss_or = train_logistic(X_or, y_or, lr=0.5, epochs=1000)
prob_or = sigmoid(X_or @ w_or + b_or)
pred_or = (prob_or &gt;= 0.5).astype(int)

print(&#39;w =&#39;, np.round(w_or.ravel(), 4), &#39;b =&#39;, round(float(b_or), 4))
print(&#39;prob =&#39;, np.round(prob_or.ravel(), 4))
print(&#39;pred =&#39;, pred_or.ravel())
print(&#39;true =&#39;, y_or.ravel().astype(int))</code></pre>
<pre><code class="language-python">fig, ax = plt.subplots(figsize=(6.2, 3.6))
ax.plot(loss_or, color=&#39;#2b6cb0&#39;)
ax.set_title(&#39;OR: Logistic Regression Loss&#39;)
ax.set_xlabel(&#39;epoch&#39;)
ax.set_ylabel(&#39;BCE loss&#39;)
plt.tight_layout()
plt.show()</code></pre>
<p>次に XOR 問題で同じモデルを試します。<br>
XOR では正例 <code>(0,1), (1,0)</code> と負例 <code>(0,0), (1,1)</code> が対角にあり、1本の直線では分けられません。</p>

<pre><code class="language-python">X_xor = np.array([
    [0.0, 0.0],
    [0.0, 1.0],
    [1.0, 0.0],
    [1.0, 1.0],
])

y_xor = np.array([[0.0], [1.0], [1.0], [0.0]])

w_xor, b_xor, loss_xor_logistic = train_logistic(X_xor, y_xor, lr=0.5, epochs=5000)
prob_xor_logistic = sigmoid(X_xor @ w_xor + b_xor)
pred_xor_logistic = (prob_xor_logistic &gt;= 0.5).astype(int)

print(&#39;logistic prob =&#39;, np.round(prob_xor_logistic.ravel(), 4))
print(&#39;logistic pred =&#39;, pred_xor_logistic.ravel())
print(&#39;true          =&#39;, y_xor.ravel().astype(int))</code></pre>
<pre><code class="language-python">fig, ax = plt.subplots(figsize=(6.2, 3.6))
ax.plot(loss_xor_logistic, color=&#39;#c05621&#39;)
ax.set_title(&#39;XOR: Logistic Regression Loss&#39;)
ax.set_xlabel(&#39;epoch&#39;)
ax.set_ylabel(&#39;BCE loss&#39;)
plt.tight_layout()
plt.show()</code></pre>
<p>ここで 2層MLP（入力→隠れ層→出力）を使います。<br>
隠れ層の非線形変換が入ることで、XORのような非線形境界を表現できます。</p>
<p>この後のコードで使う変数名:</p>
<ul>
<li><code>z1</code>: 隠れ層への入力</li>
<li><code>h</code>: 隠れ層の出力</li>
<li><code>z2</code>: 出力層への入力</li>
<li><code>p</code>: 予測確率</li>
<li><code>d*</code>: 損失の勾配（どちらへ動かせば損失が減るか）</li>
</ul>
<p>特に <code>tanh</code> の微分は <code>1 - tanh(z)^2</code> を使います。</p>

<pre><code class="language-python">def train_mlp_xor(X, y, hidden_dim=4, lr=0.2, epochs=5000, batch_size=None, seed=0):
    rng = np.random.default_rng(seed)

    W1 = rng.normal(0.0, 0.6, size=(X.shape[1], hidden_dim))
    b1 = np.zeros((1, hidden_dim))
    W2 = rng.normal(0.0, 0.6, size=(hidden_dim, 1))
    b2 = np.zeros((1, 1))

    loss_history = []

    if batch_size is None:
        batch_size = len(X)

    for _ in range(epochs):
        indices = rng.permutation(len(X))
        X_shuf = X[indices]
        y_shuf = y[indices]

        for start in range(0, len(X), batch_size):
            end = start + batch_size
            xb = X_shuf[start:end]
            yb = y_shuf[start:end]

            z1 = xb @ W1 + b1
            h = np.tanh(z1)
            z2 = h @ W2 + b2
            p = sigmoid(z2)

            dz2 = (p - yb) / len(xb)
            dW2 = h.T @ dz2
            db2 = np.sum(dz2, axis=0, keepdims=True)

            dh = dz2 @ W2.T
            dz1 = dh * (1 - np.tanh(z1) ** 2)
            dW1 = xb.T @ dz1
            db1 = np.sum(dz1, axis=0, keepdims=True)

            W2 -= lr * dW2
            b2 -= lr * db2
            W1 -= lr * dW1
            b1 -= lr * db1

        full_logits = np.tanh(X @ W1 + b1) @ W2 + b2
        loss_history.append(bce_from_logits(y, full_logits))

    params = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2}
    return params, loss_history


def mlp_predict_prob(X, params):
    z1 = X @ params[&quot;W1&quot;] + params[&quot;b1&quot;]
    h = np.tanh(z1)
    z2 = h @ params[&quot;W2&quot;] + params[&quot;b2&quot;]
    return sigmoid(z2)</code></pre>
<pre><code class="language-python">params_full, loss_xor_mlp = train_mlp_xor(X_xor, y_xor, hidden_dim=4, lr=0.2, epochs=5000, batch_size=4, seed=1)
prob_xor_mlp = mlp_predict_prob(X_xor, params_full)
pred_xor_mlp = (prob_xor_mlp &gt;= 0.5).astype(int)

print(&#39;mlp prob =&#39;, np.round(prob_xor_mlp.ravel(), 4))
print(&#39;mlp pred =&#39;, pred_xor_mlp.ravel())
print(&#39;true     =&#39;, y_xor.ravel().astype(int))</code></pre>
<pre><code class="language-python">fig, ax = plt.subplots(figsize=(6.4, 3.7))
ax.plot(loss_xor_logistic, label=&#39;logistic (single neuron)&#39;, color=&#39;#c05621&#39;)
ax.plot(loss_xor_mlp, label=&#39;2-layer MLP&#39;, color=&#39;#2b6cb0&#39;)
ax.set_title(&#39;XOR: Loss Comparison&#39;)
ax.set_xlabel(&#39;epoch&#39;)
ax.set_ylabel(&#39;BCE loss&#39;)
ax.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>逆伝播の実装が合っているかを確認する定番手法が勾配チェックです。</p>
<p>手順は3つです。</p>
<ol>
<li>ある重みを <code>+eps</code> / <code>-eps</code> だけ動かす</li>
<li>そのときの損失差から傾きを近似する（数値微分）</li>
<li>逆伝播で計算した勾配と近ければ実装は妥当と判断する</li>
</ol>
<p>目安として、<code>rel_err</code> が <code>1e-4</code> 以下なら逆伝播実装は概ね妥当と判断できます。</p>

<pre><code class="language-python">def mlp_forward(X, params):
    z1 = X @ params[&quot;W1&quot;] + params[&quot;b1&quot;]
    h = np.tanh(z1)
    z2 = h @ params[&quot;W2&quot;] + params[&quot;b2&quot;]
    p = sigmoid(z2)
    return z1, h, z2, p


def mlp_backward(X, y, params):
    z1, h, z2, p = mlp_forward(X, params)
    dz2 = (p - y) / len(X)
    dW2 = h.T @ dz2
    db2 = np.sum(dz2, axis=0, keepdims=True)

    dh = dz2 @ params[&quot;W2&quot;].T
    dz1 = dh * (1 - np.tanh(z1) ** 2)
    dW1 = X.T @ dz1
    db1 = np.sum(dz1, axis=0, keepdims=True)

    grads = {&quot;dW1&quot;: dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2}
    return grads


def mlp_loss(X, y, params):
    _, _, z2, _ = mlp_forward(X, params)
    return bce_from_logits(y, z2)


g = mlp_backward(X_xor, y_xor, params_full)
checks = [
    (&quot;W1&quot;, (0, 0)),
    (&quot;W1&quot;, (1, 2)),
    (&quot;W2&quot;, (0, 0)),
    (&quot;W2&quot;, (3, 0)),
]

eps = 1e-5
grad_map = {&quot;W1&quot;: g[&quot;dW1&quot;], &quot;W2&quot;: g[&quot;dW2&quot;]}

for name, idx in checks:
    params_plus = {k: v.copy() for k, v in params_full.items()}
    params_minus = {k: v.copy() for k, v in params_full.items()}

    params_plus[name][idx] += eps
    params_minus[name][idx] -= eps

    num = (mlp_loss(X_xor, y_xor, params_plus) - mlp_loss(X_xor, y_xor, params_minus)) / (2 * eps)
    ana = float(grad_map[name][idx])
    rel = abs(num - ana) / max(1e-12, abs(num) + abs(ana))

    print(f&quot;{name}{idx} -&gt; analytic={ana:.8f}, numeric={num:.8f}, rel_err={rel:.3e}&quot;)</code></pre>
<p>同じMLPでも、バッチサイズを変えると更新の揺れ方が変わります。<br>
このデータは4件なので、full batch は毎回4件で更新、mini-batch(2) は2件ずつ更新します。</p>

<pre><code class="language-python">params_batch2, loss_xor_batch2 = train_mlp_xor(X_xor, y_xor, hidden_dim=4, lr=0.2, epochs=5000, batch_size=2, seed=1)

fig, ax = plt.subplots(figsize=(6.4, 3.7))
ax.plot(loss_xor_mlp, label=&#39;full batch (4)&#39;, color=&#39;#2b6cb0&#39;)
ax.plot(loss_xor_batch2, label=&#39;mini-batch (2)&#39;, color=&#39;#2f855a&#39;)
ax.set_title(&#39;Batch Size Effect on XOR Training&#39;)
ax.set_xlabel(&#39;epoch&#39;)
ax.set_ylabel(&#39;BCE loss&#39;)
ax.legend()
plt.tight_layout()
plt.show()

prob_batch2 = mlp_predict_prob(X_xor, params_batch2)
pred_batch2 = (prob_batch2 &gt;= 0.5).astype(int)
print(&#39;mini-batch pred =&#39;, pred_batch2.ravel())</code></pre>
<p>最後に同じXORを PyTorch でも学習します。<br>
PyTorch が未導入の環境ではこの節をスキップするので、ノート全体はそのまま読み進められます。</p>

<pre><code class="language-python">if TORCH_AVAILABLE:
    torch.manual_seed(42)

    X_t = torch.tensor(X_xor, dtype=torch.float32)
    y_t = torch.tensor(y_xor, dtype=torch.float32)

    model = nn.Sequential(
        nn.Linear(2, 4),
        nn.Tanh(),
        nn.Linear(4, 1),
    )

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.2)

    torch_loss = []
    for _ in range(3000):
        logits = model(X_t)
        loss = criterion(logits, y_t)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        torch_loss.append(float(loss.detach()))

    with torch.no_grad():
        logits_t = model(X_t)
        prob_t = torch.sigmoid(logits_t).numpy()
        pred_t = (prob_t &gt;= 0.5).astype(int)

    print(&#39;torch prob =&#39;, np.round(prob_t.ravel(), 4))
    print(&#39;torch pred =&#39;, pred_t.ravel())

    fig, ax = plt.subplots(figsize=(6.2, 3.6))
    ax.plot(torch_loss, color=&#39;#6b46c1&#39;)
    ax.set_title(&#39;PyTorch MLP Loss (XOR)&#39;)
    ax.set_xlabel(&#39;epoch&#39;)
    ax.set_ylabel(&#39;BCE loss&#39;)
    plt.tight_layout()
    plt.show()
else:
    print(&#39;PyTorchが未導入のため、この節はスキップしました。&#39;)</code></pre>
<p>ここまでの流れを対応づけると、</p>
<ul>
<li>単一ニューロン: 線形境界の分類</li>
<li>2層MLP: 非線形境界の分類</li>
<li>逆伝播: 誤差を各層へ配る更新規則</li>
<li>勾配チェック: 実装検証</li>
<li>ミニバッチ: 計算効率と更新ノイズの調整</li>
<li>PyTorch: 実装を安全に高速化する実務ツール</li>
</ul>
<p>さらに対応を明示すると、<br>
<code>BCEWithLogitsLoss</code> は NumPy 側の logits ベース損失、<br>
<code>loss.backward()</code> は <code>mlp_backward</code>、<br>
<code>optimizer.step()</code> は重み更新ステップに対応します。</p>
<p>次のノート（損失関数と勾配降下法）では、更新規則をさらに体系化していきます。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>