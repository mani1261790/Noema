{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "except ModuleNotFoundError:\n",
        "    LinearRegression = None\n",
        "    print('scikit-learn is not installed')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# scikit-learnとXGBoostの使い方\n\nscikit-learnとXGBoostの使い方では、式を暗記するより、計算の意味をコードで検証する姿勢を優先します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 概念の土台\n\nscikit-learnとXGBoostの使い方に入る前に、つまずきやすい用語を先にそろえます。以降のコードでは、変数がどの概念を表しているかを対応付けながら読んでください。\n\n- **特徴量**: モデルが予測に使う入力変数です。列の作り方が精度に直結します。\n- **目的変数**: 当てたい値です。回帰では連続値、分類ではラベルになります。\n- **損失関数**: 予測のずれを数値化する関数です。学習はこの値を下げる方向に進みます。\n- **fit/predict**: fit で学習し、predict で推論するのが標準インターフェースです。\n- **勾配ブースティング**: 弱学習器を逐次追加して誤差を減らす手法群です。\n\nこのノートでは、ここで定義した語を実験セルの変数・式に直接対応させて確認します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 手を動かす 1: scikit-learn最小実行\n\nライブラリを使った実装は中身理解の後に行います。ここでは API の最小単位を確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = [1, 2, 3, 4, 5]\n",
        "y = [3, 5, 7, 9, 11]\n",
        "pairs = list(zip(x, y))\n",
        "print('pairs =', pairs)\n",
        "if globals().get('LinearRegression') is None:\n",
        "    print('scikit-learn is not installed')\n",
        "else:\n",
        "    X = [[1], [2], [3], [4]]\n",
        "    y = [2, 4, 6, 8]\n",
        "    model = LinearRegression().fit(X, y)\n",
        "    print('coef =', model.coef_[0], 'intercept =', model.intercept_)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "API を最小例で確認してから本番データへ移ると、デバッグの切り分けがしやすくなります。\n\nこの節では、特徴量 が入出力のどこを決めるかを中心に読める状態になれば十分です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 手を動かす 2: 単回帰を手で計算する\n\n次に、最小二乗法の係数を手計算で求めます。既存ライブラリを使う前に中身を一度体験すると理解が安定します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_bar = sum(x) / len(x)\ny_bar = sum(y) / len(y)\nnum = sum((xi - x_bar) * (yi - y_bar) for xi, yi in pairs)\nden = sum((xi - x_bar) ** 2 for xi in x)\nw1 = num / den; w0 = y_bar - w1 * x_bar\nprint('w0, w1 =', round(w0, 4), round(w1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "式の記号は抽象的に見えますが、コードでは平均・差分・和に分解されます。難しい式ほど、実装で部品に分けて確認するのが有効です。\n\nこの節では、特徴量 が入出力のどこを決めるかを中心に読める状態になれば十分です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 数式メモ\n\n1. $\\hat{y} = f_{\\theta}(x)$\n2. $L(\\theta) = \\frac{1}{N}\\sum_i \\ell(\\hat{y}_i, y_i)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 手を動かす 3: 予測と誤差を見る\n\nここで、求めた係数で予測を出し、誤差を数値化します。モデル改善は誤差の観察から始まります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = [w0 + w1 * xi for xi in x]\nresidual = [yi - pi for yi, pi in zip(y, pred)]\nmse = sum((r ** 2) for r in residual) / len(residual)\nprint('pred =', pred)\nprint('mse =', round(mse, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "誤差を観察するときは、平均値だけでなく個別の残差も見てください。偏りがあると、モデル構造の見直しが必要になります。\n\nこの節では、特徴量 が入出力のどこを決めるかを中心に読める状態になれば十分です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 手を動かす 4: 特徴量を拡張する\n\n次に、特徴量を追加したときに表現力がどう変わるかを確認します。特徴量設計は精度に直接効く実装作業です。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x2 = [xi ** 2 for xi in x]\nfeature = list(zip(x, x2))\nprint('feature sample =', feature[:3])\nscaled_x = [(xi - x_bar) / (max(x) - min(x)) for xi in x]\nprint('scaled_x =', [round(v, 4) for v in scaled_x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "特徴量を増やすと表現力は上がりますが、同時に過学習リスクも増えます。増やす理由を説明できる特徴だけを採用する姿勢が重要です。\n\nこの節では、特徴量 が入出力のどこを決めるかを中心に読める状態になれば十分です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 手を動かす 5: 評価分割を体験する\n\n最後に、学習と評価を分ける意味をコードで確かめます。分割しない評価は自己採点に近く、実運用の性能を過大評価しがちです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x, test_x = x[:3], x[3:]\ntrain_y, test_y = y[:3], y[3:]\npred_test = [w0 + w1 * xi for xi in test_x]\nmae_test = sum(abs(yi - pi) for yi, pi in zip(test_y, pred_test)) / len(test_y)\nprint('test mae =', round(mae_test, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "検証データでの誤差が急に悪化したら、モデルより先にデータ分布の差を疑ってください。この視点は実務で非常に重要です。\n\nこの節では、特徴量 が入出力のどこを決めるかを中心に読める状態になれば十分です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## この節の要点\n\n今回のノートで押さえておくべき誤解しやすい点を整理します。\n\n1. 訓練データと評価データの分布差を見ない\n2. 単一スコアだけでモデルを選んでしまう\n3. 前処理の漏れを評価後に気づく\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
