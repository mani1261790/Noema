{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# scikit-learnとXGBoostの使い方\n\nこのノートでは、分類モデルの実験を「分割設計 → 交差検証 → チューニング → 最終評価」の順で実装します。\n重要なのは、比較に使う設計を先に固定することです。モデルだけを入れ替えても、評価設計が曖昧だと結果を正しく解釈できません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 実行前準備\n\n想定環境は Python 3.10 以上です。\n未導入の場合は次を実行してください。\n\n`%pip install scikit-learn xgboost pandas numpy matplotlib seaborn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    StratifiedKFold,\n    cross_validate,\n    cross_val_predict,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n)\n\ntry:\n    from xgboost import XGBClassifier\n    XGBOOST_AVAILABLE = True\nexcept ModuleNotFoundError:\n    XGBClassifier = None\n    XGBOOST_AVAILABLE = False\n\nsns.set_theme(style=\"whitegrid\", context=\"notebook\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まずデータを確認します。\nこのデータでは `target=0` が malignant（悪性）、`target=1` が benign（良性）です。\nこのノートでは class 1（benign）を正例として Precision/Recall/F1 を計算します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cancer = load_breast_cancer(as_frame=True)\nX = cancer.data\ny = cancer.target\n\nsummary = pd.DataFrame({\n    \"n_samples\": [len(X)],\n    \"n_features\": [X.shape[1]],\n    \"benign_ratio(class1)\": [float(y.mean())],\n})\nsummary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_counts = y.value_counts().sort_index()\nclass_names = [cancer.target_names[i] for i in class_counts.index]\n\nfig, ax = plt.subplots(figsize=(6, 3.4))\nax.bar(class_names, class_counts.values, color=[\"#4c78a8\", \"#f58518\"])\nax.set_title(\"Class Balance\")\nax.set_ylabel(\"count\")\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここでデータを `dev`（開発用: 学習・CV・チューニングに使う）と `test`（最終確認専用）に分けます。\n`test` は最後の1回だけ使い、探索中には触れません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_dev, X_test, y_dev, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y,\n)\n\nprint(\"dev shape :\", X_dev.shape)\nprint(\"test shape:\", X_test.shape)\nprint(\"dev  benign ratio:\", round(float(y_dev.mean()), 3))\nprint(\"test benign ratio:\", round(float(y_test.mean()), 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "scikit-learn では Pipeline で前処理とモデルを一体化します。\nこれにより各 fold での前処理が訓練部分だけで fit され、リークを防ぎやすくなります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "primary_metric = \"roc_auc\"\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nbaseline_lr = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression(max_iter=4000, random_state=42)),\n])\n\nbaseline_cv = cross_validate(\n    baseline_lr,\n    X_dev,\n    y_dev,\n    cv=cv,\n    scoring={\"roc_auc\": \"roc_auc\", \"f1\": \"f1\"},\n    n_jobs=-1,\n    return_train_score=False,\n)\n\nbaseline_summary = {\n    \"roc_auc_mean\": float(np.mean(baseline_cv[\"test_roc_auc\"])),\n    \"roc_auc_std\": float(np.std(baseline_cv[\"test_roc_auc\"])),\n    \"f1_mean\": float(np.mean(baseline_cv[\"test_f1\"])),\n    \"f1_std\": float(np.std(baseline_cv[\"test_f1\"])),\n}\n\npd.Series(baseline_summary).round(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ROC-AUC mean/std:\", round(baseline_summary[\"roc_auc_mean\"], 4), \"/\", round(baseline_summary[\"roc_auc_std\"], 4))\nprint(\"F1      mean/std:\", round(baseline_summary[\"f1_mean\"], 4), \"/\", round(baseline_summary[\"f1_std\"], 4))\nprint(\"std が小さいほど、分割によるブレが小さく安定していると解釈できます。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`cross_val_predict` で得る予測は OOF（Out-of-Fold）予測です。\n各サンプルは、そのサンプルを学習に使っていないモデルで予測されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_oof_pred = cross_val_predict(\n    baseline_lr,\n    X_dev,\n    y_dev,\n    cv=cv,\n    method=\"predict\",\n    n_jobs=-1,\n)\ncm = confusion_matrix(y_dev, baseline_oof_pred)\n\nfig, ax = plt.subplots(figsize=(5, 4.2))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    cbar=False,\n    ax=ax,\n    xticklabels=class_names,\n    yticklabels=class_names,\n)\nax.set_title(\"Baseline Logistic (CV/OOF Confusion Matrix)\")\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"Actual\")\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に Logistic Regression をチューニングします。\nPipeline 内のパラメータは `<step名>__<パラメータ名>` で指定します。\nたとえば `model__C` は LogisticRegression の `C` を意味します。\nここでは L1/L2 の両方を扱える `solver='liblinear'` を使います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_search = GridSearchCV(\n    estimator=Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", LogisticRegression(max_iter=5000, random_state=42, solver=\"liblinear\")),\n    ]),\n    param_grid={\n        \"model__penalty\": [\"l1\", \"l2\"],\n        \"model__C\": [0.01, 0.1, 0.5, 1.0, 3.0, 10.0],\n    },\n    cv=cv,\n    scoring=primary_metric,\n    n_jobs=-1,\n)\nlr_search.fit(X_dev, y_dev)\n\nprint(\"best lr params:\", lr_search.best_params_)\nprint(f\"best lr cv {primary_metric}:\", round(lr_search.best_score_, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGBoost でも同じ `dev` データと同じ CV 設定で探索します。\n主なパラメータの意味は次の通りです。\n\n- `max_depth`: 木の深さ。大きいほど複雑な境界を表現できます。\n- `learning_rate`: 1本ごとの更新幅。小さくすると学習は遅いが安定しやすいです。\n- `n_estimators`: 木の本数。多いほど表現力は上がるが過学習リスクも増えます。\n- `min_child_weight`: 葉を分割するために必要な最小重み。大きいほど過学習を抑えます。\n\nここでは速度と探索範囲のバランスのため `RandomizedSearchCV(n_iter=20)` を使います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if XGBOOST_AVAILABLE:\n    xgb_search = RandomizedSearchCV(\n        estimator=XGBClassifier(\n            eval_metric=\"logloss\",\n            random_state=42,\n        ),\n        param_distributions={\n            \"n_estimators\": [150, 250, 400, 600],\n            \"max_depth\": [2, 3, 4, 5, 6],\n            \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n            \"subsample\": [0.7, 0.8, 0.9, 1.0],\n            \"colsample_bytree\": [0.6, 0.8, 1.0],\n            \"reg_lambda\": [0.5, 1.0, 3.0, 10.0],\n            \"min_child_weight\": [1, 3, 5],\n        },\n        n_iter=20,\n        scoring=primary_metric,\n        cv=cv,\n        random_state=42,\n        n_jobs=-1,\n    )\n    xgb_search.fit(X_dev, y_dev)\n\n    print(\"best xgb params:\", xgb_search.best_params_)\n    print(f\"best xgb cv {primary_metric}:\", round(xgb_search.best_score_, 4))\nelse:\n    xgb_search = None\n    print(\"xgboost が未導入のため、XGBoost探索をスキップしました。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "候補モデルの比較は、テストではなく CV スコアで行います。\nここで勝者を決め、最後にその1モデルだけを `test` で評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "candidates = {\n    \"Logistic (baseline)\": baseline_summary[f\"{primary_metric}_mean\"],\n    \"Logistic (tuned)\": float(lr_search.best_score_),\n}\nif xgb_search is not None:\n    candidates[\"XGBoost (tuned)\"] = float(xgb_search.best_score_)\n\ncv_compare = pd.DataFrame({\n    \"model\": list(candidates.keys()),\n    f\"cv_{primary_metric}\": list(candidates.values()),\n}).sort_values(f\"cv_{primary_metric}\", ascending=False)\ncv_compare\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "winner_name = cv_compare.iloc[0][\"model\"]\nif winner_name == \"Logistic (baseline)\":\n    final_model = baseline_lr\nelif winner_name == \"Logistic (tuned)\":\n    final_model = lr_search.best_estimator_\nelse:\n    final_model = xgb_search.best_estimator_\n\nfinal_model.fit(X_dev, y_dev)\nfinal_pred = final_model.predict(X_test)\nfinal_proba = final_model.predict_proba(X_test)[:, 1]\n\nfinal_metrics = {\n    \"accuracy\": accuracy_score(y_test, final_pred),\n    \"precision\": precision_score(y_test, final_pred),\n    \"recall\": recall_score(y_test, final_pred),\n    \"f1\": f1_score(y_test, final_pred),\n    \"roc_auc\": roc_auc_score(y_test, final_proba),\n}\n\nprint(\"selected model:\", winner_name)\npd.Series(final_metrics).round(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_test = confusion_matrix(y_test, final_pred)\nfig, ax = plt.subplots(figsize=(5, 4.2))\nsns.heatmap(\n    cm_test,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Greens\",\n    cbar=False,\n    ax=ax,\n    xticklabels=class_names,\n    yticklabels=class_names,\n)\nax.set_title(f\"Final Model on Test ({winner_name})\")\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"Actual\")\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if xgb_search is not None and winner_name == \"XGBoost (tuned)\":\n    importances = pd.Series(final_model.feature_importances_, index=X.columns).sort_values(ascending=False).head(12)\n    fig, ax = plt.subplots(figsize=(7.2, 5))\n    sns.barplot(x=importances.values, y=importances.index, orient=\"h\", ax=ax, color=\"#4c78a8\")\n    ax.set_title(\"Top 12 Feature Importances (Final XGBoost)\")\n    ax.set_xlabel(\"importance\")\n    ax.set_ylabel(\"feature\")\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"最終モデルがXGBoostではないか、xgboost未導入のため重要度表示をスキップしました。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "この流れで重要なのは、モデル比較に test を使わないことです。\n比較は dev 内 CV で完結させ、test は最終確認に一度だけ使います。\n\nscikit-learn は実験設計の再現性を作る土台として強く、XGBoost はその土台の上で精度を詰める有力候補です。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
