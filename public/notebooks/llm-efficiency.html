<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>llm-efficiency</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="llm軽量化-圧縮-最適化-効率化">LLM軽量化（圧縮・最適化・効率化）</h1>
<p>LLMの軽量化は、品質を必要以上に落とさずに、同じ予算でより速く・より長い文脈を扱うための設計です。ここでは、量子化・蒸留・PEFT・推論最適化を、実務で使う判断軸と一緒に整理します。</p>

<p>最初に前提を揃えます。軽量化を考える理由は大きく3つです。</p>
<p>1つ目はメモリ制約です。GPUメモリにモデル本体・KV cache・一時バッファが収まらないと、そもそも推論できません。2つ目はレイテンシです。応答が遅いと、対話体験が崩れます。3つ目はコストです。同じ問い合わせ数をさばくなら、1トークンあたりの計算とメモリ転送を下げるほど運用が楽になります。</p>
<p>このノートでは「なぜその手法を使うのか」を数字で確認しながら進めます。</p>

<p>用語ミニ辞典</p>
<ul>
<li>KV cache: デコード時に再利用する中間状態。長文になるほどメモリを圧迫しやすい。</li>
<li>PEFT: 一部パラメータだけ更新して学習コストを下げる手法群。</li>
<li>疎行列カーネル: 0の多い行列を効率計算する実装。無いとpruningが速度に効きにくい。</li>
<li>LLM-as-a-judge: 別モデルで出力品質を採点する手法。自己強化バイアスや位置バイアスがあるため、rubric評価と人手監査を併用する。</li>
</ul>

<pre><code class="language-python">import math
import random
from typing import Dict, List, Tuple</code></pre>
<h2 id="1-まずはメモリ予算を数字で見る">1. まずはメモリ予算を数字で見る</h2>
<p>モデルを軽くする話は、だいたい「重みを何bitで持つか」から始まります。まずは重みだけの理想値を計算します。</p>

<pre><code class="language-python">def bytes_for_params(num_params: int, bits: int) -&gt; float:
    return num_params * bits / 8


def to_gib(num_bytes: float) -&gt; float:
    return num_bytes / (1024 ** 3)


models = {
    &#39;7B&#39;: 7_000_000_000,
    &#39;13B&#39;: 13_000_000_000,
}

precisions = {
    &#39;FP16&#39;: 16,
    &#39;INT8&#39;: 8,
    &#39;INT4&#39;: 4,
}

for name, params in models.items():
    print(f&#39;--- {name} ({params:,} params) ---&#39;)
    for p_name, bits in precisions.items():
        print(f&#39;{p_name:&gt;4}: {to_gib(bytes_for_params(params, bits)):6.2f} GiB&#39;)
    print()</code></pre>
<p>同じ7Bでも、FP16からINT4に落とすと重みメモリは理論上1/4になります。ここで大事なのは、実際の推論では重み以外にKV cacheも効いてくる点です。長文対話ほどKV cacheの影響が大きくなるので、重みだけ見て安心すると詰まります。</p>

<h2 id="2-quantization-weight-activation-kv-cache">2. Quantization（Weight / Activation / KV cache）</h2>
<p>量子化は、数値の表現bit幅を下げてメモリ帯域と容量を削る手法です。重み量子化が最も普及していますが、長文推論ではKV cache量子化も効きます。さらに学習時にはActivation量子化が効く場面があります。</p>

<pre><code class="language-python">def kv_cache_bytes(
    batch_size: int,
    seq_len: int,
    n_layers: int,
    n_kv_heads: int,
    head_dim: int,
    bits: int,
) -&gt; float:
    # KとVの2本を保持する
    elements = batch_size * seq_len * n_layers * n_kv_heads * head_dim * 2
    return elements * bits / 8


cfg = {
    &#39;batch_size&#39;: 4,
    &#39;n_layers&#39;: 32,
    &#39;n_kv_heads&#39;: 8,
    &#39;head_dim&#39;: 128,
}

for seq in [1024, 4096, 8192]:
    fp16 = to_gib(kv_cache_bytes(seq_len=seq, bits=16, **cfg))
    int8 = to_gib(kv_cache_bytes(seq_len=seq, bits=8, **cfg))
    int4 = to_gib(kv_cache_bytes(seq_len=seq, bits=4, **cfg))
    print(f&#39;seq={seq:&gt;4}: FP16={fp16:5.2f} GiB, INT8={int8:5.2f} GiB, INT4={int4:5.2f} GiB&#39;)</code></pre>
<pre><code class="language-python">def activation_tensor_bytes(batch_size: int, seq_len: int, hidden_size: int, bits: int) -&gt; float:
    elements = batch_size * seq_len * hidden_size
    return elements * bits / 8


act_cfg = {
    &#39;batch_size&#39;: 8,
    &#39;seq_len&#39;: 4096,
    &#39;hidden_size&#39;: 4096,
}

for bits in [16, 8, 4]:
    gib = to_gib(activation_tensor_bytes(bits=bits, **act_cfg))
    print(f&#39;activation tensor ({bits:&gt;2}bit) = {gib:5.2f} GiB&#39;)</code></pre>
<p>Activation量子化は学習時メモリを下げるのに有効ですが、推論品質への影響が出やすい箇所でもあります。導入時は、レイテンシだけでなく精度劣化を必ず同時評価します。</p>

<p>同じモデルでも、文脈長を4倍にするとKV cacheもほぼ4倍になります。つまり、長文対応をしたいときの軽量化は「重みだけ」では不十分です。Weight量子化は入口で、運用ではKV cache設計がボトルネックになることが多いです。</p>

<h2 id="3-pruning-structured-unstructured">3. Pruning（Structured / Unstructured）</h2>
<p>Pruningは使っていない重みを削る手法です。ここで重要なのは、ゼロを増やすだけでは速くならないことです。実行カーネルが疎行列をうまく使えなければ、理論削減と実測速度は一致しません。</p>
<ul>
<li>Unstructured pruning: 個々の重みを間引く（精度維持しやすいが実装依存）</li>
<li>Structured pruning: 行・列・チャネル単位で削る（速度改善につなげやすい）</li>
</ul>

<pre><code class="language-python">random.seed(7)
rows, cols = 64, 64
weights = [[random.uniform(-1.0, 1.0) for _ in range(cols)] for _ in range(rows)]

# Unstructured pruning: 小さい重みを50%ゼロ化
flat = sorted(abs(x) for r in weights for x in r)
threshold = flat[len(flat) // 2]
unstructured_nonzero = sum(1 for r in weights for x in r if abs(x) &gt;= threshold)

# Structured pruning: 列ノルムの小さい50%を削除
col_norms = []
for c in range(cols):
    col_norms.append((sum(abs(weights[r][c]) for r in range(rows)), c))
col_norms.sort()
pruned_cols = set(c for _, c in col_norms[: cols // 2])
structured_nonzero = rows * (cols - len(pruned_cols))

print(&#39;total params        =&#39;, rows * cols)
print(&#39;unstructured kept   =&#39;, unstructured_nonzero)
print(&#39;structured kept     =&#39;, structured_nonzero)
print(&#39;structured speed proxy (active width ratio)=&#39;, round((cols - len(pruned_cols)) / cols, 3))</code></pre>
<p>Unstructuredは保持重みを柔軟に選べる一方、ハードウェアでそのまま速くなるとは限りません。Structuredは削る自由度は落ちますが、行列サイズそのものが小さくなるため、実運用で速度向上に結びつけやすくなります。</p>

<h2 id="4-knowledge-distillation">4. Knowledge Distillation</h2>
<p>蒸留は「大きいモデルのふるまい」を小さいモデルに移す方法です。なぜ嬉しいかというと、推論は小型モデルで回しつつ、教師モデルの一般化傾向を取り込めるからです。</p>
<ul>
<li><code>T</code>（温度）: 分布をなだらかにして、教師の暗黙知を見えやすくする。</li>
<li><code>T^2</code> 係数: 温度を入れたときに勾配スケールが崩れすぎないよう補正する。</li>
<li><code>alpha</code>: 正解ラベル重視（hard）と教師分布重視（soft）の比率を決める。</li>
</ul>

<pre><code class="language-python">def softmax(logits: List[float], temperature: float = 1.0) -&gt; List[float]:
    scaled = [x / temperature for x in logits]
    m = max(scaled)
    exps = [math.exp(x - m) for x in scaled]
    s = sum(exps)
    return [e / s for e in exps]


def kl_div(p: List[float], q: List[float]) -&gt; float:
    eps = 1e-12
    out = 0.0
    for pi, qi in zip(p, q):
        pi = max(pi, eps)
        qi = max(qi, eps)
        out += pi * math.log(pi / qi)
    return out


def cross_entropy(probs: List[float], gold_index: int) -&gt; float:
    return -math.log(max(probs[gold_index], 1e-12))


teacher_logits = [3.2, 2.4, -0.8, 0.1]
student_logits = [2.0, 1.9, -0.1, 0.2]
gold = 0

T = 2.0
alpha = 0.4  # hard target の重み

teacher_soft = softmax(teacher_logits, temperature=T)
student_soft = softmax(student_logits, temperature=T)
student_hard = softmax(student_logits, temperature=1.0)

hard_loss = cross_entropy(student_hard, gold)
soft_loss = (T ** 2) * kl_div(teacher_soft, student_soft)
loss = alpha * hard_loss + (1 - alpha) * soft_loss

print(&#39;hard_loss =&#39;, round(hard_loss, 4))
print(&#39;soft_loss =&#39;, round(soft_loss, 4))
print(&#39;total_distillation_loss =&#39;, round(loss, 4))</code></pre>
<p>蒸留で見ているのは「正解ラベル」だけではありません。教師の確率分布（どの選択肢をどれくらいあり得ると見ているか）も移すことで、学生モデルの判断を滑らかにできます。</p>

<h2 id="5-peft-lora-qlora-dora">5. PEFT（LoRA / QLoRA / DoRA）</h2>
<p>全重みを更新する代わりに、更新箇所を限定して学習コストを下げるのがPEFTです。</p>
<ul>
<li>LoRA: 低ランク行列だけ学習</li>
<li>QLoRA: ベース重みを4bit近傍で保持しつつLoRA学習</li>
<li>DoRA: LoRAの表現力を拡張した亜種（方向と大きさの扱いを分離）</li>
</ul>

<pre><code class="language-python">def lora_trainable_params(hidden_size: int, rank: int, n_layers: int, n_proj_per_layer: int = 4) -&gt; int:
    # 1つの線形層に対して A(in-&gt;r) と B(r-&gt;out) を持つ
    # ここでは in=out=hidden_size として概算
    per_proj = 2 * hidden_size * rank
    return per_proj * n_proj_per_layer * n_layers


def attention_proj_params(hidden_size: int, n_layers: int, n_proj_per_layer: int = 4) -&gt; int:
    # q,k,v,o の投影層のみを対象とした概算
    per_proj = hidden_size * hidden_size
    return per_proj * n_proj_per_layer * n_layers


def rough_transformer_params(hidden_size: int, n_layers: int, vocab_size: int = 32_000) -&gt; int:
    # 非常に粗い近似: 1層あたり attention(約4h^2) + MLP(約8h^2) = 約12h^2
    # + 埋め込み語彙
    return n_layers * (12 * hidden_size * hidden_size) + vocab_size * hidden_size


hidden = 4096
layers = 32
rank = 16

lora = lora_trainable_params(hidden, rank, layers)
attn_only = attention_proj_params(hidden, layers)
rough_total = rough_transformer_params(hidden, layers)

print(&#39;LoRA trainable params            =&#39;, f&#39;{lora:,}&#39;)
print(&#39;attention-proj params (partial)  =&#39;, f&#39;{attn_only:,}&#39;)
print(&#39;rough total model params         =&#39;, f&#39;{rough_total:,}&#39;)
print(&#39;LoRA ratio vs rough total        =&#39;, f&#39;{100 * lora / rough_total:.3f}%&#39;)

# QLoRA風のメモリ感（概算）
base_params = 7_000_000_000
base_int4_gib = to_gib(bytes_for_params(base_params, 4))
adapter_fp16_gib = to_gib(bytes_for_params(lora, 16))

# 学習時オーバーヘッドの粗い目安（grad, optimizer state, 一時バッファ）
overhead_gib = adapter_fp16_gib * 3.0 + 2.0
rough_train_peak = base_int4_gib + adapter_fp16_gib + overhead_gib

print(&#39;base(INT4) GiB                   =&#39;, round(base_int4_gib, 3))
print(&#39;adapter(FP16) GiB                =&#39;, round(adapter_fp16_gib, 3))
print(&#39;rough training peak GiB (toy)    =&#39;, round(rough_train_peak, 3))</code></pre>
<p>ここでの学びは、学習対象を絞るだけでメモリと学習時間のボトルネックが大きく変わる点です。QLoRAの実務価値は「1枚GPUでも回しやすくなる」ことですが、実際には勾配・optimizer・バッファ分の余裕を見積もってから判断します。INT4の理論値は量子化メタデータや実装差で増減するため、最終判断は実測ピークメモリで行います。</p>

<h2 id="6-アーキテクチャ最適化-attention-mlp">6. アーキテクチャ最適化（Attention / MLP）</h2>
<p>軽量化は量子化だけではありません。計算グラフ側を変えると、同じ精度帯で速度を稼げることがあります。特に文脈長が伸びるほど、Attentionの計算量をどう扱うかが効いてきます。</p>

<pre><code class="language-python">def attention_complexity(seq_len: int) -&gt; int:
    # QK^T の主要項だけを見た O(L^2) のスケール
    return seq_len * seq_len


def linear_attention_complexity(seq_len: int) -&gt; int:
    # 線形近似系を O(L) のスケールとして比較
    return seq_len


for L in [512, 2048, 8192, 32768]:
    quad = attention_complexity(L)
    lin = linear_attention_complexity(L)
    print(f&#39;L={L:&gt;5}: quadratic/linear ratio = {quad/lin:&gt;7.0f}x&#39;)


def kv_memory_factor(num_heads: int, num_kv_heads: int) -&gt; float:
    return num_kv_heads / num_heads


print(&#39;\nKV cache factor examples:&#39;)
print(&#39;MHA  (32/32):&#39;, kv_memory_factor(32, 32))
print(&#39;GQA  (32/8) :&#39;, kv_memory_factor(32, 8))
print(&#39;MQA  (32/1) :&#39;, kv_memory_factor(32, 1))</code></pre>
<p>GQA/MQAのような設計は、KV cacheを削って長文推論を実用化しやすくします。MLP側でも、活性化関数や中間次元の設計を見直すと、品質を保ったまま演算量を下げられる余地があります。</p>

<h2 id="7-推論最適化-pagedattention-continuous-batching-speculative-decoding">7. 推論最適化（PagedAttention / Continuous Batching / Speculative Decoding）</h2>
<p>推論では、モデルそのものより「リクエストのさばき方」が支配的になる場面があります。ここは運用側の最適化です。</p>

<pre><code class="language-python">def static_batch_utilization(lengths: List[int], slots: int) -&gt; Tuple[int, float]:
    steps = 0
    work = 0
    for i in range(0, len(lengths), slots):
        batch = lengths[i:i+slots]
        max_len = max(batch)
        steps += max_len
        work += sum(batch)
    util = work / (steps * slots)
    return steps, util


def continuous_batch_utilization(lengths: List[int], slots: int) -&gt; Tuple[int, float]:
    queue = list(lengths)
    active: List[int] = []
    steps = 0
    work = 0

    while queue or active:
        while queue and len(active) &lt; slots:
            active.append(queue.pop(0))

        steps += 1
        next_active = []
        for remain in active:
            remain -= 1
            work += 1
            if remain &gt; 0:
                next_active.append(remain)
        active = next_active

    util = work / (steps * slots)
    return steps, util


requests = [120, 80, 40, 20, 60, 55, 40, 35]
slots = 4

s_steps, s_util = static_batch_utilization(requests, slots)
c_steps, c_util = continuous_batch_utilization(requests, slots)

print(&#39;static steps      =&#39;, s_steps, &#39;| utilization =&#39;, round(s_util, 3))
print(&#39;continuous steps  =&#39;, c_steps, &#39;| utilization =&#39;, round(c_util, 3))</code></pre>
<pre><code class="language-python">def progress_per_verify_step_toy(draft_block: int, accept_rate: float) -&gt; float:
    # 1回の検証で平均して何トークン進むか（近似）
    return max(draft_block * accept_rate, 1e-6)


def speculative_speedup_toy(draft_block: int, accept_rate: float, draft_cost_ratio: float = 0.25) -&gt; float:
    # ごく粗い速度近似:
    #   1ラウンドのコスト = teacher検証(1.0) + draft生成(draft_cost_ratio * draft_block)
    #   進捗 = draft_block * accept_rate
    progress = progress_per_verify_step_toy(draft_block, accept_rate)
    cost_per_round = 1.0 + draft_cost_ratio * draft_block
    cost_per_token = cost_per_round / progress
    return 1.0 / cost_per_token


for k in [2, 4, 8]:
    for p in [0.4, 0.6, 0.8]:
        progress = progress_per_verify_step_toy(k, p)
        speed = speculative_speedup_toy(k, p, draft_cost_ratio=0.25)
        print(f&#39;k={k}, accept={p:.1f} -&gt; progress/verify={progress:.2f} tok, speedup(toy)=x{speed:.2f}&#39;)</code></pre>
<p>PagedAttentionはKV cacheをページ単位で管理して断片化を減らし、Continuous BatchingはGPUスロットの遊びを減らします。Speculative Decodingは、小さい下書きモデルがどれだけ当たるかで効果が変わります。ここで使った速度式はあくまで近似なので、実運用では必ず実測で検証します。</p>

<h2 id="8-どの順で導入するか">8. どの順で導入するか</h2>
<p>現場では、全部を同時に入れるより段階導入が安全です。次の順序が失敗しにくいです。</p>
<ol>
<li>まず推論観測（レイテンシ、GPUメモリ、トークン/秒）を取る</li>
<li>Weight量子化 + KV cache設計を入れる</li>
<li>バッチング戦略（Continuous Batching）を入れる</li>
<li>学習が必要ならLoRA/QLoRAで適応</li>
<li>品質が足りない場合だけ蒸留やアーキ変更に進む</li>
</ol>
<p>この順序にすると、どこで改善したかを切り分けやすく、ロールバックもしやすくなります。しきい値はプロダクト条件で変わるため、次のコードは目安ルールです。</p>

<pre><code class="language-python">def recommend_stack(
    vram_gib: float,
    latency_ms: int,
    quality_priority: str,
    draft_accept_rate_estimate: float = 0.5,
) -&gt; Dict[str, str]:
    # しきい値は単一GPU運用を想定した経験則の目安
    plan = {}

    if vram_gib &lt; 16:
        plan[&#39;model_precision&#39;] = &#39;INT4 or INT8 + KV cache optimization&#39;
        plan[&#39;fine_tuning&#39;] = &#39;QLoRA&#39;
    else:
        plan[&#39;model_precision&#39;] = &#39;INT8 or FP16 (quality-sensitive)&#39;
        plan[&#39;fine_tuning&#39;] = &#39;LoRA or full fine-tune (if required)&#39;

    if latency_ms &lt; 120 and draft_accept_rate_estimate &gt;= 0.55:
        plan[&#39;serving&#39;] = &#39;Continuous batching + speculative decoding&#39;
    else:
        plan[&#39;serving&#39;] = &#39;Continuous batching first, speculative optional&#39;

    if quality_priority == &#39;high&#39;:
        plan[&#39;quality_guard&#39;] = &#39;LLM-as-a-judge + rubric eval + bias-check + human spot-check&#39;
    else:
        plan[&#39;quality_guard&#39;] = &#39;rule-based eval + periodic human check&#39;

    return plan


print(recommend_stack(vram_gib=12, latency_ms=100, quality_priority=&#39;high&#39;, draft_accept_rate_estimate=0.6))</code></pre>
<p>軽量化の本質は、単にモデルを小さくすることではありません。限られた計算資源の中で、必要な品質・速度・コストを同時に満たす設計を作ることです。量子化、PEFT、蒸留、推論最適化は、すべてそのための道具です。だからこそ、導入のたびに品質評価と運用計測をセットで回すことが重要です。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>