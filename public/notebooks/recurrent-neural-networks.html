<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>recurrent-neural-networks</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<pre><code class="language-python">import math
import numpy as np
import matplotlib.pyplot as plt

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ModuleNotFoundError:
    torch = None
    nn = None
    optim = None
    TORCH_AVAILABLE = False</code></pre>
<h1 id="再帰型ニューラルネットワーク-rnn-lstm-gru">再帰型ニューラルネットワーク（RNN/LSTM/GRU）</h1>
<p>時系列データでは、いま見ている値だけでは正解が決まらないことがよくあります。<br>
例えば文章なら、次の単語を決めるために前の単語列が必要です。<br>
RNN系モデルは、過去の情報を「隠れ状態」として持ち運ぶことで、こうした問題を扱います。</p>

<p>最初に、最も単純なRNNの更新を実装します。</p>
<p>ここでは 1 次元入力を仮定します。<br>
<code>x_t</code> は時刻 <code>t</code> の入力、<code>h_t</code> は時刻 <code>t</code> の隠れ状態（記憶）です。<br>
<code>W_x</code> は入力に掛かる重み、<code>W_h</code> は前時刻の記憶に掛かる重み、<code>b</code> はバイアスです。</p>
<p>h_t = tanh(W_x x_t + W_h h_{t-1} + b)</p>
<p>この式の意味は「新しい入力 <code>x_t</code> と、ひとつ前の記憶 <code>h_{t-1}</code> を混ぜて、新しい記憶 <code>h_t</code> を作る」です。</p>

<pre><code class="language-python">def simple_rnn_forward(xs, wxh=0.9, whh=0.8, bh=0.0):
    h = 0.0
    history = []
    for t, x_t in enumerate(xs):
        # pre = W_x x_t + W_h h_{t-1} + b
        pre = wxh * x_t + whh * h + bh
        # h_t = tanh(pre)
        h = math.tanh(pre)
        history.append({&#39;t&#39;: t, &#39;x_t&#39;: x_t, &#39;pre&#39;: pre, &#39;h_t&#39;: h})
    return h, history


sequence = [0.2, -0.1, 0.5, 0.3, -0.4, 0.1]
final_h, hist = simple_rnn_forward(sequence, wxh=1.0, whh=0.75, bh=0.0)

for row in hist:
    print(f&quot;t={row[&#39;t&#39;]:&gt;2d}, x_t={row[&#39;x_t&#39;]:&gt;5.2f}, pre={row[&#39;pre&#39;]:&gt;7.4f}, h_t={row[&#39;h_t&#39;]:&gt;7.4f}&quot;)

print(&#39;final hidden state =&#39;, round(final_h, 4))</code></pre>
<pre><code class="language-python">t_axis = [r[&#39;t&#39;] for r in hist]
x_axis = [r[&#39;x_t&#39;] for r in hist]
h_axis = [r[&#39;h_t&#39;] for r in hist]

plt.figure(figsize=(7.2, 3.6))
plt.plot(t_axis, x_axis, marker=&#39;o&#39;, label=&#39;input x_t&#39;)
plt.plot(t_axis, h_axis, marker=&#39;s&#39;, label=&#39;hidden h_t&#39;)
plt.axhline(0, color=&#39;#999999&#39;, linewidth=1)
plt.xlabel(&#39;time step t&#39;)
plt.ylabel(&#39;value&#39;)
plt.title(&#39;Simple RNN: input vs hidden state&#39;)
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>RNNの弱点として、長い系列になると初期の情報が消えやすい問題があります。<br>
逆伝播では、時刻をまたいで勾配が連鎖的に掛け算されるため、係数が1より小さいと急速に小さくなります。<br>
この現象が勾配消失（vanishing gradient）です。</p>
<p>下の可視化は直感用の近似で、<code>|W_h|^T</code> のスカラー連鎖だけを取り出して見ています。<br>
実際には <code>tanh</code> の導関数やヤコビアン積も効くので、ここでの図は「雰囲気の把握」が目的です。</p>

<pre><code class="language-python">def recurrent_chain_gain(whh, steps):
    return np.abs(whh) ** steps


steps = np.arange(1, 61)
for whh in [0.5, 0.9, 1.1]:
    gains = recurrent_chain_gain(whh, steps)
    print(f&#39;whh={whh}: step 1 -&gt; {gains[0]:.6f}, step 30 -&gt; {gains[29]:.6f}, step 60 -&gt; {gains[59]:.6f}&#39;)

plt.figure(figsize=(7.2, 3.6))
for whh in [0.5, 0.9, 1.1]:
    plt.plot(steps, recurrent_chain_gain(whh, steps), label=f&#39;|whh|={whh}&#39;)
plt.yscale(&#39;log&#39;)
plt.xlabel(&#39;time distance&#39;)
plt.ylabel(&#39;gradient scale (log)&#39;)
plt.title(&#39;Why long-term dependency is hard for plain RNN&#39;)
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>LSTMはこの問題に対して、記憶セル <code>c_t</code> と3つのゲートを導入します。</p>
<ul>
<li>忘却ゲート <code>f_t</code>: どれだけ過去記憶を残すか</li>
<li>入力ゲート <code>i_t</code>: 新しい情報をどれだけ入れるか</li>
<li>出力ゲート <code>o_t</code>: 記憶をどれだけ外に見せるか</li>
<li>候補情報 <code>g_t</code>: 新しく書き込みたい内容</li>
</ul>
<p><code>c_t</code> を足し算中心で更新することで、勾配が流れやすくなります。</p>

<pre><code class="language-python">def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def init_lstm_params(input_size=1, hidden_size=2, seed=7):
    rng = np.random.default_rng(seed)
    concat_size = input_size + hidden_size

    def w(shape):
        return rng.normal(0.0, 0.35, size=shape)

    params = {
        &#39;W_i&#39;: w((hidden_size, concat_size)),
        &#39;b_i&#39;: np.zeros(hidden_size),
        &#39;W_f&#39;: w((hidden_size, concat_size)),
        &#39;b_f&#39;: np.zeros(hidden_size),
        &#39;W_g&#39;: w((hidden_size, concat_size)),
        &#39;b_g&#39;: np.zeros(hidden_size),
        &#39;W_o&#39;: w((hidden_size, concat_size)),
        &#39;b_o&#39;: np.zeros(hidden_size),
    }
    return params


def lstm_step(x_t, h_prev, c_prev, p):
    # x_t:(1,), h_prev/c_prev:(2,), W_*:(2,3), b_*:(2,)
    z = np.concatenate([x_t, h_prev])

    i_t = sigmoid(p[&#39;W_i&#39;] @ z + p[&#39;b_i&#39;])
    f_t = sigmoid(p[&#39;W_f&#39;] @ z + p[&#39;b_f&#39;])
    g_t = np.tanh(p[&#39;W_g&#39;] @ z + p[&#39;b_g&#39;])
    o_t = sigmoid(p[&#39;W_o&#39;] @ z + p[&#39;b_o&#39;])

    c_t = f_t * c_prev + i_t * g_t
    h_t = o_t * np.tanh(c_t)

    return h_t, c_t, {&#39;i_t&#39;: i_t, &#39;f_t&#39;: f_t, &#39;g_t&#39;: g_t, &#39;o_t&#39;: o_t}


params = init_lstm_params(input_size=1, hidden_size=2, seed=7)
xs = [np.array([v]) for v in [0.6, -0.2, 0.1, 0.5, -0.4]]
h = np.zeros(2)
c = np.zeros(2)

for t, x_t in enumerate(xs):
    h, c, gates = lstm_step(x_t, h, c, params)
    print(f&#39;t={t}, x={x_t[0]:&gt;5.2f}&#39;)
    print(&#39;  i_t=&#39;, np.round(gates[&#39;i_t&#39;], 4), &#39;f_t=&#39;, np.round(gates[&#39;f_t&#39;], 4), &#39;o_t=&#39;, np.round(gates[&#39;o_t&#39;], 4))
    print(&#39;  c_t=&#39;, np.round(c, 4), &#39;h_t=&#39;, np.round(h, 4))

print(&#39;読み方の目安: f_t が 1 に近いほど過去記憶を残し、0 に近いほど忘れます。&#39;)</code></pre>
<p>GRUはLSTMを簡素化した派生モデルです。<br>
セル状態 <code>c_t</code> を分離せず、隠れ状態 <code>h_t</code> だけを更新します。</p>
<ul>
<li>更新ゲート <code>z_t</code>: 以前の隠れ状態をどれだけ残すか（PyTorch流儀）</li>
<li>リセットゲート <code>r_t</code>: 候補状態計算で過去情報をどれだけ使うか</li>
</ul>
<p>一般に、GRUはLSTMよりパラメータが少なく、学習が軽いことがあります。</p>

<pre><code class="language-python">def count_rnn_params_single_bias(input_size, hidden_size):
    # 理論の最小形（重み1組 + bias1組）
    return hidden_size * (input_size + hidden_size) + hidden_size


def count_params_pytorch_style(gates, input_size, hidden_size):
    # PyTorchは bias_ih と bias_hh の2本を持つ
    return gates * (hidden_size * input_size + hidden_size * hidden_size + 2 * hidden_size)


input_size = 16
hidden_size = 64

print(&#39;--- single-bias theoretical count ---&#39;)
print(&#39;RNN :&#39;, count_rnn_params_single_bias(input_size, hidden_size))
print(&#39;LSTM:&#39;, 4 * count_rnn_params_single_bias(input_size, hidden_size))
print(&#39;GRU :&#39;, 3 * count_rnn_params_single_bias(input_size, hidden_size))

print(&#39;--- PyTorch parameter count (bias_ih + bias_hh) ---&#39;)
print(&#39;RNN :&#39;, count_params_pytorch_style(1, input_size, hidden_size))
print(&#39;LSTM:&#39;, count_params_pytorch_style(4, input_size, hidden_size))
print(&#39;GRU :&#39;, count_params_pytorch_style(3, input_size, hidden_size))</code></pre>
<pre><code class="language-python">def init_gru_params(input_size=1, hidden_size=2, seed=9):
    rng = np.random.default_rng(seed)
    concat_size = input_size + hidden_size

    def w(shape):
        return rng.normal(0.0, 0.35, size=shape)

    return {
        &#39;W_z&#39;: w((hidden_size, concat_size)),
        &#39;b_z&#39;: np.zeros(hidden_size),
        &#39;W_r&#39;: w((hidden_size, concat_size)),
        &#39;b_r&#39;: np.zeros(hidden_size),
        &#39;W_h&#39;: w((hidden_size, concat_size)),
        &#39;b_h&#39;: np.zeros(hidden_size),
    }


def gru_step(x_t, h_prev, p):
    z_in = np.concatenate([x_t, h_prev])
    z_t = sigmoid(p[&#39;W_z&#39;] @ z_in + p[&#39;b_z&#39;])
    r_t = sigmoid(p[&#39;W_r&#39;] @ z_in + p[&#39;b_r&#39;])

    h_candidate_in = np.concatenate([x_t, r_t * h_prev])
    h_tilde = np.tanh(p[&#39;W_h&#39;] @ h_candidate_in + p[&#39;b_h&#39;])

    # PyTorchと同じ流儀: z_t は過去状態を残す割合
    h_t = z_t * h_prev + (1.0 - z_t) * h_tilde
    return h_t, {&#39;z_t&#39;: z_t, &#39;r_t&#39;: r_t, &#39;h_tilde&#39;: h_tilde}


gru_params = init_gru_params(input_size=1, hidden_size=2, seed=9)
h = np.zeros(2)
for t, x_t in enumerate(xs):
    h, gates = gru_step(x_t, h, gru_params)
    print(f&#39;t={t}, x={x_t[0]:&gt;5.2f}, z_t={np.round(gates[&quot;z_t&quot;],4)}, r_t={np.round(gates[&quot;r_t&quot;],4)}, h_t={np.round(h,4)}&#39;)</code></pre>
<p>ここからはPyTorchで、RNN/LSTM/GRUの違いを同じ課題で比較します。<br>
課題は「系列の最初の値が正か負かを、最後の時刻で判定する」です。<br>
途中にノイズが多いため、初期情報を保持できるモデルほど有利です。</p>
<p><code>logit</code> は確率に変換する前の値で、<code>BCEWithLogitsLoss</code> は2値分類の標準損失です。<br>
予測時は <code>sigmoid(logit) &gt;= 0.5</code> を陽性判定に使います。</p>
<p>期待値としては RNN &lt; (GRU, LSTM) になりやすいですが、データ分布やハイパーパラメータで逆転もあります。</p>

<pre><code class="language-python">if TORCH_AVAILABLE:
    np.random.seed(0)

    def build_memory_dataset(n_samples=768, seq_len=28):
        x = np.random.randn(n_samples, seq_len, 1).astype(np.float32)
        y = (x[:, 0, 0] &gt; 0).astype(np.float32)
        return torch.tensor(x), torch.tensor(y)

    x_train, y_train = build_memory_dataset(n_samples=768, seq_len=28)
    x_val, y_val = build_memory_dataset(n_samples=256, seq_len=28)

    class SequenceClassifier(nn.Module):
        def __init__(self, cell_type=&#39;RNN&#39;, hidden_size=24):
            super().__init__()
            if cell_type == &#39;RNN&#39;:
                self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, batch_first=True)
            elif cell_type == &#39;LSTM&#39;:
                self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_size, batch_first=True)
            elif cell_type == &#39;GRU&#39;:
                self.rnn = nn.GRU(input_size=1, hidden_size=hidden_size, batch_first=True)
            else:
                raise ValueError(&#39;Unknown cell_type&#39;)
            self.head = nn.Linear(hidden_size, 1)

        def forward(self, x):
            out, _ = self.rnn(x)
            last = out[:, -1, :]
            logit = self.head(last).squeeze(-1)
            return logit

    def train_and_eval(cell_type, seed=0, epochs=6, lr=1e-2):
        torch.manual_seed(seed)
        model = SequenceClassifier(cell_type=cell_type, hidden_size=24)
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)

        batch_size = 64
        for _ in range(epochs):
            perm = torch.randperm(x_train.size(0))
            for i in range(0, x_train.size(0), batch_size):
                idx = perm[i:i+batch_size]
                xb, yb = x_train[idx], y_train[idx]

                optimizer.zero_grad()
                logits = model(xb)
                loss = criterion(logits, yb)
                loss.backward()
                optimizer.step()

        with torch.no_grad():
            logits = model(x_val)
            preds = (torch.sigmoid(logits) &gt;= 0.5).float()
            acc = (preds == y_val).float().mean().item()
        return acc

    seeds = [0, 1, 2]
    for ct in [&#39;RNN&#39;, &#39;LSTM&#39;, &#39;GRU&#39;]:
        accs = [train_and_eval(ct, seed=s) for s in seeds]
        print(f&#39;{ct} accuracy: mean={np.mean(accs):.4f}, std={np.std(accs):.4f}, each={np.round(accs,4)}&#39;)

    print(&#39;注: hidden_sizeを固定した簡易比較で、パラメータ数を厳密に一致させた比較ではありません。&#39;)
else:
    print(&#39;PyTorch未導入のため比較実験セルはスキップしました。&#39;)</code></pre>
<p>実務での使い分けは、まずGRUかLSTMを基準にし、必要に応じて単純RNNを比較に置くのが安全です。<br>
系列が長い、あるいは初期情報の保持が重要な課題では、単純RNNだけで戦うよりLSTM/GRUのほうが安定しやすくなります。</p>
<p>一方で、長大な系列や並列計算効率が重要な場面ではTransformer系が選ばれることも多く、RNN系は軽量性やオンライン処理の文脈で依然有効です。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>