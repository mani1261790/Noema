{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 畳み込みとCNN\n\n畳み込みニューラルネットワーク（CNN）は、画像の中から「どこにあるか」をある程度保ちながら特徴を抽出するための設計です。\nこのノートでは、畳み込みの計算を手で追える形から始め、stride・padding・pooling の役割を確認し、最後に小さな画像分類を実装します。\n\nさらに、分類CNNがどう Fully Convolutional Network（FCN）へ拡張されるかまでつなげます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    TORCH_AVAILABLE = True\nexcept ModuleNotFoundError:\n    torch = None\n    nn = None\n    optim = None\n    TORCH_AVAILABLE = False\n\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "まず、1チャネル画像に対する2次元演算を自分で実装します。\nここでは深層学習ライブラリ（`Conv2d`）と同じ慣習に合わせ、カーネル反転なしの相互相関（cross-correlation）を標準にします。\n\n厳密な数学的畳み込みを見たいときは、カーネルを上下左右反転してから積和します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv2d_single(image, kernel, stride=1, padding=0, flip_kernel=False):\n    h, w = image.shape\n    kh, kw = kernel.shape\n    k = np.flip(kernel, axis=(0, 1)) if flip_kernel else kernel\n\n    if padding > 0:\n        padded = np.pad(image, ((padding, padding), (padding, padding)), mode='constant')\n    else:\n        padded = image\n\n    ph, pw = padded.shape\n    out_h = (ph - kh) // stride + 1\n    out_w = (pw - kw) // stride + 1\n    out = np.zeros((out_h, out_w), dtype=np.float64)\n\n    for i in range(out_h):\n        for j in range(out_w):\n            patch = padded[i * stride:i * stride + kh, j * stride:j * stride + kw]\n            out[i, j] = np.sum(patch * k)\n\n    return out\n\n\ndef maxpool2d_single(feature_map, pool=2, stride=2):\n    h, w = feature_map.shape\n    out_h = (h - pool) // stride + 1\n    out_w = (w - pool) // stride + 1\n    out = np.zeros((out_h, out_w), dtype=np.float64)\n\n    for i in range(out_h):\n        for j in range(out_w):\n            patch = feature_map[i * stride:i * stride + pool, j * stride:j * stride + pool]\n            out[i, j] = np.max(patch)\n\n    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "toy = np.zeros((10, 10), dtype=np.float64)\ntoy[2:8, 4:6] = 1.0\ntoy[6:8, 1:9] = 1.0\n\nkernel_vertical = np.array([\n    [-1, 0, 1],\n    [-2, 0, 2],\n    [-1, 0, 1],\n], dtype=np.float64)\n\nkernel_horizontal = np.array([\n    [-1, -2, -1],\n    [ 0,  0,  0],\n    [ 1,  2,  1],\n], dtype=np.float64)\n\nfeat_v = conv2d_single(toy, kernel_vertical, stride=1, padding=1)\nfeat_h = conv2d_single(toy, kernel_horizontal, stride=1, padding=1)\n\nfig, axes = plt.subplots(1, 3, figsize=(10.5, 3.2))\naxes[0].imshow(toy, cmap='gray', vmin=0, vmax=1)\naxes[0].set_title('Input')\naxes[1].imshow(feat_v, cmap='coolwarm')\naxes[1].set_title('Vertical-edge response')\naxes[2].imshow(feat_h, cmap='coolwarm')\naxes[2].set_title('Horizontal-edge response')\nfor ax in axes:\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`padding` は周辺情報を残したまま畳み込みするために使います。\n`stride` はカーネルの移動幅で、値を大きくすると出力解像度が下がります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_no_pad = conv2d_single(toy, kernel_vertical, stride=1, padding=0)\nout_pad1 = conv2d_single(toy, kernel_vertical, stride=1, padding=1)\nout_stride2 = conv2d_single(toy, kernel_vertical, stride=2, padding=1)\n\nprint('input shape      :', toy.shape)\nprint('no padding shape :', out_no_pad.shape)\nprint('padding=1 shape  :', out_pad1.shape)\nprint('stride=2 shape   :', out_stride2.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pooled = maxpool2d_single(np.maximum(feat_v, 0.0), pool=2, stride=2)\n\nfig, axes = plt.subplots(1, 2, figsize=(7.4, 3.0))\naxes[0].imshow(np.maximum(feat_v, 0.0), cmap='magma')\naxes[0].set_title('ReLU(feature)')\naxes[1].imshow(pooled, cmap='magma')\naxes[1].set_title('MaxPool 2x2')\nfor ax in axes:\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、CNNが全結合層よりパラメータ効率が良い理由を数で確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 32x32x3 画像を 64 ユニットへ直接全結合する場合\nfc_params = 32 * 32 * 3 * 64 + 64\n\n# 3x3 Conv (in=3, out=64) の場合\nconv_params = 3 * 3 * 3 * 64 + 64\n\nprint('Fully connected params:', fc_params)\nprint('Conv 3x3 params      :', conv_params)\nprint('FC / Conv ratio      :', round(fc_params / conv_params, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ここから小規模データでCNN的な分類を体験します。\n16x16画像に「横帯（class 0）」か「縦帯（class 1）」を描き、ノイズを混ぜたデータを作ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_stripe_image(kind, size=16, noise_std=0.12, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    img = np.zeros((size, size), dtype=np.float64)\n\n    if kind == 0:  # horizontal stripe\n        y0 = int(rng.integers(3, size - 3))\n        img[max(0, y0 - 1):min(size, y0 + 1), :] = 1.0\n    else:  # vertical stripe\n        x0 = int(rng.integers(3, size - 3))\n        img[:, max(0, x0 - 1):min(size, x0 + 1)] = 1.0\n\n    img += rng.normal(0.0, noise_std, size=(size, size))\n    return np.clip(img, 0.0, 1.0)\n\n\ndef make_dataset(n_samples=400, size=16, seed=0):\n    rng = np.random.default_rng(seed)\n    X = np.zeros((n_samples, size, size), dtype=np.float64)\n    y = np.zeros((n_samples,), dtype=np.int64)\n\n    for i in range(n_samples):\n        label = int(rng.integers(0, 2))\n        X[i] = make_stripe_image(label, size=size, rng=rng)\n        y[i] = label\n\n    return X, y\n\nX_all, y_all = make_dataset(n_samples=500, size=16, seed=7)\n\nfig, axes = plt.subplots(2, 5, figsize=(9.2, 3.8))\nfor i, ax in enumerate(axes.ravel()):\n    ax.imshow(X_all[i], cmap='gray', vmin=0, vmax=1)\n    ax.set_title(f'label={y_all[i]}', fontsize=9)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN全体を最初から実装する代わりに、\n\n1. 畳み込み + ReLU + Pooling で特徴抽出（ここは固定フィルタで学習しない）\n2. その特徴に線形分類器（Softmax）を学習（ここで学習するのは `W, b`）\n\nという2段構成で、CNNの役割分担を見える化します。まずは「何が固定で、何を学習しているか」に注目してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filters = [\n    np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float64),\n    np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=np.float64),\n    np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float64),\n]\n\n\ndef extract_features(images, kernels):\n    feats = []\n    for img in images:\n        f_img = []\n        for k in kernels:\n            conv = conv2d_single(img, k, stride=1, padding=1)\n            act = np.maximum(conv, 0.0)\n            pool = maxpool2d_single(act, pool=2, stride=2)\n            f_img.append(np.mean(pool))\n            f_img.append(np.max(pool))\n        feats.append(f_img)\n    return np.array(feats, dtype=np.float64)\n\nfeatures_all = extract_features(X_all, filters)\nprint('feature shape:', features_all.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = np.random.permutation(len(X_all))\ntrain_size = int(0.8 * len(X_all))\ntrain_idx = idx[:train_size]\ntest_idx = idx[train_size:]\n\nX_train = features_all[train_idx]\ny_train = y_all[train_idx]\nX_test = features_all[test_idx]\ny_test = y_all[test_idx]\n\n\ndef softmax(logits):\n    z = logits - np.max(logits, axis=1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n\ndef one_hot(y, n_classes):\n    out = np.zeros((len(y), n_classes), dtype=np.float64)\n    out[np.arange(len(y)), y] = 1.0\n    return out\n\n\ndef train_softmax(X, y, lr=0.1, epochs=500):\n    n, d = X.shape\n    c = int(np.max(y)) + 1\n    W = np.zeros((d, c), dtype=np.float64)\n    b = np.zeros((1, c), dtype=np.float64)\n    Y = one_hot(y, c)\n\n    hist = []\n    for _ in range(epochs):\n        logits = X @ W + b\n        prob = softmax(logits)\n\n        loss = -np.mean(np.sum(Y * np.log(prob + 1e-12), axis=1))\n        hist.append(loss)\n\n        dlogits = (prob - Y) / n\n        dW = X.T @ dlogits\n        db = np.sum(dlogits, axis=0, keepdims=True)\n\n        W -= lr * dW\n        b -= lr * db\n\n    return W, b, hist\n\nW_cls, b_cls, loss_cls = train_softmax(X_train, y_train, lr=0.2, epochs=600)\n\ntrain_pred = np.argmax(X_train @ W_cls + b_cls, axis=1)\ntest_pred = np.argmax(X_test @ W_cls + b_cls, axis=1)\n\ntrain_acc = np.mean(train_pred == y_train)\ntest_acc = np.mean(test_pred == y_test)\n\nprint('train acc:', round(float(train_acc), 4))\nprint('test  acc:', round(float(test_acc), 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6.0, 3.5))\nax.plot(loss_cls, color='#2b6cb0')\nax.set_title('Classifier Loss on Conv Features')\nax.set_xlabel('epoch')\nax.set_ylabel('cross-entropy')\nplt.tight_layout()\nplt.show()\n\ncm = np.zeros((2, 2), dtype=int)\nfor yt, yp in zip(y_test, test_pred):\n    cm[yt, yp] += 1\nprint('confusion matrix (rows=true, cols=pred)')\nprint(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "同じタスクを PyTorch の小さなCNNで学習します。\n今回は出力を2ユニット（class 0/1）で表す設計なので `CrossEntropyLoss` を使います。\n（1ユニットで陽性確率を直接出す設計なら `BCEWithLogitsLoss` を使います。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n    torch.manual_seed(42)\n\n    X_train_img = torch.tensor(X_all[train_idx][:, None, :, :], dtype=torch.float32)\n    y_train_t = torch.tensor(y_all[train_idx], dtype=torch.long)\n    X_test_img = torch.tensor(X_all[test_idx][:, None, :, :], dtype=torch.float32)\n    y_test_t = torch.tensor(y_all[test_idx], dtype=torch.long)\n\n    model = nn.Sequential(\n        nn.Conv2d(1, 8, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2),\n        nn.Conv2d(8, 16, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.AdaptiveAvgPool2d((1, 1)),\n        nn.Flatten(),\n        nn.Linear(16, 2),\n    )\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n    loss_torch = []\n    for _ in range(60):\n        logits = model(X_train_img)\n        loss = criterion(logits, y_train_t)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_torch.append(float(loss.detach()))\n\n    with torch.no_grad():\n        train_pred_t = torch.argmax(model(X_train_img), dim=1)\n        test_pred_t = torch.argmax(model(X_test_img), dim=1)\n        train_acc_t = (train_pred_t == y_train_t).float().mean().item()\n        test_acc_t = (test_pred_t == y_test_t).float().mean().item()\n\n    print('torch train acc:', round(train_acc_t, 4))\n    print('torch test  acc:', round(test_acc_t, 4))\n\n    fig, ax = plt.subplots(figsize=(6.0, 3.5))\n    ax.plot(loss_torch, color='#6b46c1')\n    ax.set_title('PyTorch CNN Training Loss')\n    ax.set_xlabel('epoch')\n    ax.set_ylabel('cross-entropy')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('PyTorchが未導入のため、この節はスキップしました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "最後に、分類CNNとFCNの関係を形で確認します。\n分類CNNは最後に全結合でクラスを出しますが、FCNは1x1畳み込みで「各位置のクラススコア」を出すため、セグメンテーションに使えます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 形だけを確認する簡易デモ\nfeat_map = np.random.randn(1, 8, 6, 6)  # (batch, channel, height, width)\nconv1x1_w = np.random.randn(3, 8)        # 3クラス用 1x1 conv 重み\n\n# 1x1 conv: 各位置で channel 方向の内積\nlogits_map = np.einsum('oc,bchw->bohw', conv1x1_w, feat_map)\nupsampled = np.repeat(np.repeat(logits_map, 2, axis=2), 2, axis=3)\n\nprint('feature map shape :', feat_map.shape)\nprint('logits map shape  :', logits_map.shape)\nprint('upsampled shape   :', upsampled.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "このノートで押さえるべき点は次の通りです。\n\n- 畳み込みは「局所パターン検出」をパラメータ共有で行う\n- stride/padding/pooling は解像度と情報量の調整器\n- CNNは特徴抽出器 + 分類器として分解して考えると理解しやすい\n- FCNは分類器部分を空間出力に置き換えた拡張\n\n次は損失関数・最適化・正則化の観点から、CNN学習をより安定化する方法を扱います。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
