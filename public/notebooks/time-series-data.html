<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>time-series-data</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="時系列データの扱い">時系列データの扱い</h1>
<p>時系列データは、表の1行1行が独立ではなく、時間順でつながっています。<br>
この順序を無視して分割や特徴量設計をすると、評価だけ高くて本番で崩れるモデルになりやすくなります。</p>
<p>このノートでは、売上の1ステップ先予測を題材に、観察、特徴量化、リーク回避、評価、将来予測までを一気通貫で確認します。</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.base import clone
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler</code></pre>
<p>まず、上昇トレンド、季節性、キャンペーン効果を持つ疑似データを作ります。<br>
実データがなくても、構造を入れた疑似データを作って小さく検証すると、何が効くのかを読み取りやすくなります。</p>

<pre><code class="language-python">plt.style.use(&quot;seaborn-v0_8-whitegrid&quot;)
rng = np.random.default_rng(7)

dates = pd.date_range(&quot;2018-01-01&quot;, periods=96, freq=&quot;MS&quot;)
trend = np.linspace(0, 95, len(dates))
seasonal = 18 * np.sin(2 * np.pi * np.arange(len(dates)) / 12)
promo = (np.arange(len(dates)) % 6 == 0).astype(int)
noise = rng.normal(loc=0.0, scale=4.5, size=len(dates))

sales = 220 + trend + seasonal + 16 * promo + noise

df = pd.DataFrame({
    &quot;date&quot;: dates,
    &quot;sales&quot;: sales,
    &quot;promo&quot;: promo,
})

df.head()</code></pre>
<pre><code class="language-python">fig, ax = plt.subplots(figsize=(11, 4.4))
ax.plot(df[&quot;date&quot;], df[&quot;sales&quot;], color=&quot;#2b6cb0&quot;, linewidth=2, label=&quot;sales&quot;)
ax.scatter(
    df.loc[df[&quot;promo&quot;] == 1, &quot;date&quot;],
    df.loc[df[&quot;promo&quot;] == 1, &quot;sales&quot;],
    color=&quot;#c05621&quot;,
    s=28,
    label=&quot;promo month&quot;,
)
ax.plot(df[&quot;date&quot;], df[&quot;sales&quot;].rolling(12).mean(), color=&quot;#4a5568&quot;, linewidth=1.6, label=&quot;12-month moving average&quot;)
ax.set_title(&quot;Monthly Sales&quot;)
ax.set_xlabel(&quot;date&quot;)
ax.set_ylabel(&quot;sales&quot;)
ax.legend(loc=&quot;upper left&quot;)
plt.tight_layout()
plt.show()</code></pre>
<p>時系列予測で最重要なのは、「予測時点で使える情報だけ」で特徴量を作ることです。<br>
ここでは <code>lag_1, lag_2, lag_3, lag_6, lag_12</code> と移動平均を作ります。<br>
<code>lag_12</code> を使うため先頭12か月は欠損となり、<code>dropna()</code> で除外されます。これは正常な挙動です。</p>
<p>また、月を 1〜12 の整数のまま使うと 12月と1月が遠く見えるため、<code>sin/cos</code> で円環的な季節性を表します。<br>
移動平均には <code>shift(1)</code> を入れ、当月値の混入を防いでリークを避けます。</p>

<pre><code class="language-python">def make_time_features(frame: pd.DataFrame, lag_steps=(1, 2, 3, 6, 12), roll_windows=(3, 6)) -&gt; pd.DataFrame:
    out = frame.sort_values(&quot;date&quot;).reset_index(drop=True).copy()

    for lag in lag_steps:
        out[f&quot;lag_{lag}&quot;] = out[&quot;sales&quot;].shift(lag)

    for window in roll_windows:
        out[f&quot;roll_mean_{window}&quot;] = out[&quot;sales&quot;].shift(1).rolling(window=window).mean()

    month = out[&quot;date&quot;].dt.month
    out[&quot;month_sin&quot;] = np.sin(2 * np.pi * month / 12)
    out[&quot;month_cos&quot;] = np.cos(2 * np.pi * month / 12)

    return out.dropna().reset_index(drop=True)

feature_df = make_time_features(df)
feature_df.head()</code></pre>
<pre><code class="language-python">feature_cols = [
    &quot;promo&quot;,
    &quot;month_sin&quot;,
    &quot;month_cos&quot;,
    &quot;lag_1&quot;,
    &quot;lag_2&quot;,
    &quot;lag_3&quot;,
    &quot;lag_6&quot;,
    &quot;lag_12&quot;,
    &quot;roll_mean_3&quot;,
    &quot;roll_mean_6&quot;,
]

print(&quot;raw period :&quot;, df[&quot;date&quot;].min().date(), &quot;to&quot;, df[&quot;date&quot;].max().date(), &quot;rows:&quot;, len(df))
print(&quot;feat period:&quot;, feature_df[&quot;date&quot;].min().date(), &quot;to&quot;, feature_df[&quot;date&quot;].max().date(), &quot;rows:&quot;, len(feature_df))
print(&quot;features:&quot;, feature_cols)</code></pre>
<p>自己相関を見ると、「何か月前の値と似た動きをするか」が分かります。<br>
季節周期の候補やラグの当たりを付けるときに役立ちます。</p>

<pre><code class="language-python">def autocorr_at_lag(values: np.ndarray, lag: int) -&gt; float:
    x = values[lag:]
    y = values[:-lag]
    if x.std() == 0 or y.std() == 0:
        return 0.0
    return float(np.corrcoef(x, y)[0, 1])

series = feature_df[&quot;sales&quot;].to_numpy()
lags = np.arange(1, 25)
autocorr_values = np.array([autocorr_at_lag(series, int(lag)) for lag in lags])

fig, ax = plt.subplots(figsize=(10, 3.6))
ax.bar(lags, autocorr_values, color=&quot;#2f855a&quot;)
ax.axhline(0, color=&quot;#1a202c&quot;, linewidth=1)
ax.set_title(&quot;Autocorrelation by lag&quot;)
ax.set_xlabel(&quot;lag&quot;)
ax.set_ylabel(&quot;corr&quot;)
plt.tight_layout()
plt.show()</code></pre>
<p>ここからは、最後の12か月をテストに固定して評価します。<br>
まずは基準として、1か月前の値をそのまま予測に使う単純ベースラインを作ります。</p>

<pre><code class="language-python">test_horizon = 12
train_df = feature_df.iloc[:-test_horizon].copy()
test_df = feature_df.iloc[-test_horizon:].copy()

X_train = train_df[feature_cols]
y_train = train_df[&quot;sales&quot;]
X_test = test_df[feature_cols]
y_test = test_df[&quot;sales&quot;]

naive_pred = test_df[&quot;lag_1&quot;].to_numpy()
naive_mae = mean_absolute_error(y_test, naive_pred)
print(f&quot;naive MAE: {naive_mae:.3f}&quot;)</code></pre>
<p>対照として、やってはいけないランダム分割も試します。<br>
ランダム分割では「後年のデータを学習に使いながら前の年をテストする」状態が起き、本番では使えない未来情報が暗黙に混ざります。</p>

<pre><code class="language-python">X_all = feature_df[feature_cols]
y_all = feature_df[&quot;sales&quot;]

X_tr_rand, X_te_rand, y_tr_rand, y_te_rand = train_test_split(
    X_all, y_all, test_size=0.2, random_state=42, shuffle=True
)

rand_pipeline = Pipeline([
    (&quot;scaler&quot;, StandardScaler()),
    (&quot;ridge&quot;, Ridge(alpha=1.0)),
])
rand_pipeline.fit(X_tr_rand, y_tr_rand)
rand_pred = rand_pipeline.predict(X_te_rand)
rand_mae = mean_absolute_error(y_te_rand, rand_pred)

print(f&quot;random split ridge MAE (invalid for TS): {rand_mae:.3f}&quot;)</code></pre>
<p>次に、時系列順を守った1ステップ先予測（予測のたびに実測で更新する前提）でモデルを比較します。<br>
線形モデルは解釈しやすく、木モデルは非線形性を拾いやすいという違いがあります。</p>

<pre><code class="language-python">ridge_model = Pipeline([
    (&quot;scaler&quot;, StandardScaler()),
    (&quot;ridge&quot;, Ridge(alpha=1.0)),
])

rf_model = RandomForestRegressor(
    n_estimators=350,
    max_depth=8,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1,
)

ridge_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)

ridge_pred = ridge_model.predict(X_test)
rf_pred = rf_model.predict(X_test)

ridge_mae = mean_absolute_error(y_test, ridge_pred)
rf_mae = mean_absolute_error(y_test, rf_pred)

print(f&quot;naive MAE                : {naive_mae:.3f}&quot;)
print(f&quot;chronological ridge MAE  : {ridge_mae:.3f}&quot;)
print(f&quot;chronological rf MAE     : {rf_mae:.3f}&quot;)
print(f&quot;random split ridge MAE   : {rand_mae:.3f} (invalid setup)&quot;)</code></pre>
<pre><code class="language-python">one_step_df = test_df[[&quot;date&quot;, &quot;sales&quot;]].copy()
one_step_df[&quot;naive&quot;] = naive_pred
one_step_df[&quot;ridge&quot;] = ridge_pred
one_step_df[&quot;rf&quot;] = rf_pred

fig, ax = plt.subplots(figsize=(11, 4.4))
ax.plot(one_step_df[&quot;date&quot;], one_step_df[&quot;sales&quot;], marker=&quot;o&quot;, linewidth=2, label=&quot;actual&quot;)
ax.plot(one_step_df[&quot;date&quot;], one_step_df[&quot;naive&quot;], marker=&quot;o&quot;, linewidth=1.5, label=&quot;naive&quot;)
ax.plot(one_step_df[&quot;date&quot;], one_step_df[&quot;ridge&quot;], marker=&quot;o&quot;, linewidth=1.5, label=&quot;ridge&quot;)
ax.plot(one_step_df[&quot;date&quot;], one_step_df[&quot;rf&quot;], marker=&quot;o&quot;, linewidth=1.5, label=&quot;random forest&quot;)
ax.set_title(&quot;One-Step Forecast (Observed Updates)&quot;)
ax.set_xlabel(&quot;date&quot;)
ax.set_ylabel(&quot;sales&quot;)
ax.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>固定起点で12か月先まで当てるには、逐次予測のバックテストが必要です。<br>
ここでは学習期間を固定し、予測値を次月ラグへ入れながら12ステップ先まで進めます。<br>
<code>promo</code> は「6か月ごとに実施される既知の計画値」を使える前提にしています。</p>

<pre><code class="language-python">def predict_next_months(history: pd.DataFrame, model, steps: int = 12) -&gt; pd.DataFrame:
    working = history.sort_values(&quot;date&quot;).reset_index(drop=True).copy()
    preds = []

    for _ in range(steps):
        next_date = (working[&quot;date&quot;].iloc[-1] + pd.offsets.MonthBegin(1)).normalize()
        month_index = len(working)
        next_promo = int(month_index % 6 == 0)

        next_row = {
            &quot;date&quot;: next_date,
            &quot;promo&quot;: next_promo,
            &quot;lag_1&quot;: float(working[&quot;sales&quot;].iloc[-1]),
            &quot;lag_2&quot;: float(working[&quot;sales&quot;].iloc[-2]),
            &quot;lag_3&quot;: float(working[&quot;sales&quot;].iloc[-3]),
            &quot;lag_6&quot;: float(working[&quot;sales&quot;].iloc[-6]),
            &quot;lag_12&quot;: float(working[&quot;sales&quot;].iloc[-12]),
            &quot;roll_mean_3&quot;: float(working[&quot;sales&quot;].iloc[-3:].mean()),
            &quot;roll_mean_6&quot;: float(working[&quot;sales&quot;].iloc[-6:].mean()),
            &quot;month_sin&quot;: float(np.sin(2 * np.pi * next_date.month / 12)),
            &quot;month_cos&quot;: float(np.cos(2 * np.pi * next_date.month / 12)),
        }

        row_df = pd.DataFrame([next_row])
        y_hat = float(model.predict(row_df[feature_cols])[0])

        preds.append({&quot;date&quot;: next_date, &quot;forecast&quot;: y_hat})
        working = pd.concat(
            [working, pd.DataFrame([{&quot;date&quot;: next_date, &quot;sales&quot;: y_hat, &quot;promo&quot;: next_promo}])],
            ignore_index=True,
        )

    return pd.DataFrame(preds)

train_raw = df.iloc[:-test_horizon].copy()
test_raw = df.iloc[-test_horizon:].copy()
train_raw_feat = make_time_features(train_raw)

ridge_recursive = clone(ridge_model)
rf_recursive = clone(rf_model)
ridge_recursive.fit(train_raw_feat[feature_cols], train_raw_feat[&quot;sales&quot;])
rf_recursive.fit(train_raw_feat[feature_cols], train_raw_feat[&quot;sales&quot;])

ridge_backtest = predict_next_months(train_raw, ridge_recursive, steps=test_horizon)
rf_backtest = predict_next_months(train_raw, rf_recursive, steps=test_horizon)

ridge_recursive_mae = mean_absolute_error(test_raw[&quot;sales&quot;], ridge_backtest[&quot;forecast&quot;])
rf_recursive_mae = mean_absolute_error(test_raw[&quot;sales&quot;], rf_backtest[&quot;forecast&quot;])

print(f&quot;recursive ridge MAE (12-step): {ridge_recursive_mae:.3f}&quot;)
print(f&quot;recursive rf MAE (12-step)   : {rf_recursive_mae:.3f}&quot;)</code></pre>
<pre><code class="language-python">recursive_plot = test_raw[[&quot;date&quot;, &quot;sales&quot;]].copy()
recursive_plot[&quot;ridge_recursive&quot;] = ridge_backtest[&quot;forecast&quot;].to_numpy()
recursive_plot[&quot;rf_recursive&quot;] = rf_backtest[&quot;forecast&quot;].to_numpy()

fig, ax = plt.subplots(figsize=(11, 4.4))
ax.plot(recursive_plot[&quot;date&quot;], recursive_plot[&quot;sales&quot;], marker=&quot;o&quot;, linewidth=2, label=&quot;actual&quot;)
ax.plot(recursive_plot[&quot;date&quot;], recursive_plot[&quot;ridge_recursive&quot;], marker=&quot;o&quot;, linewidth=1.5, label=&quot;ridge recursive&quot;)
ax.plot(recursive_plot[&quot;date&quot;], recursive_plot[&quot;rf_recursive&quot;], marker=&quot;o&quot;, linewidth=1.5, label=&quot;rf recursive&quot;)
ax.set_title(&quot;Recursive 12-Step Backtest&quot;)
ax.set_xlabel(&quot;date&quot;)
ax.set_ylabel(&quot;sales&quot;)
ax.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>分割1回だけでは偶然の影響が残るため、<code>TimeSeriesSplit</code> でウォークフォワード評価も確認します。<br>
この評価では常に「過去で学習し未来を検証する」順序が守られます。</p>

<pre><code class="language-python">tscv = TimeSeriesSplit(n_splits=5)
cv_scores = cross_val_score(
    ridge_model,
    X_all,
    y_all,
    cv=tscv,
    scoring=&quot;neg_mean_absolute_error&quot;,
)

cv_mae = -cv_scores
print(&quot;TimeSeriesSplit MAE:&quot;, np.round(cv_mae, 3))
print(&quot;mean:&quot;, round(cv_mae.mean(), 3), &quot;std:&quot;, round(cv_mae.std(), 3))</code></pre>
<p>最後に運用想定として、利用可能な全期間で再学習して未来12か月を予測します。</p>

<pre><code class="language-python">ridge_final = clone(ridge_model)
ridge_final.fit(X_all, y_all)

future_pred = predict_next_months(df, ridge_final, steps=12)
future_pred.head()</code></pre>
<pre><code class="language-python">recent_hist = df.tail(24)
fig, ax = plt.subplots(figsize=(11, 4.4))
ax.plot(recent_hist[&quot;date&quot;], recent_hist[&quot;sales&quot;], marker=&quot;o&quot;, label=&quot;history (last 24 months)&quot;)
ax.plot(future_pred[&quot;date&quot;], future_pred[&quot;forecast&quot;], marker=&quot;o&quot;, label=&quot;future forecast (ridge)&quot;)
ax.set_title(&quot;History and 12-Month Forecast&quot;)
ax.set_xlabel(&quot;date&quot;)
ax.set_ylabel(&quot;sales&quot;)
ax.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>時系列モデリングでは、モデルを複雑にする前に「時間順を壊していないか」を必ず確認してください。<br>
ラグ特徴と移動平均のような基本設計でも、評価設計が正しければ実務で使える強い基準線になります。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>