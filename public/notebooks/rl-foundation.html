<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>rl-foundation</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="強化学習の考え方">強化学習の考え方</h1>
<p>強化学習 セクションの学習ステップ 1/12。<br>
強化学習の考え方では、価値更新を紙ではなくコードで追い、再帰の意味を体感します。</p>
<p>このステップの到達目標: ベルマン方程式の見方を、価値更新のコードとセットで理解し、手法差を説明できる状態にします。<br>
前提: 期待値、再帰的な定義、逐次意思決定の基本が前提です。</p>
<p>今回の中心語: 「状態」、「行動」、「報酬」、「価値関数」、「方策」、「強化学習の考え方」<br>
このセクションの最初のステップです。ここで使う記号と変数の読み方を揃えます。<br>
セクション全体のゴール: ベルマン方程式の見方を、価値更新のコードとセットで理解し、手法差を説明できる状態にします。</p>

<h2 id="検証-1-割引報酬の基礎">検証 1: 割引報酬の基礎</h2>
<p>強化学習の原点として、割引和の計算を基礎例で確認します。</p>

<pre><code class="language-python">rewards = [0, 1, 0, 2]
gamma = 0.90
g = 0.0
for r in reversed(rewards):
    g = r + gamma * g
print(&#39;task = rl-foundation&#39;, &#39;return=&#39;, round(g, 6))</code></pre>
<p>ここでの return 計算が価値関数理解の土台になります。</p>
<p>この節では、状態 が入出力のどこを決めるかを中心に読める状態になれば十分です。</p>

<h2 id="検証-2-ベルマン更新を1回行う">検証 2: ベルマン更新を1回行う</h2>
<p>次に、価値更新を1ステップだけ計算します。1回更新でも、再帰構造の意味は十分に見えてきます。</p>

<pre><code class="language-python">v_next = {&#39;s0&#39;: 0.4, &#39;s1&#39;: 0.8}
reward = {&#39;left&#39;: 0.2, &#39;right&#39;: 1.0}
trans = {&#39;left&#39;: &#39;s0&#39;, &#39;right&#39;: &#39;s1&#39;}
v_s = max(reward[a] + gamma * v_next[trans[a]] for a in [&#39;left&#39;, &#39;right&#39;])
print(&#39;updated V(s)=&#39;, round(v_s, 6))</code></pre>
<p>ベルマン更新は『今の価値』を『次状態の価値』で再定義する操作です。この再帰が強化学習の中心です。</p>
<p>この節では、状態 が入出力のどこを決めるかを中心に読める状態になれば十分です。</p>

<h2 id="定義の確認">定義の確認</h2>
<ol>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>k</mi><mo>≥</mo><mn>0</mn></mrow></msub><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">G_t = \sum_{k\ge 0} \gamma^k R_{t+k+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.244297em;vertical-align:-0.395189em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">G</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mrel">≥</span><span class="mord mathrm">0</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.05556em;">γ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.00773em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>←</mo><mi>Q</mi><mo>+</mo><mi>α</mi><mspace width="0.16667em"></mspace><msub><mi>δ</mi><mrow><mi>T</mi><mi>D</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q \leftarrow Q + \alpha\,\delta_{TD}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">Q</span><span class="mrel">←</span><span class="mord mathit">Q</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="mord mspace thinspace"></span><span class="mord"><span class="mord mathit" style="margin-right:0.03785em;">δ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03785em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span><span class="mord mathit" style="margin-right:0.02778em;">D</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></li>
</ol>

<h2 id="検証-3-q値更新を比較する">検証 3: Q値更新を比較する</h2>
<p>ここで Q学習の更新式をコードに写し、数値の動きを確認します。式を読むだけでは掴みにくい感覚を得る段階です。</p>

<pre><code class="language-python">Q = {(&#39;s0&#39;,&#39;left&#39;): 0.3, (&#39;s0&#39;,&#39;right&#39;): 0.1, (&#39;s1&#39;,&#39;left&#39;): 0.5, (&#39;s1&#39;,&#39;right&#39;): 0.7}
alpha = 0.2
r, s, a, s_next = 1.0, &#39;s0&#39;, &#39;right&#39;, &#39;s1&#39;
td_target = r + gamma * max(Q[(s_next,&#39;left&#39;)], Q[(s_next,&#39;right&#39;)])
Q[(s,a)] += alpha * (td_target - Q[(s,a)])
print(&#39;Q(s0,right)=&#39;, round(Q[(s,a)], 6))</code></pre>
<p>更新後の値が過去の値とどれだけ違うかは、学習率と TD 誤差で決まります。ここが調整ポイントです。</p>
<p>この節では、状態 が入出力のどこを決めるかを中心に読める状態になれば十分です。</p>

<h2 id="検証-4-探索と活用の切り替え">検証 4: 探索と活用の切り替え</h2>
<p>次に、探索率を変えたときの行動選択を見ます。探索不足は局所最適に閉じる典型的な原因です。</p>

<pre><code class="language-python">def choose_action(q_left, q_right, epsilon):
    if epsilon &gt; 0.3:
        return &#39;explore&#39;
    return &#39;left&#39; if q_left &gt;= q_right else &#39;right&#39;
print(choose_action(0.4, 0.7, 0.5), choose_action(0.4, 0.7, 0.1))</code></pre>
<p>探索率は固定せず、学習段階に応じて減衰させるのが一般的です。初期は広く探索し、後半で活用へ寄せます。</p>
<p>この節では、状態 が入出力のどこを決めるかを中心に読める状態になれば十分です。</p>

<h2 id="検証-5-方策評価の簡易チェック">検証 5: 方策評価の簡易チェック</h2>
<p>最後に、方策の平均報酬を簡易的に比較します。アルゴリズムの評価は、更新式だけでなく結果の検証が不可欠です。</p>

<pre><code class="language-python">episode_rewards = [1.2, 0.8, 1.5, 1.1, 1.4]
avg_reward = sum(episode_rewards) / len(episode_rewards)
variance = sum((r - avg_reward) ** 2 for r in episode_rewards) / len(episode_rewards)
print(&#39;avg =&#39;, round(avg_reward, 4))
print(&#39;var =&#39;, round(variance, 4))</code></pre>
<p>平均だけでなく分散を見ると、方策の安定性も評価できます。実運用ではこの二軸が重要です。</p>
<p>この節では、状態 が入出力のどこを決めるかを中心に読める状態になれば十分です。</p>

<h2 id="まとめ">まとめ</h2>
<p>今回のノートで押さえておくべき誤解しやすい点を整理します。</p>
<ol>
<li>探索率が低すぎて行動が固定化する</li>
<li>報酬設計が目的とずれている</li>
<li>長期ロールアウトの不安定性を検証しない</li>
</ol>
<p>次は学習ステップ 2/12「価値関数」へ進み、今回のコードとの差分を確認してください。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>