<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>sklearn-xgboost</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="scikit-learnとxgboostの使い方">scikit-learnとXGBoostの使い方</h1>
<p>scikit-learn は、前処理・学習・評価を統一された API で扱えるライブラリです。<br>
XGBoost は勾配ブースティングを高速かつ高精度に実装した手法で、タブularデータで強力な選択肢になります。</p>
<p>このノートでは、まず scikit-learn の基本フローを固め、その上で XGBoost を同じ問題に適用して比較します。</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix

try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ModuleNotFoundError:
    XGBClassifier = None
    XGBOOST_AVAILABLE = False

sns.set_theme(style=&quot;whitegrid&quot;, context=&quot;notebook&quot;)</code></pre>
<h2 id="1-scikit-learnの基本フロー">1. scikit-learnの基本フロー</h2>
<p>ここでは乳がんデータセットを使って、<br>
<code>データ分割 → パイプライン定義 → 学習 → 評価</code> を一気通貫で実行します。</p>
<p>評価指標は Accuracy と ROC-AUC を使います。<br>
Accuracy は分類の正解率、ROC-AUC は分類しきい値を動かしたときの識別性能（順位づけの上手さ）を見ます。</p>

<pre><code class="language-python">cancer = load_breast_cancer(as_frame=True)
X = cancer.data
y = cancer.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

baseline = Pipeline([
    (&quot;scaler&quot;, StandardScaler()),
    (&quot;model&quot;, LogisticRegression(max_iter=2000, random_state=42)),
])

baseline.fit(X_train, y_train)
pred = baseline.predict(X_test)
proba = baseline.predict_proba(X_test)[:, 1]

print(f&quot;accuracy: {accuracy_score(y_test, pred):.3f}&quot;)
print(f&quot;roc_auc : {roc_auc_score(y_test, proba):.3f}&quot;)</code></pre>
<p>混同行列を確認すると、単純な精度だけでは見えない誤分類の偏りを把握できます。</p>

<pre><code class="language-python">cm = confusion_matrix(y_test, pred)
fig, ax = plt.subplots(figsize=(4.5, 4))
sns.heatmap(cm, annot=True, fmt=&quot;d&quot;, cmap=&quot;Greens&quot;, cbar=False, ax=ax)
ax.set_title(&quot;Logistic Regression Confusion Matrix&quot;)
ax.set_xlabel(&quot;Predicted&quot;)
ax.set_ylabel(&quot;Actual&quot;)
plt.show()</code></pre>
<h2 id="2-交差検証とハイパーパラメータ探索">2. 交差検証とハイパーパラメータ探索</h2>
<p>分割1回の評価だけでは偶然の影響が残るので、交差検証で安定性を見ます。<br>
その後、<code>GridSearchCV</code> で <code>C</code> を探索します。</p>
<p>ここでの流れは次の通りです。</p>
<ul>
<li>まず CV で、モデル設定が全体として安定しているかを確認</li>
<li>次に訓練データ側で GridSearch を実行し、テストデータは最後まで未使用に保つ</li>
</ul>
<p>こうすることで、チューニング結果を過度に楽観視しにくくなります。</p>

<pre><code class="language-python">cv_scores = cross_val_score(
    baseline,
    X,
    y,
    cv=5,
    scoring=&quot;roc_auc&quot;,
)
print(&quot;CV ROC-AUC (5-fold):&quot;, np.round(cv_scores, 4))
print(&quot;mean:&quot;, cv_scores.mean())</code></pre>
<pre><code class="language-python">search = GridSearchCV(
    estimator=Pipeline([
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;model&quot;, LogisticRegression(max_iter=2000, random_state=42)),
    ]),
    param_grid={
        &quot;model__C&quot;: [0.01, 0.1, 1.0, 3.0, 10.0],
    },
    cv=5,
    scoring=&quot;roc_auc&quot;,
    n_jobs=-1,
)
search.fit(X_train, y_train)

best_lr = search.best_estimator_
best_pred = best_lr.predict(X_test)
best_proba = best_lr.predict_proba(X_test)[:, 1]

print(&quot;best params:&quot;, search.best_params_)
print(f&quot;tuned accuracy: {accuracy_score(y_test, best_pred):.3f}&quot;)
print(f&quot;tuned roc_auc : {roc_auc_score(y_test, best_proba):.3f}&quot;)</code></pre>
<h2 id="3-xgboostを同じ問題に適用する">3. XGBoostを同じ問題に適用する</h2>
<p>XGBoost が利用可能な環境では、同じ学習データで性能比較します。<br>
XGBoost は木ベースなので、今回の設定では標準化を必須としません。</p>

<pre><code class="language-python">if XGBOOST_AVAILABLE:
    xgb = XGBClassifier(
        n_estimators=300,
        max_depth=4,
        learning_rate=0.05,
        subsample=0.9,
        colsample_bytree=0.9,
        eval_metric=&quot;logloss&quot;,
        random_state=42,
    )
    xgb.fit(X_train, y_train)

    xgb_pred = xgb.predict(X_test)
    xgb_proba = xgb.predict_proba(X_test)[:, 1]

    print(f&quot;xgb accuracy: {accuracy_score(y_test, xgb_pred):.3f}&quot;)
    print(f&quot;xgb roc_auc : {roc_auc_score(y_test, xgb_proba):.3f}&quot;)
else:
    print(&quot;xgboost がインストールされていないため、このセルはスキップされました。&quot;)</code></pre>
<p>XGBoost を使えた場合は、重要特徴量を見て学習がどこに注目したかを確認します。</p>

<pre><code class="language-python">if XGBOOST_AVAILABLE:
    importances = pd.Series(xgb.feature_importances_, index=X.columns).sort_values(ascending=False).head(12)
    fig, ax = plt.subplots(figsize=(7, 4.5))
    sns.barplot(x=importances.values, y=importances.index, orient=&quot;h&quot;, ax=ax)
    ax.set_title(&quot;Top 12 Feature Importances (XGBoost)&quot;)
    ax.set_xlabel(&quot;importance&quot;)
    ax.set_ylabel(&quot;feature&quot;)
    plt.tight_layout()
    plt.show()
else:
    print(&quot;xgboost 未導入のため、重要特徴量の表示をスキップしました。&quot;)</code></pre>
<h2 id="4-使い分けの実務メモ">4. 使い分けの実務メモ</h2>
<ul>
<li>scikit-learn: 実験の土台を作るのが速く、比較実験を回しやすい</li>
<li>XGBoost: 表形式データで高い精度を狙いやすい</li>
</ul>
<p>まず scikit-learn で前処理と評価設計を固め、<br>
次に XGBoost を含む複数モデルを同一条件で比較する流れが実践的です。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>