<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>supervised-unsupervised-learning</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<h1 id="教師あり学習と教師なし学習">教師あり学習と教師なし学習</h1>
<p>機械学習の入口で最初に分かれるのが「正解ラベルがあるかどうか」です。<br>
教師あり学習は、入力と正解の対応を学んで予測する枠組みです。教師なし学習は、正解がないデータから構造やまとまりを見つけます。</p>
<p>このノートでは、同じような数値データを使って両者を比較し、<br>
どんな問いにどちらを使うべきかを、コードと結果の対応で確認します。</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_classification, make_blobs, load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, silhouette_score

sns.set_theme(style=&quot;whitegrid&quot;, context=&quot;notebook&quot;)</code></pre>
<h2 id="教師あり学習-ラベルありデータで予測する">教師あり学習: ラベルありデータで予測する</h2>
<p>まずは二値分類データを作り、<code>0</code> と <code>1</code> を予測する問題を作ります。<br>
ここでは「ラベルがある」ことが重要で、モデルはこのラベルを目標に学習します。</p>

<pre><code class="language-python">X, y = make_classification(
    n_samples=600,
    n_features=6,
    n_informative=4,
    n_redundant=1,
    class_sep=1.2,
    random_state=42,
)

feature_names = [f&quot;x{i}&quot; for i in range(X.shape[1])]
df_cls = pd.DataFrame(X, columns=feature_names)
df_cls[&quot;label&quot;] = y

df_cls.head()</code></pre>
<p>学習時に訓練データと評価データを分けるのは、汎化性能を確認するためです。<br>
評価データを分けずに性能を見ると、実運用での精度を過大評価しやすくなります。</p>

<pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(
    df_cls[feature_names],
    df_cls[&quot;label&quot;],
    test_size=0.25,
    random_state=42,
    stratify=df_cls[&quot;label&quot;],
)

clf = Pipeline([
    (&quot;scaler&quot;, StandardScaler()),
    (&quot;model&quot;, LogisticRegression(max_iter=1000, random_state=42)),
])
clf.fit(X_train, y_train)

pred = clf.predict(X_test)
acc = accuracy_score(y_test, pred)
print(f&quot;test accuracy: {acc:.3f}&quot;)</code></pre>
<p>分類では、正解率だけでなく混同行列を確認すると、<br>
どのクラスで間違っているかを具体的に把握できます。</p>

<pre><code class="language-python">cm = confusion_matrix(y_test, pred)
fig, ax = plt.subplots(figsize=(4.5, 4))
sns.heatmap(cm, annot=True, fmt=&quot;d&quot;, cmap=&quot;Blues&quot;, cbar=False, ax=ax)
ax.set_title(&quot;Confusion Matrix&quot;)
ax.set_xlabel(&quot;Predicted&quot;)
ax.set_ylabel(&quot;Actual&quot;)
plt.show()

print(classification_report(y_test, pred, digits=3))</code></pre>
<p>標準化の有無で精度がどう変わるかを見ると、<br>
前処理がモデル性能に与える影響を体感できます。</p>

<pre><code class="language-python">clf_no_scale = LogisticRegression(max_iter=1000, random_state=42)
clf_no_scale.fit(X_train, y_train)
pred_no_scale = clf_no_scale.predict(X_test)

acc_no_scale = accuracy_score(y_test, pred_no_scale)
print(f&quot;without scaling: {acc_no_scale:.3f}&quot;)
print(f&quot;with scaling   : {acc:.3f}&quot;)</code></pre>
<h2 id="教師なし学習-ラベルなしデータで構造を見つける">教師なし学習: ラベルなしデータで構造を見つける</h2>
<p>次はクラスタリングです。ここでは学習時に正解ラベルを使いません。<br>
「似た点を同じグループにまとめる」こと自体が目的になります。</p>

<pre><code class="language-python">X_blob, _ = make_blobs(
    n_samples=450,
    centers=3,
    cluster_std=1.2,
    n_features=2,
    random_state=42,
)

fig, ax = plt.subplots(figsize=(5, 4))
ax.scatter(X_blob[:, 0], X_blob[:, 1], s=18, alpha=0.8)
ax.set_title(&quot;Unlabeled Data for Clustering&quot;)
ax.set_xlabel(&quot;feature 1&quot;)
ax.set_ylabel(&quot;feature 2&quot;)
plt.show()</code></pre>
<p>クラスタ数 <code>k</code> は先に与える必要があります。<br>
<code>inertia</code> と <code>silhouette</code> を併せて見ると、極端な <code>k</code> を避けやすくなります。</p>
<ul>
<li>inertia: 各点と所属クラスタ中心の距離二乗和（小さいほどまとまりが良い）</li>
<li>silhouette: クラスタ内の近さとクラスタ間の離れ具合のバランス（-1〜1で高いほど良い）</li>
</ul>
<p>inertia は <code>k</code> を増やすと下がりやすいので、silhouette とセットで判断するのが実践的です。</p>

<pre><code class="language-python">rows = []
for k in range(2, 8):
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(X_blob)
    rows.append({
        &quot;k&quot;: k,
        &quot;inertia&quot;: km.inertia_,
        &quot;silhouette&quot;: silhouette_score(X_blob, labels)
    })

k_report = pd.DataFrame(rows)
k_report</code></pre>
<p>ここでは <code>k=3</code> を選んで可視化します。<br>
教師ありと違い、評価は「正解と一致したか」ではなく、点群のまとまりの良さで判断します。</p>

<pre><code class="language-python">kmeans = KMeans(n_clusters=3, n_init=20, random_state=42)
cluster_id = kmeans.fit_predict(X_blob)
centers = kmeans.cluster_centers_

fig, ax = plt.subplots(figsize=(5.5, 4.2))
scatter = ax.scatter(X_blob[:, 0], X_blob[:, 1], c=cluster_id, cmap=&quot;tab10&quot;, s=22, alpha=0.8)
ax.scatter(centers[:, 0], centers[:, 1], c=&quot;black&quot;, marker=&quot;X&quot;, s=140, label=&quot;centers&quot;)
ax.legend(loc=&quot;best&quot;)
ax.set_title(&quot;KMeans Clustering (k=3)&quot;)
ax.set_xlabel(&quot;feature 1&quot;)
ax.set_ylabel(&quot;feature 2&quot;)
plt.show()</code></pre>
<p>教師なし学習の代表例として次元圧縮も確認します。<br>
PCA は「情報の分散が大きい方向」を優先して、特徴量を少数次元へ圧縮します。</p>
<p>PCA は特徴量スケールに影響されるため、先に標準化してから実行するのが基本です。<br>
出力される <code>explained_variance_ratio_</code> は、各主成分が全体の分散の何割を説明しているかを表します。</p>

<pre><code class="language-python">iris = load_iris(as_frame=True)
X_iris = iris.data
y_iris = iris.target

X_iris_scaled = StandardScaler().fit_transform(X_iris)
pca = PCA(n_components=2, random_state=42)
X_iris_2d = pca.fit_transform(X_iris_scaled)

pca_df = pd.DataFrame(X_iris_2d, columns=[&quot;PC1&quot;, &quot;PC2&quot;])
pca_df[&quot;species&quot;] = y_iris.map({0: &quot;setosa&quot;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;})

fig, ax = plt.subplots(figsize=(6, 4.5))
sns.scatterplot(data=pca_df, x=&quot;PC1&quot;, y=&quot;PC2&quot;, hue=&quot;species&quot;, ax=ax)
ax.set_title(&quot;PCA Projection of Iris&quot;)
plt.show()

print(&quot;explained variance ratio:&quot;, pca.explained_variance_ratio_)</code></pre>
<h2 id="まとめ">まとめ</h2>
<ul>
<li>教師あり学習: ラベルを使って予測器を作る（分類・回帰）</li>
<li>教師なし学習: ラベルなしデータから構造を見つける（クラスタリング・次元圧縮）</li>
</ul>
<p>実務では、教師なしでデータの構造を把握してから、教師ありで予測モデルを構築する流れもよく使われます。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>