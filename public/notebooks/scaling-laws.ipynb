{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# スケーリング則\n\nLLMの事前学習では、モデルサイズ `N`、学習トークン数 `D`、計算資源 `C` を大きくすると、検証損失がべき乗的に下がる傾向があります。\nこのノートでは、実験データからべき乗則を推定し、同一計算資源（isoflops）での最適配分を計算する流れを確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "よく使う近似は次の形です。\n\n`L(N, D) ≈ L_inf + a * N^{-alpha} + b * D^{-beta}`\n\n- `L_inf`: これ以上は下げにくい下限（不可約損失）\n- `alpha`, `beta`: モデル拡大・データ拡大の効き方\n\nまずは `N` と `D` をそれぞれ変えた観測データを作り、`alpha`, `beta` を推定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 観測例（教育用の合成データ）\nN_million = np.array([30, 60, 120, 240, 480, 960], dtype=np.float64)   # million params\nD_billion = np.array([5, 10, 20, 40, 80, 160], dtype=np.float64)        # billion tokens\n\n# 実際の単位へ変換\nN_params = N_million * 1e6\nD_tokens = D_billion * 1e9\n\n# 生成側の真値（未知だと思って推定する）\nL_inf_true = 1.60\nA_true, alpha_true = 3.2, 0.37\nB_true, beta_true = 2.1, 0.29\n\n# 損失生成では見やすさのため M/B 単位でべき乗を作る\nrng = np.random.default_rng(7)\nL_of_N = L_inf_true + A_true * (N_million ** (-alpha_true)) + rng.normal(0, 0.01, size=N_million.shape)\nL_of_D = L_inf_true + B_true * (D_billion ** (-beta_true)) + rng.normal(0, 0.01, size=D_billion.shape)\n\nprint('N sweep losses:', np.round(L_of_N, 4))\nprint('D sweep losses:', np.round(L_of_D, 4))\nprint('unit note: N uses params count, D uses token count in isoflops section')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7.2, 3.6))\nplt.subplot(1, 2, 1)\nplt.plot(N_million, L_of_N, marker='o')\nplt.xscale('log')\nplt.xlabel('N (million params, log)')\nplt.ylabel('validation loss')\nplt.title('Loss vs Model Size')\n\nplt.subplot(1, 2, 2)\nplt.plot(D_billion, L_of_D, marker='o', color='#d77f00')\nplt.xscale('log')\nplt.xlabel('D (billion tokens, log)')\nplt.ylabel('validation loss')\nplt.title('Loss vs Data Size')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`L_inf` が未知なので、候補値を走査しながら\n`log(L - L_inf)` と `log(x)` の一次回帰で指数を推定します。\n\n`L_inf` は観測損失の最小値より少し小さい値にしかなりえないので、\nその近傍を候補として探索します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_power_with_fixed_floor(x, y, floor):\n    # y ~= floor + A * x^{-exponent}\n    z = y - floor\n    if np.any(z <= 0):\n        return None\n\n    lx = np.log(x)\n    lz = np.log(z)\n    # lz = c + m*lx, where m=-exponent\n    m, c = np.polyfit(lx, lz, 1)\n    pred = floor + np.exp(c) * (x ** m)\n    mse = float(np.mean((pred - y) ** 2))\n    return {\n        'floor': float(floor),\n        'A': float(np.exp(c)),\n        'exponent': float(-m),\n        'mse': mse,\n        'pred': pred,\n    }\n\n\nmin_obs = float(min(np.min(L_of_N), np.min(L_of_D)))\nfloor_candidates = np.linspace(min_obs - 0.25, min_obs - 1e-4, 400)\n\nbest = None\nfor floor in floor_candidates:\n    fN = fit_power_with_fixed_floor(N_million, L_of_N, floor)\n    fD = fit_power_with_fixed_floor(D_billion, L_of_D, floor)\n    if fN is None or fD is None:\n        continue\n    total_mse = fN['mse'] + fD['mse']\n    if best is None or total_mse < best['total_mse']:\n        best = {\n            'L_inf': float(floor),\n            'N_fit': fN,\n            'D_fit': fD,\n            'total_mse': float(total_mse),\n        }\n\nfit_joint = best\nprint('Shared-floor fit summary:')\nprint('L_inf =', round(fit_joint['L_inf'], 5), 'total_mse =', round(fit_joint['total_mse'], 7))\nprint('N_fit =', {k: round(v, 5) for k, v in fit_joint['N_fit'].items() if k != 'pred'})\nprint('D_fit =', {k: round(v, 5) for k, v in fit_joint['D_fit'].items() if k != 'pred'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7.2, 3.5))\nplt.subplot(1, 2, 1)\nplt.scatter(N_million, L_of_N, label='observed')\nplt.plot(N_million, fit_joint['N_fit']['pred'], label='power fit', color='#cc3344')\nplt.xscale('log')\nplt.xlabel('N (M params)')\nplt.ylabel('loss')\nplt.title(f\"alpha≈{fit_joint['N_fit']['exponent']:.3f}\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(D_billion, L_of_D, label='observed')\nplt.plot(D_billion, fit_joint['D_fit']['pred'], label='power fit', color='#cc3344')\nplt.xscale('log')\nplt.xlabel('D (B tokens)')\nplt.ylabel('loss')\nplt.title(f\"beta≈{fit_joint['D_fit']['exponent']:.3f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に isoflops を考えます。\nデコーダ型学習の粗い近似として `C ≈ 6ND`（`N`: パラメータ数, `D`: 学習トークン数）を使います。\n\n`D = C/(6N)` を `L(N, D)` に代入すると\n`f(N) = aN^{-alpha} + b(C/6)^{-beta}N^{beta}` になり、\nこれを `df/dN = 0` で解くと `N*` と `D*` が得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 共有L_infで推定した係数を使って、L(N,D)=L_inf+aN^-alpha+bD^-beta を最適化\nL_inf = fit_joint['L_inf']\na_fit, alpha = fit_joint['N_fit']['A'], fit_joint['N_fit']['exponent']\nb_fit, beta = fit_joint['D_fit']['A'], fit_joint['D_fit']['exponent']\n\n# a_fit,b_fit は N(M params), D(B tokens) の単位で推定されているので\n# isoflops (N: params, D: tokens) へ合わせて係数を変換する\n# (N/1e6)^-alpha = (1e6^alpha) * N^-alpha\n# (D/1e9)^-beta  = (1e9^beta)  * D^-beta\na_raw = a_fit * (1e6 ** alpha)\nb_raw = b_fit * (1e9 ** beta)\n\n\ndef optimal_N_D_for_compute(C, a, alpha, b, beta):\n    # C = 6ND -> D = C/(6N)\n    # minimize f(N)=aN^-alpha + b(C/6)^-beta N^beta\n    numer = a * alpha\n    denom = b * beta\n    N_star = (numer / denom) ** (1.0 / (alpha + beta)) * (C / 6.0) ** (beta / (alpha + beta))\n    D_star = C / (6.0 * N_star)\n    return N_star, D_star\n\n\n# C の単位は FLOPs 相当の抽象値（ここでは比較目的）\nC_values = np.logspace(18, 22, 9)\nN_star = []\nD_star = []\nfor C in C_values:\n    n, d = optimal_N_D_for_compute(C, a_raw, alpha, b_raw, beta)\n    N_star.append(n)\n    D_star.append(d)\nN_star = np.array(N_star)\nD_star = np.array(D_star)\n\nprint('first 4 optimal pairs (N*, D*):')\nfor i in range(4):\n    print(f\"C={C_values[i]:.1e} -> N*={N_star[i]/1e6:.2f}M params, D*={D_star[i]/1e9:.2f}B tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7.2, 3.6))\nplt.plot(C_values, N_star / 1e6, marker='o', label='optimal N* (M params)')\nplt.plot(C_values, D_star / 1e9, marker='s', label='optimal D* (B tokens)')\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('Compute C (log)')\nplt.ylabel('Optimal scale (log)')\nplt.title('Isoflops-optimal allocation')\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "実務でありがちな失敗は、計算資源が同じなのに\n- モデルだけ大きくしてデータ不足（undertrained）\n- データだけ増やしてモデル不足（underparameterized）\n\nになることです。下で同一 `C` に対する損失差を比較します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def approx_loss(N, D, L_inf, a, alpha, b, beta):\n    return L_inf + a * (N ** (-alpha)) + b * (D ** (-beta))\n\n\nratios = [0.5, 1.0, 2.0]  # Nを最適比の何倍にするか\nexample_C = 1e20\nn_opt, d_opt = optimal_N_D_for_compute(example_C, a_raw, alpha, b_raw, beta)\n\nprint(f'compute C={example_C:.1e}')\nprint(f'optimal N={n_opt/1e6:.2f}M params, D={d_opt/1e9:.2f}B tokens')\n\nfor r in ratios:\n    n = n_opt * r\n    d = example_C / (6.0 * n)\n    L = approx_loss(n, d, L_inf, a_raw, alpha, b_raw, beta)\n    label = 'optimal' if abs(r - 1.0) < 1e-9 else f'N x {r}'\n    print(f'{label:8s}: N={n/1e6:8.2f}M, D={d/1e9:8.2f}B, approx loss={L:.5f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "スケーリング則は万能ではありません。\nデータ品質、ドメインミスマッチ、最適化設定、アーキテクチャ変更で指数や下限は変わります。\nそれでも、実験計画の初期段階で「どこに計算資源を使うか」を決める強力な指針になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 価格の粗い見積もり（仮定値）\ntrain_flops = 3.0e22\nhardware_tflops = 250.0   # 1 GPUあたり\nnum_gpus = 64\nutilization = 0.35\n\nseconds = train_flops / (hardware_tflops * 1e12 * num_gpus * utilization)\nhours = seconds / 3600\n\nprint('Estimated wall-clock hours:', round(hours, 2))\n\nusd_per_gpu_hour = 1.8\ncost = hours * num_gpus * usd_per_gpu_hour\nprint('Estimated training cost (USD, rough):', round(cost, 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "このノートで押さえたい実務ポイント:\n\n1. まず小規模スイープで `alpha, beta, L_inf` を推定する\n2. その推定に基づき isoflops で `N` と `D` を配分する\n3. 本番では品質劣化要因（データ品質・最適化不安定）を別監視する\n\nこの3段階を回すと、計算予算の無駄打ちを減らしやすくなります。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
