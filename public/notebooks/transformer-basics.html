<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>transformer-basics</title>
  <link rel="stylesheet" href="/highlight/atom-one-dark.min.css" />
  <link rel="stylesheet" href="/katex/katex.min.css" />
  <style>
    :root {
      --bg-0: #f3f8fb;
      --bg-1: #d7e8f4;
      --bg-2: #f9f1e7;
      --text: #09162b;
      --muted: #44556f;
      --panel: rgba(255,255,255,.72);
      --border: rgba(255,255,255,.62);
      --code-bg: #09131a;
      --code-text: #e6f0f5;
      --shadow: 0 24px 54px rgba(10, 26, 54, 0.18), inset 0 1px 0 rgba(255,255,255,.62);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg-0: #071225;
        --bg-1: #0f2238;
        --bg-2: #1a2f44;
        --text: #ebf3ff;
        --muted: #9db3cf;
        --panel: rgba(12, 21, 40, 0.74);
        --border: rgba(145, 183, 227, 0.33);
        --code-bg: #040b17;
        --code-text: #e4efff;
        --shadow: 0 30px 66px rgba(2, 7, 16, 0.58), inset 0 1px 0 rgba(166,205,255,.16);
      }
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      min-height: 100vh;
      padding: 2rem 1rem;
      color: var(--text);
      font-family: "IBM Plex Sans", system-ui, sans-serif;
      background:
        radial-gradient(circle at 12% 12%, rgba(87,196,223,.18), transparent 44%),
        radial-gradient(circle at 88% 5%, rgba(255, 155, 96, 0.16), transparent 40%),
        radial-gradient(circle at 80% 80%, rgba(109, 196, 255, 0.2), transparent 45%),
        linear-gradient(155deg, var(--bg-0) 0%, var(--bg-1) 48%, var(--bg-2) 100%);
    }
    main {
      max-width: 980px;
      margin: 0 auto;
      border-radius: 24px;
      border: 1px solid var(--border);
      background: var(--panel);
      backdrop-filter: blur(20px) saturate(145%);
      -webkit-backdrop-filter: blur(20px) saturate(145%);
      box-shadow: var(--shadow);
      padding: 1.25rem 1.25rem 1.5rem;
    }
    .prose-noema h1, .prose-noema h2, .prose-noema h3 {
      line-height: 1.25;
      margin-top: 1.25rem;
      margin-bottom: .65rem;
    }
    .prose-noema h1 { margin-top: .1rem; font-size: 1.8rem; }
    .prose-noema h2 { font-size: 1.35rem; }
    .prose-noema p {
      line-height: 1.85;
      color: var(--text);
      margin: .7rem 0;
    }
    .prose-noema ul, .prose-noema ol {
      margin: .7rem 0;
      padding-left: 1.4rem;
    }
    .prose-noema ul { list-style: disc; }
    .prose-noema ol { list-style: decimal; }
    .prose-noema li { margin: .28rem 0; line-height: 1.72; }
    .prose-noema a { color: inherit; text-underline-offset: 2px; }
    .prose-noema pre {
      background: var(--code-bg);
      color: var(--code-text);
      border-radius: 12px;
      padding: 1rem;
      overflow: auto;
      border: 1px solid rgba(255,255,255,.12);
    }
    .prose-noema code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }
    .prose-noema img {
      max-width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <main>
<article class="prose-noema">
<pre><code class="language-python">import math
import numpy as np
import matplotlib.pyplot as plt

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ModuleNotFoundError:
    torch = None
    nn = None
    optim = None
    TORCH_AVAILABLE = False</code></pre>
<h1 id="transformer-gpt-vit-mae">Transformer（GPT / ViT / MAE）</h1>
<p>Transformerの中心は自己注意です。系列中の各要素が、ほかの要素をどれくらい参照すべきかを重みとして計算し、文脈に応じた表現を作ります。<br>
このノートでは、まず自己注意の式を最小コードで確認し、次にGPTの因果マスク、ViTのパッチ化、MAEのマスク再構成へ進みます。最後にVAEとの違いも整理します。</p>

<p>自己注意では、入力埋め込み <code>X</code> から <code>Q, K, V</code> を作り、次で重みを計算します。</p>
<p><code>Attention(Q, K, V) = softmax((QK^T) / sqrt(d_k)) V</code></p>
<p>ここで <code>X</code> の形状を <code>(T, d_model)</code>、射影行列 <code>W_Q, W_K, W_V</code> を <code>(d_model, d_k)</code> とすると、<br>
<code>Q, K, V</code> の形状は <code>(T, d_k)</code> になります。<br>
要素で見ると <code>score_{i,j} = (q_i ・ k_j) / sqrt(d_k)</code> で、行ごとに <code>softmax</code> して <code>V</code> を重み付き和します。</p>
<p><code>Q</code> は「何を探しているか」、<code>K</code> は「何を持っているか」、<code>V</code> は「実際に受け渡す情報」と見ると理解しやすくなります。</p>

<pre><code class="language-python">def softmax_rowwise(x):
    x = x - np.max(x, axis=-1, keepdims=True)
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)


# 4トークン, 埋め込み次元3の玩具例
X = np.array([
    [1.0, 0.2, 0.0],   # token 0
    [0.9, 0.1, 0.1],   # token 1
    [0.1, 0.8, 0.2],   # token 2
    [0.0, 0.7, 1.0],   # token 3
], dtype=np.float64)

W_Q = np.array([[0.8, 0.0, 0.1], [0.2, 0.9, 0.1], [0.1, 0.2, 0.7]])
W_K = np.array([[0.7, 0.1, 0.1], [0.1, 0.8, 0.2], [0.2, 0.1, 0.6]])
W_V = np.array([[1.0, 0.2, 0.0], [0.1, 0.9, 0.1], [0.0, 0.2, 0.8]])

Q = X @ W_Q
K = X @ W_K
V = X @ W_V

scores = (Q @ K.T) / math.sqrt(Q.shape[-1])
weights = softmax_rowwise(scores)
context = weights @ V

print(&#39;attention weights:&#39;)
print(np.round(weights, 4))
print(&#39;\ncontext vectors:&#39;)
print(np.round(context, 4))</code></pre>
<pre><code class="language-python">plt.figure(figsize=(5.0, 4.2))
plt.imshow(weights, cmap=&#39;Blues&#39;)
plt.colorbar(label=&#39;attention weight&#39;)
plt.xticks(range(len(X)), [f&#39;k{j}&#39; for j in range(len(X))])
plt.yticks(range(len(X)), [f&#39;q{i}&#39; for i in range(len(X))])
plt.title(&#39;Self-Attention Weight Matrix&#39;)
plt.tight_layout()
plt.show()</code></pre>
<p>GPTのような自己回帰モデルでは、未来トークンを見ないように因果マスク（causal mask）を入れます。<br>
<code>j &gt; i</code>（未来位置）をマスクし、スコアを <code>-∞</code> 相当に落として <code>softmax</code> 後の重みをほぼ0にします。</p>
<p>下のコードでは、同じ入力で「マスクなし」と「マスクあり」を比較します。</p>

<pre><code class="language-python">def causal_masked_attention(Q, K, V):
    T, d = Q.shape
    scores = (Q @ K.T) / math.sqrt(d)
    mask = np.triu(np.ones((T, T), dtype=bool), k=1)  # future positions
    scores_masked = scores.copy()
    scores_masked[mask] = -np.inf
    w = softmax_rowwise(scores_masked)
    return w @ V, w


context_full = softmax_rowwise((Q @ K.T) / math.sqrt(Q.shape[-1])) @ V
context_causal, w_causal = causal_masked_attention(Q, K, V)

print(&#39;full attention (token 1):&#39;, np.round(context_full[1], 4))
print(&#39;causal attention (token 1):&#39;, np.round(context_causal[1], 4))
print(&#39;\ncausal weight matrix:&#39;)
print(np.round(w_causal, 4))</code></pre>
<p>位置情報がないと、Transformerは順序を区別できません。<br>
よく使われる方法のひとつが正弦波位置エンコーディングです。</p>

<pre><code class="language-python">def sinusoidal_positional_encoding(seq_len, d_model):
    pe = np.zeros((seq_len, d_model), dtype=np.float64)
    pos = np.arange(seq_len)[:, None]
    i = np.arange(d_model)[None, :]
    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / d_model)
    angles = pos * angle_rates

    pe[:, 0::2] = np.sin(angles[:, 0::2])
    pe[:, 1::2] = np.cos(angles[:, 1::2])
    return pe


pe = sinusoidal_positional_encoding(seq_len=24, d_model=16)
print(&#39;positional encoding shape:&#39;, pe.shape)

plt.figure(figsize=(7.2, 3.6))
plt.imshow(pe.T, aspect=&#39;auto&#39;, cmap=&#39;coolwarm&#39;)
plt.colorbar(label=&#39;value&#39;)
plt.xlabel(&#39;position&#39;)
plt.ylabel(&#39;channel&#39;)
plt.title(&#39;Sinusoidal Positional Encoding&#39;)
plt.tight_layout()
plt.show()</code></pre>
<p>ViT（Vision Transformer）は画像を小さなパッチ列に分割し、各パッチをトークンとして扱います。<br>
以下では 8x8 の画像を 2x2 パッチへ分割し、パッチ埋め込みを作る最小例を示します。</p>
<p>実際のViTでは、パッチトークンに位置埋め込みを必ず加えます。</p>

<pre><code class="language-python">def patchify(image, patch_size=2):
    h, w = image.shape
    assert h % patch_size == 0 and w % patch_size == 0
    patches = []
    for y in range(0, h, patch_size):
        for x in range(0, w, patch_size):
            p = image[y:y+patch_size, x:x+patch_size].reshape(-1)
            patches.append(p)
    return np.stack(patches, axis=0)


img = np.arange(64, dtype=np.float64).reshape(8, 8) / 63.0
patches = patchify(img, patch_size=2)  # (16, 4)

W_patch = np.random.default_rng(0).normal(0, 0.4, size=(4, 6))
patch_tokens = patches @ W_patch  # (16, 6)

# 実際のViTは学習可能な位置埋め込みを加える
pos_embed = np.random.default_rng(1).normal(0, 0.1, size=patch_tokens.shape)
patch_tokens = patch_tokens + pos_embed

# CLSトークンも通常は学習可能ベクトル（ここでは最小例として0初期化）
cls_token = np.zeros((1, 6), dtype=np.float64)
vit_tokens = np.concatenate([cls_token, patch_tokens], axis=0)  # (17, 6)

print(&#39;image shape      :&#39;, img.shape)
print(&#39;patches shape    :&#39;, patches.shape)
print(&#39;patch token shape:&#39;, patch_tokens.shape)
print(&#39;ViT token shape  :&#39;, vit_tokens.shape, &#39;(CLS + patches)&#39;)

plt.figure(figsize=(3.8, 3.8))
plt.imshow(img, cmap=&#39;gray&#39;)
plt.title(&#39;Toy image (8x8)&#39;)
plt.axis(&#39;off&#39;)
plt.show()</code></pre>
<p>MAE（Masked Autoencoder）は、パッチの一部を隠して復元する自己教師あり学習です。<br>
ViTと違い、学習時には可視パッチだけをエンコーダに入れるため、計算効率を上げやすい設計です。</p>
<p>完全な流れは<br>
<code>visible patches -&gt; encoder -&gt; (mask tokenで長さ復元) -&gt; decoder -&gt; masked patchesの再構成誤差</code><br>
です。下のセルはこの流れを最小化して追うデモです。</p>

<pre><code class="language-python">def make_random_mask(n_tokens, mask_ratio=0.75, seed=0):
    rng = np.random.default_rng(seed)
    n_mask = int(n_tokens * mask_ratio)
    perm = rng.permutation(n_tokens)
    mask_idx = perm[:n_mask]
    keep_idx = np.sort(perm[n_mask:])
    return keep_idx, np.sort(mask_idx)


n_patches = patches.shape[0]
keep_idx, mask_idx = make_random_mask(n_patches, mask_ratio=0.75, seed=1)
kept_tokens = patch_tokens[keep_idx]

print(&#39;all patches :&#39;, n_patches)
print(&#39;kept patches:&#39;, len(keep_idx), &#39;indices=&#39;, keep_idx)
print(&#39;masked      :&#39;, len(mask_idx), &#39;indices=&#39;, mask_idx)
print(&#39;encoder input token shape (without CLS):&#39;, kept_tokens.shape)

# デコーダ入力側で元の長さに戻す（masked位置にはmask tokenを置く）
mask_token = np.zeros((len(mask_idx), patch_tokens.shape[1]))
decoder_input = np.zeros_like(patch_tokens)
decoder_input[keep_idx] = kept_tokens
decoder_input[mask_idx] = mask_token

# 最小デモ: 線形デコーダでパッチを復元し、masked部分のみ誤差を計算
W_rec = np.random.default_rng(2).normal(0, 0.3, size=(patch_tokens.shape[1], patches.shape[1]))
recon_patches = decoder_input @ W_rec
masked_recon_mse = np.mean((recon_patches[mask_idx] - patches[mask_idx]) ** 2)
print(&#39;masked reconstruction MSE (toy):&#39;, round(float(masked_recon_mse), 6))

masked_view = img.copy()
patch_size = 2
for idx in mask_idx:
    gy = idx // (img.shape[1] // patch_size)
    gx = idx % (img.shape[1] // patch_size)
    y0, x0 = gy * patch_size, gx * patch_size
    masked_view[y0:y0+patch_size, x0:x0+patch_size] = 0.0

fig, axes = plt.subplots(1, 2, figsize=(7.2, 3.4))
axes[0].imshow(img, cmap=&#39;gray&#39;, vmin=0, vmax=1)
axes[0].set_title(&#39;original&#39;)
axes[0].axis(&#39;off&#39;)
axes[1].imshow(masked_view, cmap=&#39;gray&#39;, vmin=0, vmax=1)
axes[1].set_title(&#39;MAE masking (75%)&#39;)
axes[1].axis(&#39;off&#39;)
plt.tight_layout()
plt.show()</code></pre>
<p>VAEも再構成を使いますが、MAEと目的が異なります。<br>
VAEは潜在変数 <code>z</code> の確率分布を学ぶ生成モデルで、<br>
損失は <code>再構成誤差 + β * KL</code> です。</p>
<p>一方MAEは、主に表現学習を目的に「隠したパッチの再構成」を解きます。<br>
下のセルはVAE損失のうち、KL項と再構成項の最小計算デモです。</p>

<pre><code class="language-python"># VAEの再パラメータ化トリックと損失項の最小計算
mu = np.array([0.2, -0.4, 0.1], dtype=np.float64)
logvar = np.array([-0.2, 0.3, -0.5], dtype=np.float64)

rng = np.random.default_rng(42)
eps = rng.normal(0.0, 1.0, size=mu.shape)
std = np.exp(0.5 * logvar)
z = mu + std * eps

# KL項
kl = -0.5 * np.sum(1 + logvar - mu**2 - np.exp(logvar))

# 再構成項（ここではMSEの玩具例）
x_true = np.array([0.9, 0.2, 0.7], dtype=np.float64)
x_recon = np.array([0.8, 0.3, 0.6], dtype=np.float64)
recon_loss = np.mean((x_true - x_recon) ** 2)

beta = 1.0
vae_loss = recon_loss + beta * kl

print(&#39;mu    =&#39;, np.round(mu, 4))
print(&#39;logvar=&#39;, np.round(logvar, 4))
print(&#39;z sample =&#39;, np.round(z, 4))
print(&#39;recon loss =&#39;, round(float(recon_loss), 6))
print(&#39;KL(q(z|x) || p(z)) =&#39;, round(float(kl), 6))
print(&#39;VAE loss = recon + beta*KL =&#39;, round(float(vae_loss), 6))</code></pre>
<p>最後に、PyTorchで小さなDecoder-only Transformer（GPT型）を学習し、<br>
「次トークン予測」が実際に動くことを確認します。</p>
<p>ここでは次トークンが直前2トークンに依存する列を使い、文脈参照が必要な設定にしています。<br>
実装は簡略化のため <code>TransformerEncoderLayer + 因果マスク</code> で、GPT型の挙動を再現します。</p>

<pre><code class="language-python">if TORCH_AVAILABLE:
    torch.manual_seed(0)

    vocab_size = 20
    seq_len = 14
    d_model = 48

    def sample_batch(batch_size=64):
        # Fibonacci-like modulo sequence: x_t = (x_{t-1} + x_{t-2}) mod vocab
        seq = torch.zeros(batch_size, seq_len + 1, dtype=torch.long)
        seq[:, 0] = torch.randint(0, vocab_size, (batch_size,))
        seq[:, 1] = torch.randint(0, vocab_size, (batch_size,))
        for t in range(2, seq_len + 1):
            seq[:, t] = (seq[:, t - 1] + seq[:, t - 2]) % vocab_size
        return seq[:, :-1], seq[:, 1:]

    class TinyGPT(nn.Module):
        def __init__(self, vocab_size, d_model=48, nhead=4, num_layers=2, seq_len=14):
            super().__init__()
            self.seq_len = seq_len
            self.token_emb = nn.Embedding(vocab_size, d_model)
            self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))
            layer = nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=4 * d_model,
                dropout=0.0,
                batch_first=True,
                activation=&#39;gelu&#39;,
            )
            self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)
            self.norm = nn.LayerNorm(d_model)
            self.head = nn.Linear(d_model, vocab_size)

        def forward(self, x):
            bsz, t = x.shape
            h = self.token_emb(x) + self.pos_emb[:, :t, :]
            mask = torch.triu(torch.ones(t, t, device=x.device), diagonal=1).bool()
            h = self.encoder(h, mask=mask)
            h = self.norm(h)
            return self.head(h)

    model = TinyGPT(vocab_size=vocab_size, d_model=d_model, nhead=4, num_layers=2, seq_len=seq_len)
    optimizer = optim.AdamW(model.parameters(), lr=3e-3)
    criterion = nn.CrossEntropyLoss()

    for step in range(180):
        x, y = sample_batch(batch_size=64)
        logits = model(x)

        # t=0 の予測は初期2トークン中1つ目だけでは不確定なので除外
        loss = criterion(logits[:, 1:, :].reshape(-1, vocab_size), y[:, 1:].reshape(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % 45 == 0:
            print(f&#39;step={step:&gt;3d}, loss={loss.item():.4f}&#39;)

    model.eval()
    seed = torch.tensor([[3, 7]], dtype=torch.long)
    generated = seed.clone()
    for _ in range(10):
        cur = generated[:, -seq_len:]
        logits = model(cur)
        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
        generated = torch.cat([generated, next_token], dim=1)

    print(&#39;generated tokens:&#39;, generated.squeeze(0).tolist())
else:
    print(&#39;PyTorch未導入のためGPTミニ実験セルはスキップしました。&#39;)</code></pre>
<p>Transformerを使うときは、「どのトークン集合を作るか」が設計の中心になります。<br>
文章なら単語列、画像ならパッチ列、MAEなら可視パッチ列です。<br>
同じ自己注意の枠組みでも、トークン化と目的関数を変えることでGPT/ViT/MAEのように振る舞いが変わります。</p>

</article>
  </main>
  <script src="/highlight/highlight.min.js"></script>
  <script>
    (function () {
      if (!window.hljs) return;
      document.querySelectorAll("pre code").forEach(function (block) {
        window.hljs.highlightElement(block);
      });
    })();
  </script>
</body>
</html>