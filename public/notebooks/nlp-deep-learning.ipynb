{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\nimport re\nfrom collections import Counter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    TORCH_AVAILABLE = True\nexcept ModuleNotFoundError:\n    torch = None\n    nn = None\n    optim = None\n    TORCH_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 自然言語処理（NLP）\n\n自然言語処理では、まず文字列を数値列に変換し、その数値列から意味や文脈を学習します。\nこのノートでは、トークン化と語彙作成から始めて、次トークン予測、SFT（Supervised Fine-Tuning）用データ整形、LoRA/QLoRAの計算感覚までを順に確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "最初の関門はトークン化です。\n同じ文でも、単語単位で切るか、文字単位で切るかで系列長や未知語の扱いが変わります。\n\n日本語は空白で単語境界が明示されないため、`split(' ')` は失敗しやすい方法です。\nここでは、まず失敗例として空白分割を見たあと、文字単位分割と比較します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_texts = [\n    'LLMは文脈に応じて次の単語を予測する。',\n    'SFTでは指示と回答のペアを教師信号にする。',\n    'モデル評価では正答率だけでなく出力品質も見る。',\n    '未知語が多いと語彙外トークンが増えて性能が落ちやすい。',\n    '日本語でも英語でもトークン化の設計は重要。',\n]\n\n\ndef normalize_text(s):\n    s = s.lower()\n    s = re.sub(r'[。､，,.!?！？]', ' ', s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\n\ndef whitespace_tokenize(s):\n    return normalize_text(s).split(' ')\n\n\ndef char_tokenize(s):\n    s = normalize_text(s)\n    s = s.replace(' ', '')\n    return list(s)\n\n\nws_tokenized = [whitespace_tokenize(t) for t in raw_texts]\nchar_tokenized = [char_tokenize(t) for t in raw_texts]\n\nfor i in range(len(raw_texts)):\n    print(f'--- sample {i} ---')\n    print('whitespace:', ws_tokenized[i])\n    print('char      :', char_tokenized[i][:20], '...')\n\nws_lengths = np.array([len(t) for t in ws_tokenized], dtype=np.int64)\nchar_lengths = np.array([len(t) for t in char_tokenized], dtype=np.int64)\n\nprint('\\nmean length (whitespace) =', float(ws_lengths.mean()))\nprint('mean length (char)       =', float(char_lengths.mean()))\n\n# 以降は外部トークナイザ依存を避けるため、文字単位を使用\ntokenized = char_tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.arange(len(raw_texts))\nwidth = 0.36\n\nplt.figure(figsize=(7.2, 3.6))\nplt.bar(x - width/2, ws_lengths, width=width, label='whitespace split', color='#7aa2ff')\nplt.bar(x + width/2, char_lengths, width=width, label='char split', color='#8dd3a7')\nplt.xticks(x, [f's{i}' for i in range(len(raw_texts))])\nplt.ylabel('token count')\nplt.title('Token length by tokenization strategy')\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に語彙（vocabulary）を作って、トークンをIDへ変換します。\n`<pad>` と `<unk>` を先頭に置くのは実務でもよくある設計です。\n\nこのノートでは文字単位トークンを使っているので、未知語問題は「未知文字」の形で現れます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = Counter(tok for toks in tokenized for tok in toks)\nspecial_tokens = ['<pad>', '<unk>']\nbase_vocab = [tok for tok, _ in counter.most_common()]\nvocab = special_tokens + base_vocab\nstoi = {tok: i for i, tok in enumerate(vocab)}\nitos = {i: tok for tok, i in stoi.items()}\n\n\ndef encode(tokens):\n    unk = stoi['<unk>']\n    return [stoi.get(tok, unk) for tok in tokens]\n\n\ndef decode(ids):\n    return [itos.get(i, '<unk>') for i in ids]\n\n\nencoded = [encode(toks) for toks in tokenized]\nprint('vocab size =', len(vocab))\nprint('first sample ids =', encoded[0])\nprint('decoded back      =', decode(encoded[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "埋め込み（embedding）は、トークンIDを実数ベクトルへ写像する層です。\n意味の近い語が近いベクトルになる現象は、埋め込みを学習した後に現れます。\n\n下のセルでは、\n1. 学習前（乱数初期化）の類似度\n2. 小さな共起データで簡易学習した後の類似度\nを比較します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n    torch.manual_seed(0)\n\n    mini_sentences = [\n        'モデル 学習 損失 最適化',\n        'モデル 訓練 データ 評価',\n        '文章 単語 文脈 トークン',\n        '文脈 予測 モデル 生成',\n        '訓練 最適化 損失 収束',\n    ]\n\n    word_tokens = [s.split(' ') for s in mini_sentences]\n    w_vocab = sorted(set(tok for toks in word_tokens for tok in toks))\n    w_stoi = {w: i for i, w in enumerate(w_vocab)}\n\n    pairs = []\n    window = 1\n    for toks in word_tokens:\n        ids = [w_stoi[t] for t in toks]\n        for i, center in enumerate(ids):\n            for j in range(max(0, i - window), min(len(ids), i + window + 1)):\n                if i != j:\n                    pairs.append((center, ids[j]))\n\n    emb = nn.Embedding(len(w_vocab), 16)\n    out = nn.Linear(16, len(w_vocab), bias=False)\n    opt = optim.Adam(list(emb.parameters()) + list(out.parameters()), lr=5e-2)\n    criterion = nn.CrossEntropyLoss()\n\n    x_train = torch.tensor([c for c, _ in pairs], dtype=torch.long)\n    y_train = torch.tensor([ctx for _, ctx in pairs], dtype=torch.long)\n\n    with torch.no_grad():\n        init_vec = emb.weight.clone()\n\n    for _ in range(220):\n        h = emb(x_train)\n        logits = out(h)\n        loss = criterion(logits, y_train)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    def cos(v1, v2):\n        return float(torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2) + 1e-12))\n\n    i_model = w_stoi['モデル']\n    i_train = w_stoi['訓練']\n    init_sim = cos(init_vec[i_model], init_vec[i_train])\n    trained_sim = cos(emb.weight[i_model].detach(), emb.weight[i_train].detach())\n\n    print('vocab:', w_vocab)\n    print('cos(model, train) before learning =', round(init_sim, 4))\n    print('cos(model, train) after learning  =', round(trained_sim, 4))\nelse:\n    rng = np.random.default_rng(0)\n    emb = rng.normal(0, 0.4, size=(6, 8))\n    v1, v2 = emb[0], emb[1]\n    sim = float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12))\n    print('PyTorch未導入のため学習前ランダム埋め込みのみ表示します。')\n    print('random cosine =', round(sim, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "言語モデル学習では、系列 `x_0, x_1, ...` に対して\n`x_t` までを入力として `x_{t+1}` を予測します。\n損失にはクロスエントロピーを使うのが標準です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 手計算に近い最小クロスエントロピー例\nlogits = np.array([1.2, -0.4, 0.3, 2.0], dtype=np.float64)\ntarget_id = 3\n\nlogits_shift = logits - np.max(logits)\nprobs = np.exp(logits_shift) / np.sum(np.exp(logits_shift))\nloss = -math.log(probs[target_id] + 1e-12)\n\nprint('probs =', np.round(probs, 4))\nprint('target id =', target_id)\nprint('cross entropy =', round(loss, 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SFT（Supervised Fine-Tuning）では、\n「指示文（instruction）+ 入力（input）+ 望ましい回答（output）」を1本のテキストに整形して学習します。\n\nポイントは、損失をどこに掛けるかです。\n通常は回答本文に損失を掛け、指示部分は `ignore_index` で除外します。\nまた、言語モデル学習では入力と教師ラベルを1トークン右シフトして計算します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_examples = [\n    {\n        'instruction': '次の文を要約してください。',\n        'input': 'Transformerは系列全体を同時に参照できるため、長距離依存を扱いやすい。',\n        'output': 'Transformerは長距離依存を扱いやすい。',\n    },\n    {\n        'instruction': '用語を説明してください。',\n        'input': 'SFT',\n        'output': '教師ありデータでモデル応答を調整する学習。',\n    },\n]\n\n\ndef format_sft(ex):\n    return (\n        '<system>あなたは丁寧なAIアシスタントです。</system>\\n'\n        f\"<user>{ex['instruction']}\\n{ex['input']}</user>\\n\"\n        f\"<assistant>{ex['output']}</assistant>\"\n    )\n\n\nformatted = [format_sft(ex) for ex in sft_examples]\nfor i, text in enumerate(formatted):\n    print(f'--- sample {i} ---')\n    print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 文字単位の簡易トークナイズで「回答部のみloss」を可視化\nchars = sorted(set(''.join(formatted)))\nchar_vocab = ['<pad>', '<unk>'] + chars\nc_stoi = {c: i for i, c in enumerate(char_vocab)}\n\n\ndef encode_chars(s):\n    unk = c_stoi['<unk>']\n    return [c_stoi.get(ch, unk) for ch in s]\n\n\nignore_index = -100\nfor i, text in enumerate(formatted):\n    ids = encode_chars(text)\n\n    start_tag = '<assistant>'\n    end_tag = '</assistant>'\n    start_pos = text.find(start_tag)\n    end_pos = text.find(end_tag)\n\n    labels = [ignore_index] * len(ids)\n    if start_pos >= 0 and end_pos > start_pos:\n        start = start_pos + len(start_tag)\n        end = end_pos\n        for j in range(start, end):\n            labels[j] = ids[j]\n\n    # 右シフト後に実際に損失へ入るラベルを計算\n    input_ids = ids[:-1]\n    target_ids = labels[1:]\n\n    active = sum(1 for v in target_ids if v != ignore_index)\n    print(f'sample {i}: input_len={len(input_ids)}, supervised_after_shift={active}, ratio={active/max(len(input_ids),1):.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次に、LoRA/QLoRA の計算量感覚を押さえます。\n大きな重み行列 `W` を丸ごと更新せず、低ランク行列 `A, B` だけを学習するのがLoRAです。\n\n`W' = W + (alpha / r) * BA`（`A: r x d_in`, `B: d_out x r`）なので、追加パラメータは `r*(d_in + d_out)` です。\nQLoRAではベース重みを量子化し、LoRA部分だけ高精度で更新します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lora_param_count(d_in, d_out, rank):\n    full = d_in * d_out\n    lora = rank * (d_in + d_out)\n    return full, lora\n\n\nfor rank in [4, 8, 16, 32]:\n    full, lora = lora_param_count(d_in=4096, d_out=4096, rank=rank)\n    print(f'rank={rank:>2d}: full={full:,}, lora={lora:,}, ratio={lora/full:.6f}')\n\n# QLoRAの直感: baseを4bit量子化し、adapterは通常精度で学習\nbase_params = 7_000_000_000\nbase_16bit_gb = base_params * 16 / 8 / (1024**3)\nbase_4bit_gb = base_params * 4 / 8 / (1024**3)\nprint('\\nbase model memory (approx, weights only):')\nprint('fp16:', round(base_16bit_gb, 2), 'GB')\nprint('4bit:', round(base_4bit_gb, 2), 'GB')\nprint('note: optimizer state / activations / metadataは別途必要')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "最後に、PyTorchで小さな文字レベル言語モデルを学習し、\n次トークン予測とテキスト生成を体験します。\n\nこのセルは教育用の最小例で、実際のLLM学習とは規模も最適化手法も異なります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n    torch.manual_seed(0)\n\n    corpus = [\n        'transformerは文脈を使って次を予測する',\n        'sftは指示と回答のペアで学習する',\n        'loraは追加パラメータを小さくできる',\n        'token化と語彙設計は性能に効く',\n    ]\n\n    text = '\\n'.join(corpus)\n    vocab_chars = sorted(set(text))\n    vocab = ['<unk>'] + vocab_chars\n    stoi = {ch: i for i, ch in enumerate(vocab)}\n    itos = {i: ch for ch, i in stoi.items()}\n    unk_id = stoi['<unk>']\n\n    data = torch.tensor([stoi.get(ch, unk_id) for ch in text], dtype=torch.long)\n\n    block_size = 24\n    batch_size = 32\n\n    def get_batch():\n        idx = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n        x = torch.stack([data[i:i+block_size] for i in idx])\n        y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n        return x, y\n\n    class TinyCharLM(nn.Module):\n        def __init__(self, vocab_size, d_model=64):\n            super().__init__()\n            self.token_emb = nn.Embedding(vocab_size, d_model)\n            self.rnn = nn.GRU(d_model, d_model, batch_first=True)\n            self.head = nn.Linear(d_model, vocab_size)\n\n        def forward(self, x):\n            h = self.token_emb(x)\n            out, _ = self.rnn(h)\n            logits = self.head(out)\n            return logits\n\n    model = TinyCharLM(vocab_size=len(vocab), d_model=64)\n    opt = optim.AdamW(model.parameters(), lr=3e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    for step in range(220):\n        xb, yb = get_batch()\n        logits = model(xb)\n        loss = criterion(logits.reshape(-1, len(vocab)), yb.reshape(-1))\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        if step % 55 == 0:\n            print(f'step={step:>3d}, loss={loss.item():.4f}')\n\n    model.eval()\n    prompt = 'sftは？'\n    unknown_count = sum(1 for ch in prompt if ch not in stoi)\n    ids = [stoi.get(ch, unk_id) for ch in prompt]\n    print('prompt unknown chars replaced with <unk> =', unknown_count)\n\n    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n\n    for _ in range(40):\n        logits = model(x)\n        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n        x = torch.cat([x, next_id], dim=1)\n\n    generated = ''.join(itos[i] if i != unk_id else '□' for i in x.squeeze(0).tolist())\n    print('\\nGenerated text:')\n    print(generated)\nelse:\n    print('PyTorch未導入のため、言語モデル実験セルはスキップしました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NLPでは、モデル構造だけでなくデータ整形が性能を大きく左右します。\n特にSFTでは、テンプレート設計・損失マスク・系列長管理が品質とコストに直結します。\n\nこのノートで確認した最小実装を基準に、次は評価設計（自動評価+人手評価）へ進むと実務に接続しやすくなります。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
